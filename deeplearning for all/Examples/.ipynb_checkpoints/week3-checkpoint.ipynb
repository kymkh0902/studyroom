{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08 - Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09-1 - Neural Net for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1914 [[-1.24511898]\n",
      " [-1.15085232]]\n",
      "100 0.699202 [[-0.33781892]\n",
      " [-0.283503  ]]\n",
      "200 0.69575 [[-0.2118635 ]\n",
      " [-0.18284155]]\n",
      "300 0.694334 [[-0.14087538]\n",
      " [-0.12536915]]\n",
      "400 0.693688 [[-0.09403492]\n",
      " [-0.0857507 ]]\n",
      "500 0.693394 [[-0.06290072]\n",
      " [-0.05847505]]\n",
      "600 0.693259 [[-0.04214847]\n",
      " [-0.03978417]]\n",
      "700 0.693198 [[-0.02828362]\n",
      " [-0.02702055]]\n",
      "800 0.69317 [[-0.01900192]\n",
      " [-0.01832719]]\n",
      "900 0.693158 [[-0.01277826]\n",
      " [-0.0124178 ]]\n",
      "1000 0.693152 [[-0.00859954]\n",
      " [-0.00840697]]\n",
      "1100 0.693149 [[-0.00579084]\n",
      " [-0.00568795]]\n",
      "1200 0.693148 [[-0.00390137]\n",
      " [-0.0038464 ]]\n",
      "1300 0.693148 [[-0.00262943]\n",
      " [-0.00260007]]\n",
      "1400 0.693147 [[-0.00177271]\n",
      " [-0.00175703]]\n",
      "1500 0.693147 [[-0.00119542]\n",
      " [-0.00118705]]\n",
      "1600 0.693147 [[-0.00080629]\n",
      " [-0.00080182]]\n",
      "1700 0.693147 [[-0.00054392]\n",
      " [-0.00054153]]\n",
      "1800 0.693147 [[-0.00036696]\n",
      " [-0.00036568]]\n",
      "1900 0.693147 [[-0.00024762]\n",
      " [-0.00024694]]\n",
      "2000 0.693147 [[-0.00016709]\n",
      " [-0.00016672]]\n",
      "2100 0.693147 [[-0.00011275]\n",
      " [-0.00011256]]\n",
      "2200 0.693147 [[ -7.60879120e-05]\n",
      " [ -7.59807226e-05]]\n",
      "2300 0.693147 [[ -5.13415616e-05]\n",
      " [ -5.12880106e-05]]\n",
      "2400 0.693147 [[ -3.46477973e-05]\n",
      " [ -3.46195811e-05]]\n",
      "2500 0.693147 [[ -2.33825103e-05]\n",
      " [ -2.33677092e-05]]\n",
      "2600 0.693147 [[ -1.57754694e-05]\n",
      " [ -1.57696086e-05]]\n",
      "2700 0.693147 [[ -1.06405269e-05]\n",
      " [ -1.06406269e-05]]\n",
      "2800 0.693147 [[ -7.17750345e-06]\n",
      " [ -7.17760349e-06]]\n",
      "2900 0.693147 [[ -4.83951226e-06]\n",
      " [ -4.83961230e-06]]\n",
      "3000 0.693147 [[ -3.26892678e-06]\n",
      " [ -3.26902682e-06]]\n",
      "3100 0.693147 [[ -2.20945026e-06]\n",
      " [ -2.20955030e-06]]\n",
      "3200 0.693147 [[ -1.46886032e-06]\n",
      " [ -1.46896036e-06]]\n",
      "3300 0.693147 [[ -1.01139028e-06]\n",
      " [ -1.01149033e-06]]\n",
      "3400 0.693147 [[ -6.82073562e-07]\n",
      " [ -6.82173606e-07]]\n",
      "3500 0.693147 [[ -4.42164719e-07]\n",
      " [ -4.42264763e-07]]\n",
      "3600 0.693147 [[ -2.79741442e-07]\n",
      " [ -2.79841487e-07]]\n",
      "3700 0.693147 [[ -2.14176083e-07]\n",
      " [ -2.14276128e-07]]\n",
      "3800 0.693147 [[ -1.65002064e-07]\n",
      " [ -1.65102108e-07]]\n",
      "3900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4100 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4200 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4300 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4400 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4500 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4600 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4700 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4800 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "4900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5100 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5200 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5300 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5400 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5500 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5600 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5700 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5800 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "5900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6100 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6200 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6300 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6400 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6500 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6600 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6700 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6800 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "6900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7100 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7200 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7300 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7400 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7500 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7600 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7700 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7800 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "7900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8100 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8200 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8300 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8400 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8500 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8600 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8700 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8800 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "8900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9100 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9200 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9300 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9400 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9500 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9600 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9700 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9800 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "9900 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "10000 0.693147 [[ -1.33709506e-07]\n",
      " [ -1.33809550e-07]]\n",
      "\n",
      "Hypothesis:  [[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]] \n",
      "Correct:  [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "w = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(x, w) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y: y_data}), sess.run(w))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.20593 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "100 0.696729 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "200 0.694375 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "300 0.693142 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "400 0.692191 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "500 0.69136 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "600 0.690548 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "700 0.689684 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "800 0.68871 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "900 0.687571 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1000 0.686205 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1100 0.684545 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1200 0.682509 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1300 0.680006 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1400 0.67693 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1500 0.673171 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1600 0.668618 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1700 0.663179 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1800 0.656792 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1900 0.64945 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2000 0.64121 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2100 0.632201 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2200 0.622606 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2300 0.612639 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2400 0.602513 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2500 0.592406 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2600 0.582442 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2700 0.57269 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2800 0.563162 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2900 0.553836 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3000 0.544673 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3100 0.535631 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3200 0.526679 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3300 0.517786 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3400 0.508911 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3500 0.499989 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3600 0.490911 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3700 0.481498 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3800 0.471458 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3900 0.460317 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4000 0.447341 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4100 0.431477 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4200 0.411493 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4300 0.386562 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4400 0.357213 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4500 0.325615 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4600 0.294491 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4700 0.265821 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4800 0.240473 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4900 0.218516 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5000 0.199631 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5100 0.183377 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5200 0.169325 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5300 0.157098 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5400 0.146389 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5500 0.136946 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5600 0.128568 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5700 0.12109 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5800 0.114381 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5900 0.108331 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6000 0.102852 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6100 0.0978681 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6200 0.093318 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6300 0.0891492 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6400 0.085317 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6500 0.0817837 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6600 0.0785165 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6700 0.0754875 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6800 0.0726722 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6900 0.0700495 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7000 0.0676008 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7100 0.0653098 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7200 0.0631623 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7300 0.0611454 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7400 0.0592481 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7500 0.0574601 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7600 0.0557725 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7700 0.0541774 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7800 0.0526675 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7900 0.0512364 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8000 0.0498781 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8100 0.0485874 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8200 0.0473594 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8300 0.0461898 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8400 0.0450747 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8500 0.0440104 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8600 0.0429934 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8700 0.042021 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8800 0.0410902 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8900 0.0401984 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9000 0.0393433 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9100 0.0385228 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9200 0.0377348 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9300 0.0369775 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9400 0.036249 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9500 0.0355479 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9600 0.0348727 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9700 0.0342219 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9800 0.0335944 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9900 0.0329888 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "10000 0.0324041 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "\n",
      "Hypothesis:  [[ 0.02849933]\n",
      " [ 0.96394414]\n",
      " [ 0.96228963]\n",
      " [ 0.02521791]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y: y_data}), sess.run(w))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.967209 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "100 0.685119 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "200 0.668922 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "300 0.650914 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "400 0.630708 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "500 0.60782 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "600 0.581053 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "700 0.549719 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "800 0.513882 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "900 0.474095 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1000 0.431251 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1100 0.386587 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1200 0.341732 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1300 0.298591 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1400 0.25889 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1500 0.223713 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1600 0.193401 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1700 0.16775 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1800 0.146261 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "1900 0.128329 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2000 0.113363 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2100 0.100833 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2200 0.090291 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2300 0.0813692 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2400 0.0737696 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2500 0.0672532 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2600 0.0616285 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2700 0.0567426 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2800 0.0524722 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "2900 0.0487178 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3000 0.045399 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3100 0.0424498 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3200 0.0398164 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3300 0.0374542 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3400 0.0353261 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3500 0.0334012 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3600 0.0316536 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3700 0.0300613 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3800 0.0286058 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "3900 0.027271 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4000 0.0260435 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4100 0.0249115 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4200 0.0238648 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4300 0.0228946 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4400 0.0219933 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4500 0.0211541 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4600 0.0203711 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4700 0.0196391 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4800 0.0189536 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "4900 0.0183104 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5000 0.017706 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5100 0.0171369 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5200 0.0166005 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5300 0.016094 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5400 0.0156151 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5500 0.0151618 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5600 0.0147321 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5700 0.0143242 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5800 0.0139367 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "5900 0.0135681 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6000 0.0132171 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6100 0.0128825 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6200 0.0125633 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6300 0.0122583 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6400 0.0119669 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6500 0.0116879 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6600 0.0114209 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6700 0.0111649 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6800 0.0109194 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "6900 0.0106838 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7000 0.0104574 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7100 0.0102398 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7200 0.0100305 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7300 0.00982909 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7400 0.00963503 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7500 0.00944805 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7600 0.00926767 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7700 0.00909367 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7800 0.00892567 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "7900 0.00876339 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8000 0.00860654 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8100 0.00845486 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8200 0.0083081 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8300 0.00816611 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8400 0.00802855 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8500 0.00789527 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8600 0.00776609 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8700 0.00764087 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8800 0.00751934 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "8900 0.00740142 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9000 0.00728692 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9100 0.0071757 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9200 0.00706763 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9300 0.00696259 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9400 0.00686046 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9500 0.0067611 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9600 0.00666444 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9700 0.00657034 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9800 0.00647869 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "9900 0.00638947 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "10000 0.00630248 [[-0.68227279]\n",
      " [-0.16302185]]\n",
      "\n",
      "Hypothesis:  [[ 0.00549604]\n",
      " [ 0.99297523]\n",
      " [ 0.99440742]\n",
      " [ 0.00701616]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,10]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y: y_data}), sess.run(w))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09-2 - Tensorboard (Neural Net for XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 steps of using TensorBoard ##\n",
    "1) From TF graph, decide which rensors you want to log\n",
    "- w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "- cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "2) Merge all summaries\n",
    "- summary = tf.summary.merge_all()\n",
    "\n",
    "3) Create writer and add graph\n",
    "- writer = tf.summary.FileWriter('./logs')\n",
    "- writer.add_graph(sess.graph)\n",
    "\n",
    "4) Run summary merge and add_summary\n",
    "- s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "- writer.add_summary(s, global_step=global_step)\n",
    "\n",
    "5) Launch TensorBoard\n",
    "- tensorboard --logdir=./logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
