{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05 - TensorFlow로 Logistic Classification의 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.680567\n",
      "200 0.600959\n",
      "400 0.55956\n",
      "600 0.530556\n",
      "800 0.507345\n",
      "1000 0.487243\n",
      "1200 0.469055\n",
      "1400 0.452204\n",
      "1600 0.436394\n",
      "1800 0.42146\n",
      "2000 0.407306\n",
      "2200 0.393867\n",
      "2400 0.381095\n",
      "2600 0.368952\n",
      "2800 0.357404\n",
      "3000 0.34642\n",
      "3200 0.33597\n",
      "3400 0.326026\n",
      "3600 0.316562\n",
      "3800 0.307552\n",
      "4000 0.29897\n",
      "4200 0.290794\n",
      "4400 0.283\n",
      "4600 0.275567\n",
      "4800 0.268474\n",
      "5000 0.261703\n",
      "5200 0.255234\n",
      "5400 0.249052\n",
      "5600 0.243139\n",
      "5800 0.237481\n",
      "6000 0.232063\n",
      "6200 0.226873\n",
      "6400 0.221896\n",
      "6600 0.217122\n",
      "6800 0.21254\n",
      "7000 0.208139\n",
      "7200 0.20391\n",
      "7400 0.199843\n",
      "7600 0.19593\n",
      "7800 0.192163\n",
      "8000 0.188535\n",
      "8200 0.185037\n",
      "8400 0.181665\n",
      "8600 0.178411\n",
      "8800 0.17527\n",
      "9000 0.172237\n",
      "9200 0.169305\n",
      "9400 0.166471\n",
      "9600 0.163729\n",
      "9800 0.161076\n",
      "10000 0.158508\n",
      "\n",
      "Hypothesis :  [[ 0.03462026]\n",
      " [ 0.16379358]\n",
      " [ 0.32292524]\n",
      " [ 0.77325016]\n",
      " [ 0.93433487]\n",
      " [ 0.97842652]] \n",
      "Correct (Y) :  [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]] \n",
      "Accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "w = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, w) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "\n",
    "##### tf.cast 에서 cast가 정확히 어떤 의미로 쓰이는지?\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X : x_data, Y : y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], \n",
    "                       feed_dict={X : x_data, Y : y_data})\n",
    "    \n",
    "    print(\"\\nHypothesis : \", h, \"\\nCorrect (Y) : \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.843766\n",
      "200 0.709478\n",
      "400 0.677158\n",
      "600 0.658752\n",
      "800 0.643511\n",
      "1000 0.629873\n",
      "1200 0.617505\n",
      "1400 0.606263\n",
      "1600 0.596038\n",
      "1800 0.586734\n",
      "2000 0.578262\n",
      "2200 0.570543\n",
      "2400 0.563505\n",
      "2600 0.557082\n",
      "2800 0.551215\n",
      "3000 0.545849\n",
      "3200 0.540937\n",
      "3400 0.536433\n",
      "3600 0.5323\n",
      "3800 0.528501\n",
      "4000 0.525004\n",
      "4200 0.521782\n",
      "4400 0.518809\n",
      "4600 0.516061\n",
      "4800 0.513519\n",
      "5000 0.511163\n",
      "5200 0.508978\n",
      "5400 0.506948\n",
      "5600 0.505061\n",
      "5800 0.503303\n",
      "6000 0.501665\n",
      "6200 0.500136\n",
      "6400 0.498708\n",
      "6600 0.497373\n",
      "6800 0.496123\n",
      "7000 0.494951\n",
      "7200 0.493853\n",
      "7400 0.492821\n",
      "7600 0.491852\n",
      "7800 0.490941\n",
      "8000 0.490083\n",
      "8200 0.489275\n",
      "8400 0.488514\n",
      "8600 0.487795\n",
      "8800 0.487116\n",
      "9000 0.486475\n",
      "9200 0.485869\n",
      "9400 0.485296\n",
      "9600 0.484753\n",
      "9800 0.484239\n",
      "10000 0.483751\n",
      "\n",
      "Hypothesis :  [[ 0.4355621 ]\n",
      " [ 0.92531431]\n",
      " [ 0.20523708]\n",
      " [ 0.93547237]\n",
      " [ 0.24770196]\n",
      " [ 0.72961956]\n",
      " [ 0.92644924]\n",
      " [ 0.5528667 ]\n",
      " [ 0.23777249]\n",
      " [ 0.53258967]\n",
      " [ 0.70871675]\n",
      " [ 0.17466842]\n",
      " [ 0.30768076]\n",
      " [ 0.32372728]\n",
      " [ 0.72260845]\n",
      " [ 0.47735417]\n",
      " [ 0.70808035]\n",
      " [ 0.82195467]\n",
      " [ 0.81503099]\n",
      " [ 0.62249517]\n",
      " [ 0.67573172]\n",
      " [ 0.11313   ]\n",
      " [ 0.64788812]\n",
      " [ 0.64072788]\n",
      " [ 0.35701042]\n",
      " [ 0.93270749]\n",
      " [ 0.52030569]\n",
      " [ 0.65131652]\n",
      " [ 0.73873699]\n",
      " [ 0.46268415]\n",
      " [ 0.94427931]\n",
      " [ 0.85564411]\n",
      " [ 0.59763068]\n",
      " [ 0.82631326]\n",
      " [ 0.35492057]\n",
      " [ 0.6721257 ]\n",
      " [ 0.83410215]\n",
      " [ 0.60841423]\n",
      " [ 0.41999432]\n",
      " [ 0.40229329]\n",
      " [ 0.84061575]\n",
      " [ 0.20710121]\n",
      " [ 0.36489025]\n",
      " [ 0.09250106]\n",
      " [ 0.56122756]\n",
      " [ 0.93781322]\n",
      " [ 0.71826351]\n",
      " [ 0.68845451]\n",
      " [ 0.93648237]\n",
      " [ 0.92421061]\n",
      " [ 0.92021435]\n",
      " [ 0.2483435 ]\n",
      " [ 0.36352953]\n",
      " [ 0.96414542]\n",
      " [ 0.19368386]\n",
      " [ 0.51282465]\n",
      " [ 0.19449733]\n",
      " [ 0.67552835]\n",
      " [ 0.86424953]\n",
      " [ 0.47926342]\n",
      " [ 0.945759  ]\n",
      " [ 0.68844461]\n",
      " [ 0.6354056 ]\n",
      " [ 0.85019404]\n",
      " [ 0.65504962]\n",
      " [ 0.65082097]\n",
      " [ 0.95042169]\n",
      " [ 0.66937596]\n",
      " [ 0.86838627]\n",
      " [ 0.63243788]\n",
      " [ 0.29498103]\n",
      " [ 0.73117256]\n",
      " [ 0.92383027]\n",
      " [ 0.91803628]\n",
      " [ 0.87924379]\n",
      " [ 0.80158806]\n",
      " [ 0.40107515]\n",
      " [ 0.86446887]\n",
      " [ 0.88283461]\n",
      " [ 0.90678352]\n",
      " [ 0.8724032 ]\n",
      " [ 0.80919504]\n",
      " [ 0.4107843 ]\n",
      " [ 0.82247043]\n",
      " [ 0.50048351]\n",
      " [ 0.86693364]\n",
      " [ 0.37279919]\n",
      " [ 0.89681292]\n",
      " [ 0.92801541]\n",
      " [ 0.7837345 ]\n",
      " [ 0.8017844 ]\n",
      " [ 0.63559258]\n",
      " [ 0.73755378]\n",
      " [ 0.56550241]\n",
      " [ 0.89810395]\n",
      " [ 0.97063971]\n",
      " [ 0.86776304]\n",
      " [ 0.61735243]\n",
      " [ 0.30450377]\n",
      " [ 0.60078532]\n",
      " [ 0.60686678]\n",
      " [ 0.95535833]\n",
      " [ 0.80108428]\n",
      " [ 0.76637661]\n",
      " [ 0.89211792]\n",
      " [ 0.65077609]\n",
      " [ 0.90939069]\n",
      " [ 0.81084639]\n",
      " [ 0.45701176]\n",
      " [ 0.37171954]\n",
      " [ 0.92155051]\n",
      " [ 0.86605442]\n",
      " [ 0.40238735]\n",
      " [ 0.46794882]\n",
      " [ 0.61490661]\n",
      " [ 0.83065969]\n",
      " [ 0.86869645]\n",
      " [ 0.91567445]\n",
      " [ 0.14818363]\n",
      " [ 0.71157688]\n",
      " [ 0.84711474]\n",
      " [ 0.60883582]\n",
      " [ 0.60981864]\n",
      " [ 0.79709691]\n",
      " [ 0.70715237]\n",
      " [ 0.81249398]\n",
      " [ 0.82639611]\n",
      " [ 0.64151728]\n",
      " [ 0.49999136]\n",
      " [ 0.4348034 ]\n",
      " [ 0.40507662]\n",
      " [ 0.79511201]\n",
      " [ 0.93270451]\n",
      " [ 0.81128293]\n",
      " [ 0.78449196]\n",
      " [ 0.83568943]\n",
      " [ 0.46685487]\n",
      " [ 0.79412073]\n",
      " [ 0.71798933]\n",
      " [ 0.75058055]\n",
      " [ 0.86171263]\n",
      " [ 0.60470635]\n",
      " [ 0.53874034]\n",
      " [ 0.73393178]\n",
      " [ 0.90532416]\n",
      " [ 0.78248042]\n",
      " [ 0.44596618]\n",
      " [ 0.93013507]\n",
      " [ 0.61743307]\n",
      " [ 0.75978029]\n",
      " [ 0.3141973 ]\n",
      " [ 0.4403483 ]\n",
      " [ 0.11155353]\n",
      " [ 0.2691592 ]\n",
      " [ 0.91047961]\n",
      " [ 0.87686247]\n",
      " [ 0.93641055]\n",
      " [ 0.11241291]\n",
      " [ 0.5310331 ]\n",
      " [ 0.75065124]\n",
      " [ 0.60797101]\n",
      " [ 0.87486154]\n",
      " [ 0.4453769 ]\n",
      " [ 0.81888604]\n",
      " [ 0.61339468]\n",
      " [ 0.64887667]\n",
      " [ 0.71791822]\n",
      " [ 0.87469333]\n",
      " [ 0.75407559]\n",
      " [ 0.62411606]\n",
      " [ 0.90056622]\n",
      " [ 0.87815344]\n",
      " [ 0.94580966]\n",
      " [ 0.21569158]\n",
      " [ 0.81534624]\n",
      " [ 0.28903198]\n",
      " [ 0.39405918]\n",
      " [ 0.45343277]\n",
      " [ 0.86322498]\n",
      " [ 0.67286009]\n",
      " [ 0.91875714]\n",
      " [ 0.89422894]\n",
      " [ 0.57084656]\n",
      " [ 0.15978518]\n",
      " [ 0.20541471]\n",
      " [ 0.66927671]\n",
      " [ 0.72165704]\n",
      " [ 0.61536717]\n",
      " [ 0.80482745]\n",
      " [ 0.57615274]\n",
      " [ 0.33550766]\n",
      " [ 0.22619539]\n",
      " [ 0.89003515]\n",
      " [ 0.38463777]\n",
      " [ 0.84714538]\n",
      " [ 0.89509887]\n",
      " [ 0.71946007]\n",
      " [ 0.61374342]\n",
      " [ 0.68464655]\n",
      " [ 0.57394969]\n",
      " [ 0.7376743 ]\n",
      " [ 0.93710166]\n",
      " [ 0.76445138]\n",
      " [ 0.81219804]\n",
      " [ 0.14511882]\n",
      " [ 0.33185622]\n",
      " [ 0.89929253]\n",
      " [ 0.22044447]\n",
      " [ 0.93354005]\n",
      " [ 0.28077695]\n",
      " [ 0.27166227]\n",
      " [ 0.46227261]\n",
      " [ 0.69173294]\n",
      " [ 0.22052264]\n",
      " [ 0.73390841]\n",
      " [ 0.70140868]\n",
      " [ 0.85378963]\n",
      " [ 0.65770447]\n",
      " [ 0.18181485]\n",
      " [ 0.40025178]\n",
      " [ 0.66601962]\n",
      " [ 0.51659417]\n",
      " [ 0.92335629]\n",
      " [ 0.92907429]\n",
      " [ 0.67640251]\n",
      " [ 0.37825105]\n",
      " [ 0.0759315 ]\n",
      " [ 0.63024795]\n",
      " [ 0.39714152]\n",
      " [ 0.47572589]\n",
      " [ 0.94883603]\n",
      " [ 0.62914002]\n",
      " [ 0.94149727]\n",
      " [ 0.2370121 ]\n",
      " [ 0.17275546]\n",
      " [ 0.32293132]\n",
      " [ 0.74051279]\n",
      " [ 0.91521519]\n",
      " [ 0.87316948]\n",
      " [ 0.61222231]\n",
      " [ 0.69650275]\n",
      " [ 0.58622622]\n",
      " [ 0.20142201]\n",
      " [ 0.51948094]\n",
      " [ 0.17354758]\n",
      " [ 0.56903321]\n",
      " [ 0.87613732]\n",
      " [ 0.6375795 ]\n",
      " [ 0.69625258]\n",
      " [ 0.94579762]\n",
      " [ 0.80701256]\n",
      " [ 0.79189444]\n",
      " [ 0.78106678]\n",
      " [ 0.76789051]\n",
      " [ 0.85442656]\n",
      " [ 0.39990878]\n",
      " [ 0.3967433 ]\n",
      " [ 0.52331346]\n",
      " [ 0.8314575 ]\n",
      " [ 0.68251634]\n",
      " [ 0.65874845]\n",
      " [ 0.83720231]\n",
      " [ 0.33983308]\n",
      " [ 0.55443811]\n",
      " [ 0.62924337]\n",
      " [ 0.59024459]\n",
      " [ 0.51011831]\n",
      " [ 0.88205189]\n",
      " [ 0.74550933]\n",
      " [ 0.92298812]\n",
      " [ 0.52750176]\n",
      " [ 0.77006978]\n",
      " [ 0.80523938]\n",
      " [ 0.80245423]\n",
      " [ 0.68652421]\n",
      " [ 0.87877107]\n",
      " [ 0.3370859 ]\n",
      " [ 0.56106502]\n",
      " [ 0.68490654]\n",
      " [ 0.34350306]\n",
      " [ 0.8053146 ]\n",
      " [ 0.33259028]\n",
      " [ 0.66518652]\n",
      " [ 0.92367929]\n",
      " [ 0.75624061]\n",
      " [ 0.85257238]\n",
      " [ 0.68850005]\n",
      " [ 0.56583917]\n",
      " [ 0.64058787]\n",
      " [ 0.34946349]\n",
      " [ 0.45580775]\n",
      " [ 0.63120228]\n",
      " [ 0.62612087]\n",
      " [ 0.64155114]\n",
      " [ 0.62956309]\n",
      " [ 0.2107411 ]\n",
      " [ 0.65438688]\n",
      " [ 0.89297128]\n",
      " [ 0.53578031]\n",
      " [ 0.59216982]\n",
      " [ 0.749924  ]\n",
      " [ 0.43038118]\n",
      " [ 0.68826067]\n",
      " [ 0.56287038]\n",
      " [ 0.73248142]\n",
      " [ 0.89490533]\n",
      " [ 0.65737104]\n",
      " [ 0.68673295]\n",
      " [ 0.85799986]\n",
      " [ 0.62971711]\n",
      " [ 0.848315  ]\n",
      " [ 0.9363941 ]\n",
      " [ 0.29059559]\n",
      " [ 0.76144767]\n",
      " [ 0.22027515]\n",
      " [ 0.77554905]\n",
      " [ 0.81036162]\n",
      " [ 0.69172531]\n",
      " [ 0.3441965 ]\n",
      " [ 0.79617983]\n",
      " [ 0.7108078 ]\n",
      " [ 0.75588876]\n",
      " [ 0.16407236]\n",
      " [ 0.81507099]\n",
      " [ 0.82217169]\n",
      " [ 0.65253776]\n",
      " [ 0.93448079]\n",
      " [ 0.25828454]\n",
      " [ 0.65312308]\n",
      " [ 0.93997186]\n",
      " [ 0.21364483]\n",
      " [ 0.49467281]\n",
      " [ 0.67938238]\n",
      " [ 0.346908  ]\n",
      " [ 0.18171561]\n",
      " [ 0.84138966]\n",
      " [ 0.91909856]\n",
      " [ 0.86166751]\n",
      " [ 0.61981624]\n",
      " [ 0.67629653]\n",
      " [ 0.57036686]\n",
      " [ 0.75652158]\n",
      " [ 0.78159338]\n",
      " [ 0.92149949]\n",
      " [ 0.76430082]\n",
      " [ 0.77198833]\n",
      " [ 0.57499892]\n",
      " [ 0.94187403]\n",
      " [ 0.93987983]\n",
      " [ 0.76474881]\n",
      " [ 0.2634325 ]\n",
      " [ 0.69792873]\n",
      " [ 0.36983943]\n",
      " [ 0.72972852]\n",
      " [ 0.2323311 ]\n",
      " [ 0.24934879]\n",
      " [ 0.40182146]\n",
      " [ 0.73926437]\n",
      " [ 0.40586907]\n",
      " [ 0.57734507]\n",
      " [ 0.83075339]\n",
      " [ 0.6282565 ]\n",
      " [ 0.84408569]\n",
      " [ 0.94549155]\n",
      " [ 0.76412386]\n",
      " [ 0.15796134]\n",
      " [ 0.55385816]\n",
      " [ 0.83994633]\n",
      " [ 0.83538163]\n",
      " [ 0.67812335]\n",
      " [ 0.29386976]\n",
      " [ 0.86902934]\n",
      " [ 0.89110005]\n",
      " [ 0.30984738]\n",
      " [ 0.67045313]\n",
      " [ 0.84916484]\n",
      " [ 0.83009875]\n",
      " [ 0.88627779]\n",
      " [ 0.91704422]\n",
      " [ 0.85576648]\n",
      " [ 0.90283066]\n",
      " [ 0.7121228 ]\n",
      " [ 0.65009397]\n",
      " [ 0.57532167]\n",
      " [ 0.85152453]\n",
      " [ 0.87801117]\n",
      " [ 0.24302354]\n",
      " [ 0.81954968]\n",
      " [ 0.86846173]\n",
      " [ 0.35485592]\n",
      " [ 0.69084895]\n",
      " [ 0.86491114]\n",
      " [ 0.52365863]\n",
      " [ 0.90287685]\n",
      " [ 0.27154702]\n",
      " [ 0.81566578]\n",
      " [ 0.59832692]\n",
      " [ 0.88050157]\n",
      " [ 0.32825348]\n",
      " [ 0.69762892]\n",
      " [ 0.71249568]\n",
      " [ 0.7540307 ]\n",
      " [ 0.11208238]\n",
      " [ 0.26312113]\n",
      " [ 0.71872997]\n",
      " [ 0.82446706]\n",
      " [ 0.55648053]\n",
      " [ 0.77023345]\n",
      " [ 0.4749186 ]\n",
      " [ 0.37775517]\n",
      " [ 0.87875855]\n",
      " [ 0.50286281]\n",
      " [ 0.91622591]\n",
      " [ 0.78548932]\n",
      " [ 0.6961422 ]\n",
      " [ 0.91833347]\n",
      " [ 0.64625758]\n",
      " [ 0.81420553]\n",
      " [ 0.35137412]\n",
      " [ 0.27366924]\n",
      " [ 0.72521198]\n",
      " [ 0.41625223]\n",
      " [ 0.44900703]\n",
      " [ 0.89991546]\n",
      " [ 0.87389725]\n",
      " [ 0.91495734]\n",
      " [ 0.9518708 ]\n",
      " [ 0.66075552]\n",
      " [ 0.91998428]\n",
      " [ 0.34655565]\n",
      " [ 0.36759225]\n",
      " [ 0.46899658]\n",
      " [ 0.93970579]\n",
      " [ 0.63497514]\n",
      " [ 0.19717482]\n",
      " [ 0.92659825]\n",
      " [ 0.79333609]\n",
      " [ 0.60088384]\n",
      " [ 0.81376785]\n",
      " [ 0.03670316]\n",
      " [ 0.92278224]\n",
      " [ 0.7455498 ]\n",
      " [ 0.72712624]\n",
      " [ 0.7406829 ]\n",
      " [ 0.96101636]\n",
      " [ 0.63198429]\n",
      " [ 0.76911116]\n",
      " [ 0.7403546 ]\n",
      " [ 0.85126656]\n",
      " [ 0.17359112]\n",
      " [ 0.63205552]\n",
      " [ 0.89761221]\n",
      " [ 0.58104241]\n",
      " [ 0.73384017]\n",
      " [ 0.93718123]\n",
      " [ 0.84152406]\n",
      " [ 0.88211662]\n",
      " [ 0.55229008]\n",
      " [ 0.75982422]\n",
      " [ 0.92992377]\n",
      " [ 0.73610741]\n",
      " [ 0.60151017]\n",
      " [ 0.32713512]\n",
      " [ 0.52113062]\n",
      " [ 0.50232702]\n",
      " [ 0.61711568]\n",
      " [ 0.51343584]\n",
      " [ 0.74639285]\n",
      " [ 0.57588553]\n",
      " [ 0.78974527]\n",
      " [ 0.81718665]\n",
      " [ 0.68360543]\n",
      " [ 0.6745593 ]\n",
      " [ 0.49674699]\n",
      " [ 0.5856415 ]\n",
      " [ 0.92440253]\n",
      " [ 0.83179677]\n",
      " [ 0.26932836]\n",
      " [ 0.42540509]\n",
      " [ 0.54338396]\n",
      " [ 0.14713378]\n",
      " [ 0.88930476]\n",
      " [ 0.14760618]\n",
      " [ 0.90285188]\n",
      " [ 0.88923991]\n",
      " [ 0.82461274]\n",
      " [ 0.72576106]\n",
      " [ 0.87772334]\n",
      " [ 0.33924016]\n",
      " [ 0.75108784]\n",
      " [ 0.94042999]\n",
      " [ 0.26160115]\n",
      " [ 0.45285377]\n",
      " [ 0.87079048]\n",
      " [ 0.87965304]\n",
      " [ 0.69069493]\n",
      " [ 0.81542492]\n",
      " [ 0.82122654]\n",
      " [ 0.79454744]\n",
      " [ 0.2669532 ]\n",
      " [ 0.77221894]\n",
      " [ 0.9072367 ]\n",
      " [ 0.60889333]\n",
      " [ 0.7834174 ]\n",
      " [ 0.66997588]\n",
      " [ 0.78286225]\n",
      " [ 0.8597067 ]\n",
      " [ 0.91564238]\n",
      " [ 0.58387017]\n",
      " [ 0.43108279]\n",
      " [ 0.74040502]\n",
      " [ 0.78174281]\n",
      " [ 0.95973974]\n",
      " [ 0.75377458]\n",
      " [ 0.66560948]\n",
      " [ 0.41961193]\n",
      " [ 0.68272477]\n",
      " [ 0.92531157]\n",
      " [ 0.94192964]\n",
      " [ 0.87805849]\n",
      " [ 0.67898637]\n",
      " [ 0.64496732]\n",
      " [ 0.81211716]\n",
      " [ 0.51002747]\n",
      " [ 0.82639128]\n",
      " [ 0.7883848 ]\n",
      " [ 0.90935689]\n",
      " [ 0.58917809]\n",
      " [ 0.68996185]\n",
      " [ 0.89270151]\n",
      " [ 0.51114702]\n",
      " [ 0.5732879 ]\n",
      " [ 0.67430651]\n",
      " [ 0.73480725]\n",
      " [ 0.70925057]\n",
      " [ 0.90224963]\n",
      " [ 0.92009133]\n",
      " [ 0.20797627]\n",
      " [ 0.17907086]\n",
      " [ 0.74194211]\n",
      " [ 0.50319177]\n",
      " [ 0.27874666]\n",
      " [ 0.84062672]\n",
      " [ 0.90222603]\n",
      " [ 0.69018477]\n",
      " [ 0.929995  ]\n",
      " [ 0.91837591]\n",
      " [ 0.72191221]\n",
      " [ 0.84270668]\n",
      " [ 0.67236257]\n",
      " [ 0.55902267]\n",
      " [ 0.74176747]\n",
      " [ 0.5892185 ]\n",
      " [ 0.12306986]\n",
      " [ 0.89922959]\n",
      " [ 0.86878794]\n",
      " [ 0.72142196]\n",
      " [ 0.91739893]\n",
      " [ 0.86585289]\n",
      " [ 0.86095107]\n",
      " [ 0.56444377]\n",
      " [ 0.6589601 ]\n",
      " [ 0.88040322]\n",
      " [ 0.72152245]\n",
      " [ 0.83503968]\n",
      " [ 0.9021197 ]\n",
      " [ 0.64187503]\n",
      " [ 0.76391286]\n",
      " [ 0.82004768]\n",
      " [ 0.60463727]\n",
      " [ 0.47983742]\n",
      " [ 0.08715893]\n",
      " [ 0.28116524]\n",
      " [ 0.81441927]\n",
      " [ 0.63933641]\n",
      " [ 0.66670096]\n",
      " [ 0.60849458]\n",
      " [ 0.93744451]\n",
      " [ 0.40676606]\n",
      " [ 0.78162682]\n",
      " [ 0.33503565]\n",
      " [ 0.8738209 ]\n",
      " [ 0.41209725]\n",
      " [ 0.7549907 ]\n",
      " [ 0.58580351]\n",
      " [ 0.89284486]\n",
      " [ 0.58971536]\n",
      " [ 0.2530286 ]\n",
      " [ 0.79415286]\n",
      " [ 0.92966777]\n",
      " [ 0.3676441 ]\n",
      " [ 0.90547377]\n",
      " [ 0.88347512]\n",
      " [ 0.82008553]\n",
      " [ 0.79824537]\n",
      " [ 0.43906471]\n",
      " [ 0.28539735]\n",
      " [ 0.72085696]\n",
      " [ 0.23772131]\n",
      " [ 0.94100386]\n",
      " [ 0.3387416 ]\n",
      " [ 0.91434222]\n",
      " [ 0.86761647]\n",
      " [ 0.426415  ]\n",
      " [ 0.23448327]\n",
      " [ 0.74058974]\n",
      " [ 0.44037971]\n",
      " [ 0.81379634]\n",
      " [ 0.68926877]\n",
      " [ 0.97454458]\n",
      " [ 0.63005805]\n",
      " [ 0.58749694]\n",
      " [ 0.7772916 ]\n",
      " [ 0.86574388]\n",
      " [ 0.1117762 ]\n",
      " [ 0.75110781]\n",
      " [ 0.79071766]\n",
      " [ 0.8334946 ]\n",
      " [ 0.59028906]\n",
      " [ 0.45992506]\n",
      " [ 0.58925748]\n",
      " [ 0.89923304]\n",
      " [ 0.63082606]\n",
      " [ 0.74469924]\n",
      " [ 0.79581851]\n",
      " [ 0.84961253]\n",
      " [ 0.76025593]\n",
      " [ 0.54908955]\n",
      " [ 0.77140903]\n",
      " [ 0.90537947]\n",
      " [ 0.72981405]\n",
      " [ 0.94836122]\n",
      " [ 0.80517715]\n",
      " [ 0.6064651 ]\n",
      " [ 0.46617725]\n",
      " [ 0.82621914]\n",
      " [ 0.85065085]\n",
      " [ 0.48788905]\n",
      " [ 0.64810145]\n",
      " [ 0.21433046]\n",
      " [ 0.52055454]\n",
      " [ 0.78697991]\n",
      " [ 0.93624246]\n",
      " [ 0.83122361]\n",
      " [ 0.69650382]\n",
      " [ 0.73589998]\n",
      " [ 0.8757692 ]\n",
      " [ 0.51456058]\n",
      " [ 0.90804237]\n",
      " [ 0.635203  ]\n",
      " [ 0.87643111]\n",
      " [ 0.28578153]\n",
      " [ 0.12826024]\n",
      " [ 0.27410644]\n",
      " [ 0.34657374]\n",
      " [ 0.68597126]\n",
      " [ 0.81000417]\n",
      " [ 0.62198246]\n",
      " [ 0.71995628]\n",
      " [ 0.80011779]\n",
      " [ 0.45630723]\n",
      " [ 0.38750231]\n",
      " [ 0.91290069]\n",
      " [ 0.89219677]\n",
      " [ 0.50819653]\n",
      " [ 0.69684654]\n",
      " [ 0.17085923]\n",
      " [ 0.36202273]\n",
      " [ 0.71858287]\n",
      " [ 0.67683536]\n",
      " [ 0.90179092]\n",
      " [ 0.97196275]\n",
      " [ 0.21569334]\n",
      " [ 0.70726568]\n",
      " [ 0.61828512]\n",
      " [ 0.47711483]\n",
      " [ 0.73667735]\n",
      " [ 0.69845676]\n",
      " [ 0.8882879 ]\n",
      " [ 0.73701048]\n",
      " [ 0.51048446]\n",
      " [ 0.66392136]\n",
      " [ 0.17060514]\n",
      " [ 0.70852435]\n",
      " [ 0.50716293]\n",
      " [ 0.8915574 ]\n",
      " [ 0.57821596]\n",
      " [ 0.54863352]\n",
      " [ 0.75330955]\n",
      " [ 0.75379509]\n",
      " [ 0.52775258]\n",
      " [ 0.76341671]\n",
      " [ 0.6658662 ]\n",
      " [ 0.40794075]\n",
      " [ 0.606673  ]\n",
      " [ 0.8678931 ]\n",
      " [ 0.84403968]\n",
      " [ 0.55273789]\n",
      " [ 0.77367455]\n",
      " [ 0.26675686]\n",
      " [ 0.85464013]\n",
      " [ 0.60344297]\n",
      " [ 0.74032331]\n",
      " [ 0.42521486]\n",
      " [ 0.63370222]\n",
      " [ 0.81716365]\n",
      " [ 0.18224534]\n",
      " [ 0.32706085]\n",
      " [ 0.81655955]\n",
      " [ 0.80158693]\n",
      " [ 0.8122263 ]\n",
      " [ 0.91459703]\n",
      " [ 0.78752923]\n",
      " [ 0.68047911]\n",
      " [ 0.72752756]\n",
      " [ 0.78032148]\n",
      " [ 0.70855796]\n",
      " [ 0.80399448]\n",
      " [ 0.5046857 ]\n",
      " [ 0.42287308]\n",
      " [ 0.86469334]\n",
      " [ 0.7820996 ]\n",
      " [ 0.61737549]\n",
      " [ 0.31704468]\n",
      " [ 0.86829394]\n",
      " [ 0.81568491]\n",
      " [ 0.83451098]\n",
      " [ 0.65995961]\n",
      " [ 0.89704138]\n",
      " [ 0.87488371]\n",
      " [ 0.78241271]\n",
      " [ 0.4464992 ]\n",
      " [ 0.8795194 ]\n",
      " [ 0.89935905]\n",
      " [ 0.35989493]\n",
      " [ 0.19125038]\n",
      " [ 0.70709813]\n",
      " [ 0.460446  ]\n",
      " [ 0.8386876 ]\n",
      " [ 0.33376256]\n",
      " [ 0.4005889 ]\n",
      " [ 0.48066765]\n",
      " [ 0.76148659]\n",
      " [ 0.86170793]\n",
      " [ 0.15060227]\n",
      " [ 0.38963836]\n",
      " [ 0.66049492]\n",
      " [ 0.5097689 ]\n",
      " [ 0.50481576]\n",
      " [ 0.78664082]\n",
      " [ 0.16625385]\n",
      " [ 0.91381115]\n",
      " [ 0.20593956]\n",
      " [ 0.81159371]\n",
      " [ 0.68450505]\n",
      " [ 0.74681401]\n",
      " [ 0.8053003 ]\n",
      " [ 0.70368785]\n",
      " [ 0.89709961]] \n",
      "Correct (Y) :  [[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]] \n",
      "Accuracy :  0.776021\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "w = tf.Variable(tf.random_normal([8,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, w) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X : x_data, Y : y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], \n",
    "                       feed_dict={X : x_data, Y : y_data})\n",
    "    \n",
    "    print(\"\\nHypothesis : \", h, \"\\nCorrect (Y) : \", c, \"\\nAccuracy : \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06-1 - TensorFlow로 Sotfmax Classification의 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.09568\n",
      "200 0.67972\n",
      "400 0.566848\n",
      "600 0.473911\n",
      "800 0.384641\n",
      "1000 0.295685\n",
      "1200 0.232852\n",
      "1400 0.211507\n",
      "1600 0.193622\n",
      "1800 0.178408\n",
      "2000 0.165321\n",
      "\n",
      "\n",
      " [[  1.94311701e-02   9.80557442e-01   1.13979286e-05]\n",
      " [  7.41075695e-01   2.28590623e-01   3.03336661e-02]\n",
      " [  2.38032882e-08   4.19611082e-04   9.99580324e-01]] \n",
      " [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]]\n",
    "y_data = [[0,0,1], [0,0,1], [0,0,1], [0,1,0], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]\n",
    "\n",
    "\n",
    "##### tf.float32 랑 \"float\"의 차이는 무엇인가요??\n",
    "\n",
    "X = tf.placeholder(\"float\", shape=[None, 4])\n",
    "Y = tf.placeholder(\"float\", shape=[None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "w = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, w) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X : x_data, Y : y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X : x_data, Y : y_data}))\n",
    "            \n",
    "    all = sess.run(hypothesis, feed_dict={X : [[1,11,7,9], [1,3,4,3], [1,1,0,1]]})\n",
    "    print(\"\\n\\n\", all, \"\\n\", sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06-2 - TensorFlow로 Fancy Softmax Classification의 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 3)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "\n",
    "Y = tf.placeholder(tf.int32, [None, 1])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "\n",
    "##### reshape 시 -1 이 붙을 때, 왜 맨 앞???\n",
    "\n",
    "Y_one_hot2 = tf.reshape(Y_one_hot, [-1, 2])\n",
    "\n",
    "print(Y_one_hot.shape)\n",
    "print(Y_one_hot2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step :     0\tLoss : 4.166\tAcc : 0.15\n",
      "Step :   100\tLoss : 0.628\tAcc : 0.85\n",
      "Step :   200\tLoss : 0.407\tAcc : 0.9\n",
      "Step :   300\tLoss : 0.306\tAcc : 0.93\n",
      "Step :   400\tLoss : 0.246\tAcc : 0.95\n",
      "Step :   500\tLoss : 0.206\tAcc : 0.97\n",
      "Step :   600\tLoss : 0.178\tAcc : 0.97\n",
      "Step :   700\tLoss : 0.156\tAcc : 0.97\n",
      "Step :   800\tLoss : 0.138\tAcc : 0.98\n",
      "Step :   900\tLoss : 0.124\tAcc : 0.98\n",
      "Step :  1000\tLoss : 0.113\tAcc : 0.98\n",
      "Step :  1100\tLoss : 0.103\tAcc : 0.99\n",
      "Step :  1200\tLoss : 0.095\tAcc : 0.99\n",
      "Step :  1300\tLoss : 0.087\tAcc : 0.99\n",
      "Step :  1400\tLoss : 0.081\tAcc : 1.0\n",
      "Step :  1500\tLoss : 0.076\tAcc : 1.0\n",
      "Step :  1600\tLoss : 0.071\tAcc : 1.0\n",
      "Step :  1700\tLoss : 0.067\tAcc : 1.0\n",
      "Step :  1800\tLoss : 0.063\tAcc : 1.0\n",
      "Step :  1900\tLoss : 0.060\tAcc : 1.0\n",
      "Step :  2000\tLoss : 0.057\tAcc : 1.0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 4 True Y : 4\n",
      "[True] Prediction : 4 True Y : 4\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 4 True Y : 4\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 2 True Y : 2\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 2 True Y : 2\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 2 True Y : 2\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 4 True Y : 4\n",
      "[True] Prediction : 2 True Y : 2\n",
      "[True] Prediction : 2 True Y : 2\n",
      "[True] Prediction : 3 True Y : 3\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 1 True Y : 1\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 5 True Y : 5\n",
      "[True] Prediction : 0 True Y : 0\n",
      "[True] Prediction : 6 True Y : 6\n",
      "[True] Prediction : 1 True Y : 1\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "nb_classes = 7\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "\n",
    "w = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "logits = tf.matmul(X, w) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "##### 밑에 코드는 이해가 잘 안됩니다요\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X : x_data, Y : y_data})\n",
    "        if step % 100 == 0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={X : x_data, Y : y_data})\n",
    "            print(\"Step : {:5}\\tLoss : {:.3f}\\tAcc : {:.2}\".format(step, loss, acc))\n",
    "            \n",
    "    pred = sess.run(prediction, feed_dict={X : x_data})\n",
    "    \n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction : {} True Y : {}\".format(p == int(y), p, int(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07-1 - Training / Test datase, Learning rate, Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
