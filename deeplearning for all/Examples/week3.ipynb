{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08 - Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09-1 - Neural Net for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.695316 [[ 0.16143055]\n",
      " [ 0.014425  ]]\n",
      "100 0.693147 [[-0.00041297]\n",
      " [ 0.00032766]]\n",
      "200 0.693147 [[ -2.65433050e-06]\n",
      " [  1.87755518e-06]]\n",
      "300 0.693147 [[ -2.35807818e-09]\n",
      " [  8.05074123e-08]]\n",
      "400 0.693147 [[ -5.04050348e-08]\n",
      " [ -4.93789400e-08]]\n",
      "500 0.693147 [[ -1.97774455e-07]\n",
      " [ -1.68392347e-07]]\n",
      "600 0.693147 [[  9.42786755e-08]\n",
      " [ -9.70301386e-08]]\n",
      "700 0.693147 [[  1.52656852e-07]\n",
      " [ -2.86240933e-08]]\n",
      "800 0.693147 [[  8.78092550e-08]\n",
      " [  7.55516112e-08]]\n",
      "900 0.693147 [[  1.61563776e-08]\n",
      " [ -1.03197920e-07]]\n",
      "1000 0.693147 [[  8.38978451e-08]\n",
      " [ -4.99997270e-08]]\n",
      "1100 0.693147 [[ -3.23403214e-07]\n",
      " [ -1.00728101e-08]]\n",
      "1200 0.693147 [[  3.05915364e-07]\n",
      " [  4.56459645e-07]]\n",
      "1300 0.693147 [[ -1.44647731e-06]\n",
      " [ -1.45635192e-06]]\n",
      "1400 0.693148 [[ 0.00155294]\n",
      " [ 0.00157384]]\n",
      "1500 0.693147 [[  1.21425655e-05]\n",
      " [  1.21372259e-05]]\n",
      "1600 0.693267 [[-0.01445584]\n",
      " [-0.01461524]]\n",
      "1700 0.693147 [[ -8.21136200e-05]\n",
      " [ -8.25573079e-05]]\n",
      "1800 0.693147 [[  3.08754170e-06]\n",
      " [  3.34523520e-06]]\n",
      "1900 0.693148 [[ 0.00086475]\n",
      " [ 0.00087018]]\n",
      "2000 0.693147 [[ -3.01346859e-07]\n",
      " [ -2.71735530e-07]]\n",
      "2100 0.693243 [[ 0.01299914]\n",
      " [ 0.01306589]]\n",
      "2200 0.693147 [[  6.34404496e-05]\n",
      " [  6.34910248e-05]]\n",
      "2300 0.693147 [[ -1.81132077e-06]\n",
      " [ -2.29784473e-06]]\n",
      "2400 0.693148 [[-0.00152254]\n",
      " [-0.00152724]]\n",
      "2500 0.693147 [[ -1.01446130e-05]\n",
      " [ -1.06000307e-05]]\n",
      "2600 0.693147 [[  8.92542012e-05]\n",
      " [  8.93923643e-05]]\n",
      "2700 0.693147 [[-0.00021082]\n",
      " [-0.00021106]]\n",
      "2800 0.693147 [[ -3.45462922e-06]\n",
      " [ -3.40177417e-06]]\n",
      "2900 0.693147 [[-0.00065674]\n",
      " [-0.00065776]]\n",
      "3000 0.693147 [[  8.90974479e-06]\n",
      " [  9.11046664e-06]]\n",
      "3100 0.693147 [[ -9.22821055e-05]\n",
      " [ -9.26312205e-05]]\n",
      "3200 0.693147 [[  4.68120888e-05]\n",
      " [  4.69779698e-05]]\n",
      "3300 0.693147 [[  2.49226650e-06]\n",
      " [  2.38884013e-06]]\n",
      "3400 0.69315 [[ 0.00222876]\n",
      " [ 0.00223028]]\n",
      "3500 0.693147 [[ -1.20074019e-05]\n",
      " [ -1.11836162e-05]]\n",
      "3600 0.693148 [[-0.00133637]\n",
      " [-0.00133693]]\n",
      "3700 0.693147 [[ -4.01080179e-05]\n",
      " [ -4.04578605e-05]]\n",
      "3800 0.693147 [[  2.22041854e-06]\n",
      " [  2.07967400e-06]]\n",
      "3900 0.693147 [[ 0.00068205]\n",
      " [ 0.00068246]]\n",
      "4000 0.693147 [[  6.42475243e-06]\n",
      " [  7.05657749e-06]]\n",
      "4100 0.693148 [[ 0.00102626]\n",
      " [ 0.00102721]]\n",
      "4200 0.693147 [[ -5.67316310e-05]\n",
      " [ -5.65858281e-05]]\n",
      "4300 0.693147 [[ -2.04433331e-06]\n",
      " [ -1.99486908e-06]]\n",
      "4400 0.69315 [[-0.00228882]\n",
      " [-0.0022894 ]]\n",
      "4500 0.693147 [[  1.47153660e-05]\n",
      " [  1.49576208e-05]]\n",
      "4600 0.693147 [[ 0.00012445]\n",
      " [ 0.00012399]]\n",
      "4700 0.693147 [[ 0.00022086]\n",
      " [ 0.00022099]]\n",
      "4800 0.693147 [[ -1.39682527e-07]\n",
      " [  1.53283509e-07]]\n",
      "4900 0.693174 [[ 0.00692271]\n",
      " [ 0.00692386]]\n",
      "5000 0.693147 [[  2.27752917e-05]\n",
      " [  2.23322786e-05]]\n",
      "5100 0.693147 [[  1.81360301e-05]\n",
      " [  1.85855060e-05]]\n",
      "5200 0.693147 [[ -6.06317190e-06]\n",
      " [ -6.13680459e-06]]\n",
      "5300 0.693147 [[  4.08835422e-06]\n",
      " [  4.33496780e-06]]\n",
      "5400 0.693175 [[-0.00697871]\n",
      " [-0.00697922]]\n",
      "5500 0.693147 [[ -5.11336111e-08]\n",
      " [  1.04371793e-07]]\n",
      "5600 0.693147 [[  4.34798767e-06]\n",
      " [  4.37727431e-06]]\n",
      "5700 0.693148 [[-0.00088596]\n",
      " [-0.00088645]]\n",
      "5800 0.693147 [[  6.73160230e-06]\n",
      " [  6.23138840e-06]]\n",
      "5900 0.693157 [[-0.00408772]\n",
      " [-0.00408829]]\n",
      "6000 0.693147 [[ -5.86328097e-05]\n",
      " [ -5.86112110e-05]]\n",
      "6100 0.693147 [[  2.06381355e-06]\n",
      " [  1.93846404e-06]]\n",
      "6200 0.693147 [[-0.00034114]\n",
      " [-0.00034138]]\n",
      "6300 0.693147 [[  3.22572964e-06]\n",
      " [  3.23038694e-06]]\n",
      "6400 0.693154 [[-0.00357191]\n",
      " [-0.00357213]]\n",
      "6500 0.693147 [[  2.96411599e-06]\n",
      " [  2.62023605e-06]]\n",
      "6600 0.693147 [[ -4.17042247e-06]\n",
      " [ -4.33898367e-06]]\n",
      "6700 0.693147 [[ 0.00069908]\n",
      " [ 0.00069929]]\n",
      "6800 0.693147 [[  2.85776969e-06]\n",
      " [  2.71645649e-06]]\n",
      "6900 0.693187 [[ 0.00839558]\n",
      " [ 0.00839578]]\n",
      "7000 0.693147 [[ -7.55563888e-05]\n",
      " [ -7.56256995e-05]]\n",
      "7100 0.693147 [[ -6.58203248e-07]\n",
      " [ -4.08260689e-07]]\n",
      "7200 0.693164 [[ 0.00550021]\n",
      " [ 0.00550012]]\n",
      "7300 0.693147 [[  2.63780057e-05]\n",
      " [  2.63553593e-05]]\n",
      "7400 0.693147 [[ -3.69869031e-06]\n",
      " [ -3.36020003e-06]]\n",
      "7500 0.693147 [[-0.00028096]\n",
      " [-0.0002806 ]]\n",
      "7600 0.693147 [[  5.02277544e-06]\n",
      " [  4.78939182e-06]]\n",
      "7700 0.693147 [[-0.0003259 ]\n",
      " [-0.00032619]]\n",
      "7800 0.693147 [[  2.42841234e-05]\n",
      " [  2.45777483e-05]]\n",
      "7900 0.693147 [[  1.78107553e-06]\n",
      " [  1.43374587e-06]]\n",
      "8000 0.693161 [[-0.00496438]\n",
      " [-0.00496428]]\n",
      "8100 0.693147 [[ -3.09478564e-05]\n",
      " [ -3.09030256e-05]]\n",
      "8200 0.693147 [[  5.33047842e-06]\n",
      " [  5.33191178e-06]]\n",
      "8300 0.693147 [[-0.00030081]\n",
      " [-0.00030047]]\n",
      "8400 0.693147 [[  7.34100695e-07]\n",
      " [  7.22830237e-07]]\n",
      "8500 0.693149 [[-0.00190263]\n",
      " [-0.00190266]]\n",
      "8600 0.693147 [[  4.56818016e-05]\n",
      " [  4.61259806e-05]]\n",
      "8700 0.693147 [[ -1.03776199e-06]\n",
      " [ -1.27335613e-06]]\n",
      "8800 0.693157 [[-0.00413225]\n",
      " [-0.00413194]]\n",
      "8900 0.693147 [[  3.72314462e-05]\n",
      " [  3.71671849e-05]]\n",
      "9000 0.693147 [[ -4.98640566e-06]\n",
      " [ -4.96747907e-06]]\n",
      "9100 0.693148 [[ 0.00080835]\n",
      " [ 0.00080872]]\n",
      "9200 0.693147 [[ -6.63891024e-06]\n",
      " [ -6.60950900e-06]]\n",
      "9300 0.693148 [[ 0.00096553]\n",
      " [ 0.00096605]]\n",
      "9400 0.693147 [[  4.95066524e-05]\n",
      " [  4.93512780e-05]]\n",
      "9500 0.693147 [[  2.31415688e-05]\n",
      " [  2.33698811e-05]]\n",
      "9600 0.693147 [[ 0.00031197]\n",
      " [ 0.00031219]]\n",
      "9700 0.693147 [[  2.79212827e-06]\n",
      " [  2.19208482e-06]]\n",
      "9800 0.693165 [[-0.00570181]\n",
      " [-0.00570188]]\n",
      "9900 0.693147 [[ -2.68286694e-05]\n",
      " [ -2.67607247e-05]]\n",
      "10000 0.693147 [[ -4.48631399e-05]\n",
      " [ -4.49417312e-05]]\n",
      "\n",
      "Hypothesis:  [[ 0.49998879]\n",
      " [ 0.49997759]\n",
      " [ 0.49997759]\n",
      " [ 0.49996632]] \n",
      "Correct:  [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "w = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(x, w) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "cost_summ = tf.summary.scalar('cost', cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "accuracy_summ = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('./logs')\n",
    "    for step in range(10001):\n",
    "        s, _ = sess.run([summary, train], feed_dict={x: x_data, y: y_data})\n",
    "        writer.add_summary(s, global_step=step)\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y: y_data}), sess.run(w))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.20593 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "100 0.696729 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "200 0.694375 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "300 0.693142 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "400 0.692191 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "500 0.69136 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "600 0.690548 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "700 0.689684 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "800 0.68871 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "900 0.687571 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1000 0.686205 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1100 0.684545 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1200 0.682509 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1300 0.680006 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1400 0.67693 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1500 0.673171 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1600 0.668618 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1700 0.663179 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1800 0.656792 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "1900 0.64945 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2000 0.64121 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2100 0.632201 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2200 0.622606 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2300 0.612639 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2400 0.602513 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2500 0.592406 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2600 0.582442 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2700 0.57269 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2800 0.563162 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "2900 0.553836 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3000 0.544673 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3100 0.535631 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3200 0.526679 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3300 0.517786 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3400 0.508911 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3500 0.499989 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3600 0.490911 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3700 0.481498 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3800 0.471458 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "3900 0.460317 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4000 0.447341 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4100 0.431477 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4200 0.411493 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4300 0.386562 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4400 0.357213 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4500 0.325615 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4600 0.294491 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4700 0.265821 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4800 0.240473 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "4900 0.218516 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5000 0.199631 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5100 0.183377 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5200 0.169325 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5300 0.157098 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5400 0.146389 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5500 0.136946 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5600 0.128568 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5700 0.12109 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5800 0.114381 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "5900 0.108331 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6000 0.102852 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6100 0.0978681 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6200 0.093318 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6300 0.0891492 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6400 0.085317 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6500 0.0817837 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6600 0.0785165 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6700 0.0754875 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6800 0.0726722 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "6900 0.0700495 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7000 0.0676008 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7100 0.0653098 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7200 0.0631623 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7300 0.0611454 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7400 0.0592481 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7500 0.0574601 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7600 0.0557725 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7700 0.0541774 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7800 0.0526675 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "7900 0.0512364 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8000 0.0498781 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8100 0.0485874 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8200 0.0473594 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8300 0.0461898 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8400 0.0450747 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8500 0.0440104 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8600 0.0429934 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8700 0.042021 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8800 0.0410902 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "8900 0.0401984 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9000 0.0393433 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9100 0.0385228 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9200 0.0377348 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9300 0.0369775 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9400 0.036249 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9500 0.0355479 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9600 0.0348727 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9700 0.0342219 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9800 0.0335944 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "9900 0.0329888 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "10000 0.0324041 [[ 1.0489831 ]\n",
      " [ 0.25267568]]\n",
      "\n",
      "Hypothesis:  [[ 0.02849933]\n",
      " [ 0.96394414]\n",
      " [ 0.96228963]\n",
      " [ 0.02521791]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,2]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y: y_data}), sess.run(w))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.53137 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "1000 0.000386966 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "2000 0.000102153 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "3000 4.24246e-05 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "4000 2.08768e-05 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "5000 1.11312e-05 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "6000 6.21381e-06 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "7000 3.57629e-06 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "8000 2.07126e-06 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "9000 1.2517e-06 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "10000 7.15256e-07 [[ 1.35799599]\n",
      " [ 0.82612181]]\n",
      "\n",
      "Hypothesis:  [[  4.48362499e-07]\n",
      " [  9.99998927e-01]\n",
      " [  9.99999404e-01]\n",
      " [  7.38842516e-07]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2,10]), name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias')\n",
    "layer1 = tf.sigmoid(tf.matmul(x, w1) + b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10,1]), name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x: x_data, y: y_data})\n",
    "        if step % 1000 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x: x_data, y: y_data}), sess.run(w))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={x: x_data, y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09-2 - Tensorboard (Neural Net for XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 steps of using TensorBoard ##\n",
    "1) From TF graph, decide which rensors you want to log\n",
    "- w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "- cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "2) Merge all summaries\n",
    "- summary = tf.summary.merge_all()\n",
    "\n",
    "3) Create writer and add graph\n",
    "- writer = tf.summary.FileWriter('./logs')\n",
    "- writer.add_graph(sess.graph)\n",
    "\n",
    "4) Run summary merge and add_summary\n",
    "- s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "- writer.add_summary(s, global_step=global_step)\n",
    "\n",
    "5) Launch TensorBoard\n",
    "- tensorboard --logdir=./logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
