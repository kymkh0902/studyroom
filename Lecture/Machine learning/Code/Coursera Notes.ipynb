{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 수업 내용 정리\n",
    "## week1)\n",
    "### <font color=\"blue\">1. Introduction</font>\n",
    " - Two M/L algorithoms : **Supervised learnign / Unsupervised learning**<br>\n",
    "  ex) Supervised learning : **\"right answers\"** given\n",
    " - Regression(회귀) : Predict **Continuos valued output(price)**\n",
    " - Classification : **Discrete valued output(0 or 1)**<br><br>\n",
    "\n",
    "### <font color=\"blue\">2. Linear regression with one variable</font>\n",
    " - **Hypothesis** : $h_{\\theta}(x)=\\theta_{0}+\\theta_{1}x$\n",
    " - Parameters : $\\theta_0,\\theta_1$\n",
    " - **Cost Function** : $J(\\theta_{0},\\theta_{1})=\\frac{1}{2m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})^2$\n",
    " - **Goal** : minimize $J(\\theta_0,\\theta_1)$  and $\\theta_0,\\theta_1$\n",
    " - **Gradient descent algorithm** : $\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$ (for $j=0$ and $j=1$)<br>\n",
    "  *(**Simultaneous Update** = 일시적으로, temp0, temp1을 모두 **일시적**으로 계산해야 한다)*<br>\n",
    "  *(또한, $\\alpha$(learning rate)는 항상 **양수**의 값을 가진다)*<br><br>\n",
    " - $\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum^m_{i=1}(h_\\theta(x^{(i)}-y^{(i)})^2=\\frac{\\partial}{\\partial\\theta_j}\\frac{1}{2m}\\sum^m_{i=1}(\\theta_0+\\theta_1x^{(i)}-y^{(i)})^2$<br><br> \n",
    " - $j=0:\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)}-y^{(i)})$<br><br>\n",
    " - $j=1:\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)}-y^{(i)})$ $\\cdotp x^{(i)}$<br><br>\n",
    " - $\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)}-y^{(i)})$<br>\n",
    " - $\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum^m_{(i=1)}(h_\\theta(x^{(i)}-y^{(i)})$ $\\cdotp x^{(i)}$<br>\n",
    "  **(update $\\theta_0$ and $\\theta_1$ simultaneously)**<br><br>\n",
    "\n",
    "### <font color=\"blue\">3. Linear Algebra review\n",
    " - **Matrix multiplication properties**<br>\n",
    " 1) **not commutative** (교환법칙 성립이 안됨) - $A\\times B\\ne B\\times A$<br>\n",
    " 2) **Associate** (결합법칙) - $A\\times(B\\times C)=(A\\times B)\\times C$<br>\n",
    " 3) **Identity Matrix** - $A\\cdot I=I\\cdot A=A$\n",
    " - **Matrix inverse** : $AA^{-1}=A^{-1}A=I$<br>\n",
    " *(Matrices that don't have an inverse are \"singular(단일형)\" or \"degenerate\")*\n",
    " - **Matrix Transpose** : $B=A^{T}, B_{ij}=A_{ji}$<br>\n",
    " *( ex) if : $A=\\begin{bmatrix}2&3\\cr4&5\\cr6&7\\end{bmatrix}$, then $A^{T}=\\begin{bmatrix}2&4&6\\cr3&5&7\\end{bmatrix}$)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 수업 내용 정리\n",
    "## week2)\n",
    "### <font color=\"blue\">1. Linear Regression with multiple variables - Multiple features</font>\n",
    " - **Multiple features(variables)**<br>\n",
    "$n$ = number of features<br>\n",
    "$x^{i}$ = input(features) of $i^{th}$ training example<br>\n",
    "$x_{j}^{i}$ = value of feature $j$ in $i^{th}$ training example<br>\n",
    " - $h_{\\theta}(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\dotsi+\\theta_nx_n$<br>\n",
    "   (For convenience of notation, define $x_0=1$ -> $x_0^{(i)}=1$)\n",
    "\n",
    "### <font color=\"blue\">2. Linear Regression with multiple variables - Gradient descent for multile variables</font>\n",
    " - **Hypothesis :** $h_\\theta(x)=\\theta^Tx=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+\\dotsi+\\theta_nx_n$<br> \n",
    "     ($x_0=1$, n+1 dimention vector)\n",
    " - **Parameters :** $\\theta_0,\\theta_1,\\dotsi,\\theta_n=\\theta$\n",
    " - **Cost function :** $J(\\theta_0,\\theta_1,\\dotsi,\\theta_n)=J(\\theta)=\\frac{1}{2m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})^2$\n",
    " - **Gradient descent :** Repeat {$\\theta_j:=\\theta_j-\\alpha\\frac{\\alpha}{\\alpha\\theta_j}J(\\theta)$}<br>\n",
    " (simultaneously update for every $j=0,\\dotsi,n$)<br><br>\n",
    " -> $\\theta_j:=\\theta_j-\\alpha\\frac{\\alpha}{\\alpha\\theta_j}J(\\theta)=\\alpha\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$<br><br>\n",
    " -> **Vector로 표현!! 매우 중요!!**$\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ = $\\frac{1}{m}np.sum(X^T(h-y))$<br><br>\n",
    " -> $\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)}$ , ($x_0^{(i)}=1$)<br><br>\n",
    " -> $\\theta_1:=\\theta_1-\\alpha\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x_1^{(i)}$<br><br>\n",
    " -> $\\theta_2:=\\theta_2-\\alpha\\frac{1}{m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x_2^{(i)}$<br><br>\n",
    " -> $\\dotsi$\n",
    "\n",
    "### <font color=\"blue\">3. Linear Regression with multiple variables - Gradient descent in practice 1 : Feature Scaling</font>\n",
    " - **Feature Scaling :** Get every feature into approximately a $-1\\leq x_i\\leq 1$ range<br>\n",
    " (-3 to 3 or $-\\frac{1}{3}$ to $\\frac{1}{3}$ are also possible)\n",
    " - **Mean normalization :** Replace $x_i$ with $x_i-\\mu_i$ to make features have approximately zero mean (Do not apply to $x_0=1$)<br>\n",
    "   ex) $x_1<<\\frac{x_1-\\mu_1}{S_1}$, **$\\mu_1$ = avg value of x, $S_1$ = range(max-min) or standard deviation**\n",
    "\n",
    "### <font color=\"blue\">4. Linear Regression with multiple variables - Gradient descent in practice 2 : Learning rate</font>\n",
    " - **Making sure gradient descent is working cerrectly**<br>\n",
    " -> **For sufficiently small $\\alpha$**, $J(\\theta)$ should decrease on every iteration<br>\n",
    " -> **If $\\alpha$ is too small :** slow convergence<br>\n",
    " -> **If $\\alpha$ is too large :** $J(\\theta)$ may not decrease on every iteration, may not converge, increase<br>\n",
    " -> **To choose $\\alpha$ , try :** $\\dotsi,0,001,0.003,0.01,0.03,0.1,0.3,1,\\dotsi$\n",
    "\n",
    "### <font color=\"blue\">5. Linear Regression with multiple variables - Normal equation</font>\n",
    " - **Normal equation :** Method to solve for $\\theta$ analytically<br>\n",
    " - $\\theta=(X^TX)^{-1}X^Ty$<br>\n",
    " -> if $X=\\left[\\begin{smallmatrix}1 & 2104 & 5 & 1 & 45\\cr 1 & 1416 & 3 & 2 & 40\\cr 1 & 1534 & 3 & 2 & 30\\cr 1 & 852 & 2 & 1 & 36\\cr\\end{smallmatrix}\\right]$ then,  $y=\\left[\\begin{smallmatrix}460\\cr 232\\cr 315\\cr 178\\cr\\end{smallmatrix}\\right]$ (m-dimension vector) and <br>\n",
    " $\\theta$ would be (m+1 dimension vector)\n",
    " - m **training examples, **n **features**<br>\n",
    " -> **Gradient Descent :** 1) Need to choose $\\alpha$  2) Needs many iterations  3) Works well even when $n$ is large (ex) $n=10^6$)<br>\n",
    " -> **Normal Equation :** 1) No need to choose $\\alpha$  2) Don't need to iterate  3) Need to compute $(X^TX)^{-1}$  4) Slow if $n$ is very large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# 수업 내용 정리\n",
    "## week3)\n",
    "### <font color=\"blue\">1. Logistic Regression - Classification</font>\n",
    " - **$y\\in$ {0,1}** / if) 0 : \"Negative Class\", 1 : \"Positive Class\"<br>\n",
    " - **Threshold classifier output $h_\\theta(x)$ at 0.5 : **<br>\n",
    "   If $h_\\theta(x)\\geq0.5$ , predict \"y = 1\"<br>\n",
    "   If $h_\\theta(x)<0.5$ , predict \"y = 0\"<br>\n",
    " - **Decision Boundary : **$h_\\theta(x)=g(\\theta_0+\\theta_1x_1+\\theta_2x_2$)<br>\n",
    "\n",
    "### <font color=\"blue\">2. Logistic Regression - Hypothesis Representation</font>\n",
    " - **Logistic Regression Model :** Want $0\\leq h_\\theta(x)\\leq1$<br> \n",
    " - **Sigmoid function = Logistic function**<br>\n",
    " $h_\\theta(x)=g(\\theta^Tx)$ and $g(z)=\\frac{1}{1+e^{-z}}$ **then** $h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}}$<br>\n",
    " - \"Probability that y=1, given x, parameterized by $\\theta$ \"<br>\n",
    " $P(y=0|x;\\theta)=1-P(y=1|x;\\theta)$\n",
    "\n",
    "### <font color=\"blue\">3. Logistic Regression - Cost function</font>\n",
    " - Cost$(h_\\theta(x),y)=\\frac{1}{2}(h_\\theta(x)-y)^2$  **<-**  $\\frac{1}{1+e^{-\\theta^Tx}}$\n",
    " - **Cost**$(h_\\theta(x),y)$<br>\n",
    " If $y=1$  **->**  $-log(h_\\theta(x))$<br>\n",
    " If $y=0$  **->**  $-log(1-h_\\theta(x))$<br>\n",
    "\n",
    "### <font color=\"blue\">4. Logistic Regression - Simplified cost function and gradient descent</font>\n",
    " - **Logistic regression cost function**<br><br>\n",
    " $J(\\theta)=\\frac{1}{m}\\sum^{m}_{i=1}Cost(h_\\theta(x^{(i)},y^{(i)})$ = $-\\frac{1}{m}[\\sum^{m}_{i=1}y^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$<br><br>\n",
    " - **Gradient Descent**<br><br>\n",
    " $J(\\theta)=-\\frac{1}{m}[\\sum^{m}_{i=1}y^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$<br><br>\n",
    " ->**Want** $min_\\theta J(\\theta)$ : \n",
    " Repeat{$\\theta_j:=\\theta_j-\\alpha\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j$<br>\n",
    " (simultaneously update all $\\theta_j$)\n",
    "\n",
    "### <font color=\"blue\">4. Logistic Regression - Simplified cost function and gradient descent</font>\n",
    " - **Logistic regression cost function**<br><br>\n",
    " $J(\\theta)=\\frac{1}{m}\\sum^{m}_{i=1}Cost(h_\\theta(x^{(i)},y^{(i)})$ = $-\\frac{1}{m}[\\sum^{m}_{i=1}y^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$<br><br>\n",
    " - **Gradient Descent**<br><br>\n",
    " $J(\\theta)=-\\frac{1}{m}[\\sum^{m}_{i=1}y^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$<br><br>\n",
    " ->**Want** $min_\\theta J(\\theta)$ : \n",
    " Repeat{$\\theta_j:=\\theta_j-\\alpha\\frac{1}{m}\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j$}<br>\n",
    " (simultaneously update all $\\theta_j$)\n",
    "\n",
    "### <font color=\"blue\">5. Regularization - The problem of overfitting</font>\n",
    " - **Overfitting : **If we have too many features, the learned hypothesis may fit the training set very well ($J(\\theta)=\\frac{1}{2m}\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2\\approx0$), but fail to generalize to new examples (predict prices on new examples)<br><br>\n",
    " - **Adressing overfitting :**<br>\n",
    " 1)Reduce number of features<br>\n",
    " -Manually select which features to keep<br>\n",
    " -Model selection algorithm (later in course)<br>\n",
    " 2) Regularization<br>\n",
    " -Keep all the features, but reduce magnitude / values of parameters $\\theta_j$<br>\n",
    " -Works well when we have a lot of features, each of which contributes a bit to predicting $y$<br>\n",
    " \n",
    "### <font color=\"blue\">6. Regularization - Regularized linear regression</font>\n",
    " - **Regularized linear regression**<br><br>\n",
    " $J(\\theta)=\\frac{1}{2m}[\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum^{n}_{j=1}\\theta^2_j]$<br><br>\n",
    " - **Gradient descent**<br><br>\n",
    " $\\theta_0:=\\theta_0-\\alpha\\frac{1}{m}\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_0$<br><br>\n",
    " $\\theta_j:=\\theta_j-\\alpha[\\frac{1}{m}\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_j+\\frac{\\lambda}{m}\\theta_j]$<br><br>\n",
    " $\\theta_j:=\\theta_j(1-\\alpha\\frac{\\lambda}{m})-\\alpha\\frac{1}{m}\\sum^{m}_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 수업 내용 정리\n",
    "## week4)\n",
    "### <font color=\"blue\">1. Neural Networks:Representation - Model representation</font>\n",
    "![image](./images/Neural networks1.PNG)\n",
    " - **Neural Network**<br><br>\n",
    " $a^{(j)}_i$ = \"activation of unit $i$ in layer $j$<br>\n",
    " $\\theta^{(j)}$ = matrix of weight controlling function mapping from layer $j$ to layer $j$+1<br><br>\n",
    " $a^{(2)}_1=g(\\theta^{(1)}_{10}x_0+\\theta^{(1)}_{11}x_1+\\theta^{(1)}_{12}x_2+\\theta^{(1)}_{13}x_3)=g(z^{(2)}_1)$<br><br>\n",
    " $a^{(2)}_2=g(\\theta^{(1)}_{20}x_0+\\theta^{(1)}_{21}x_1+\\theta^{(1)}_{22}x_2+\\theta^{(1)}_{23}x_3)=g(z^{(2)}_2)$<br><br>\n",
    " $a^{(2)}_3=g(\\theta^{(1)}_{30}x_0+\\theta^{(1)}_{31}x_1+\\theta^{(1)}_{32}x_2+\\theta^{(1)}_{33}x_3)=g(z^{(2)}_3)$<br><br>\n",
    " $h_\\theta(x)=a^{(3)}_1=g(\\theta^{(2)}_{10}a^{(2)}_0+\\theta^{(2)}_{11}a^{(2)}_1+\\theta^{(2)}_{12}a^{(2)}_2+\\theta^{(2)}_{13}a^{(2)}_3)=g(z^{(3)}_1)$<br><br>\n",
    " ->If network has $s_j$ units in layer $j,s_j+1$ units in layer $j+1$, then $\\theta^{(j)}$ will be of dimension $s_{j+1}\\times(s_j+1)$\n",
    "![image](./images/Neural networks2.PNG)\n",
    " - **Forward propagation : Vectorized implementation**<br><br>\n",
    " $z^{(2)}=\\theta^{(1)}a^{(1)}$<br>\n",
    " $a^{(2)}=g(z^{(2)})$<br><br>\n",
    " **Add $a^{(2)}_0$ = 1**<br>\n",
    " $z^{(3)}=\\theta^{(2)}a^{2)}$<br>\n",
    " $h_\\theta(x)=a^{(3)}=g(z^{(3)})$<br><br>\n",
    " - **Neural Network learning its own features**<br>\n",
    "![image](./images/Neural networks3.PNG)\n",
    " - **Other network architectures**<br>\n",
    "![image](./images/Neural networks4.PNG)<br><br>\n",
    "\n",
    "### <font color=\"blue\">2. Neural Networks:Representation - Examples and intuitions</font>\n",
    " - **Examples of XOR/XNOR/AND/OR function**<br> \n",
    "![image](./images/Neural networks5.PNG)\n",
    "![image](./images/Neural networks6.PNG)\n",
    "![image](./images/Neural networks7.PNG)<br><br>\n",
    "\n",
    "### <font color=\"blue\">3. Neural Networks:Representation - Mulit-class classification</font>\n",
    " - **Multiple output units : One-vs-all**<br>\n",
    "![image](./images/Neural networks8.PNG)<br><br>\n",
    "![image](./images/Neural networks9.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
