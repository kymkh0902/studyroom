{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 4: Neural Networks Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import time\n",
    "import itertools\n",
    "from scipy.special import expit\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Visualizing the data\n",
    "\n",
    "데이터 확인하고 시각화해보자. 다루는 데이터가 어떤 종류인지, 몇차원으로 이루어져 있는지 생각하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 데이터 X, y에 대한 정보와 이것으로 무엇을 할 건지 아래에 서술해보자. (수업 때 맨날 강조하는 내용들!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터를 불러오자. \n",
    "datafile = './data/ex4data1.mat'\n",
    "# scipy에 loadmat이라는 mat파일을 불러오는 함수를 사용한다. \n",
    "mat = scipy.io.loadmat(datafile)\n",
    "\n",
    "# X, y를 지정해준다. \n",
    "X, y = mat['X'], mat['y']\n",
    "\n",
    "## X와 y가 어떤 값들로 이루어져 있는지 확인해보자. \n",
    "#1. X, y의 shape 확인한다. \n",
    "#2. X는 이미지, y는 label 정보이다. y가 몇 가지 class의 label로 이루어져 있는지 확인해보자. (np.unique를 쓰자.)\n",
    "\n",
    "### 코드 시작 ###\n",
    "print ()\n",
    "print\n",
    "print\n",
    "### 코드 끝 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 401)\n"
     ]
    }
   ],
   "source": [
    "# X에 1번째 열에 1을 insert 해주자. \n",
    "### 코드 시작 ###\n",
    "\n",
    "\n",
    "### 코드 끝 ###\n",
    "assert X.shape == (5000, 401), \"X의 shape가 맞지 않습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Visualizing the data\n",
    "\n",
    "X를 이미지로, y를 label로 사용할 것이다. 아래는 각 `class(label)` 에 대한 X값을 이미지로 나타낸 것이다.\n",
    "\n",
    "앞선 **Logistic, Linear** 에서는 2차원 평면에 나타냈고 이번에는 이미지라서 각각 이미지로 나타냈다.\n",
    "\n",
    "아래 코드를 보면서 matplotlib로 image 나타내는 것을 익혀보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAJCCAYAAABppgWCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45051d64d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X, y가 뭐로 이루어져 있는지 확인해보자. \n",
    "# random seed는 계속 뽑을 때마다 무작위로 되는 걸 방지하기 위해서 사용한다. 숫자 별로 무작위 값이 지정되어 있다. \n",
    "np.random.seed(42)\n",
    "# 랜덤으로 그래프 그릴 데이터의 index를 5000개 중에 9개를 뽑는다. \n",
    "random_indices = np.random.choice(5000, 9)\n",
    "# 뽑은 index를 사용해서 9개의 값만 따로 저장해준다.\n",
    "X_sample, y_sample = X[random_indices], y[random_indices]\n",
    "# 3행, 3열로 그래프를 그릴 예정이다. \n",
    "rows, cols = 3, 3\n",
    "# figure를 만들어준다. \n",
    "plt.figure(figsize=(9, 10))\n",
    "for row in range(rows):\n",
    "    for col in range(cols):\n",
    "        order = row * cols + col \n",
    "        # 그래프의 행, 열에 해당하는 정보가 들어가면서 생성된다.  \n",
    "        plt.subplot(rows, cols, order + 1)\n",
    "        # array를 받아서 이미지로 나타내주는 그래프이다. colormap은 gray로 해주어야 흑백이 나온다. \n",
    "        plt.imshow(X_sample[order][1:].reshape([20, 20]).transpose(), cmap='gray')\n",
    "        # 그래프에 맞는 y값을 매칭시켜준다. \n",
    "        plt.title(y_sample[order][0])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Model representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 아래에서 정의하는 theta1, 2에서 각각의 shape값은 어떤 의미를 지니고 있나?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Theta1 has shape :', (25, 401))\n",
      "('Theta2 has shape :', (10, 26))\n"
     ]
    }
   ],
   "source": [
    "# 이미 학습되어 있는 weight 값들을 불러와보자.  \n",
    "datafile = 'data/ex4weights.mat'\n",
    "# 위와 마찬가지로 scipy.io.loadmat으로 불러온다. \n",
    "mat = scipy.io.loadmat(datafile)\n",
    "# 각 layer 별 theta를 theta1, theta2 로 지정해준다. \n",
    "theta1, theta2 = mat['Theta1'], mat['Theta2']\n",
    "\n",
    "# theta의 shape을 확인해보고 shape의 의미를 써보자. \n",
    "### 코드 시작 ###\n",
    "\n",
    "\n",
    "### 코드 끝 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bias를 빼고 우리가 아는 정보들을 모아서 변수에 선언하자. \n",
    "# 모델을 구상한다고 생각하면 된다. \n",
    "input_layer_size = \n",
    "hidden_layer_size = \n",
    "output_layer_size =  \n",
    "n_training_samples = \n",
    "thetas = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Feedforward and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex.3에서 구현한 `forwardpropagate` 함수를 약간 변형해서 사용하자. <br>\n",
    "변형하는 이유는 `Back propagation`을 할 때, `forward propagation` 중간 결과가 필요해서 그렇다. 중간중간 결과를 저장해서 반환해주어\n",
    "`Back propagation`에서 사용할 수 있도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic regression에서 쓴 sigmoid 함수가 또 필요하다. \n",
    "def sigmoid(z):\n",
    "    ### 코드 시작 ### \n",
    "\n",
    "    ### 코드 끝 ###\n",
    "    \n",
    "# forwardpropagate을 하자. \n",
    "# 앞 강의에서 내용과 조금 다른 부분이 있는데 앞에서는 마지막 값 a(h)만 내놓은 반면에 여기에선 Backpropagation을 위해서\n",
    "# 각 layer 별 output을 전부다 모아야 한다. \n",
    "def forwardpropagate(X,thetas):\n",
    "    \"\"\"\n",
    "    X: input 값. (array)\n",
    "    Thetas: layer 별로 필요한 theta 값들. (list)\n",
    "    \n",
    "    X를 받아서 layer 별로 theta와 연산을 진행하면서 나오는 값들을 모두 저장해서 반환해준다.\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # 1. input으로 X를 받는다. \n",
    "    #  -> layer를 지날 때마다 값이 계속 바뀌므로 features에 넣고 loop를 돌리는게 낫다. \n",
    "    # 2. layer가 지날 때마다 weight(theta)와 연산이 된다. (dot product) \n",
    "    # 3. 연산이 이루어진 값은 list에 저장이 된다. (a와 z값 모두)\n",
    "    # 4. 값에 bias를 더한 뒤 마지막 layer가 아닐 경우 다시 2의 과정으로 넘어간다.   \n",
    "    # 5. 마지막 layer일 경우 bias가 더해지지 않으며 sigmoid 함수로 출력값을 0 - 1 사이로 만들어준다. \n",
    "    ##############################################################################\n",
    "    features = X\n",
    "    z_and_a_per_layer = []\n",
    "    ### 코드 시작 ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 코드 끝 ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function 구현\n",
    "\n",
    "아래 코드에서 Cost Function을 구현할 것이다. 강의 자료에 나온 수식을 그대로 구현하면 된다.\n",
    "\n",
    "$ J(\\theta) = - \\frac {1} {m} \\big [ \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)}log\\ h_{\\theta}(x^{(i)})_k + (1 - y_k^{(i)}) log(1 - h_{\\theta}(x^{(i)})_k \\big ] + \\frac {\\lambda} {2m} \\sum_{j=1}^n {\\theta}_j^2 $\n",
    "\n",
    "앞 부분은 항을 하나하나 다 풀어서 썼고 뒷 부분은 안 풀어서 썼는데, 뒷 부분 계산은 그냥 matrix로 해도 딱히 이해에 어려움이 없기 때문이다. <br> \n",
    "(*각 theta를 죄다 제곱을 취한 뒤에 모든 항을 더해주면 된다.*)\n",
    "\n",
    "이제 위에서 앞 식을 풀어야 되는데, 걸리는 부분이 $y$에 해당하는 부분이다. 우리가 데이터로 받은 y의 shape은 (5000, 1)로 식에서 \n",
    "풀려고 하는 (5000, 10) 과는 다른 형태이다. 그래서 y의 형태를 바꿔주는 것이 필요하다. \n",
    "\n",
    "y가 나타내는 정보는 class, 즉 숫자의 값을 나타낸다. <br>\n",
    "형태를 바꿔서 현재 ***class를 나타내는 scalar 값을 0과 1로 이루어진 vector로 바꿔줄 수 있는데 이를 one-hot encoding 이라고 한다.*** 앞 부분 식 계산을 위해서 $y$에 one-hot encoding 을 사용할 것이다. \n",
    "\n",
    "아래에 구현할 때 아래 페이지를 참조해서 해보자. <br>\n",
    "https://stackoverflow.com/questions/29831489/numpy-1-hot-array\n",
    "\n",
    "one-hot encoding 개념 이해가 잘 안 될 경우 아래 페이지를 보자. <br>\n",
    "https://minjejeon.github.io/learningstock/2017/06/05/easy-one-hot-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y를 one-hot 형태로 만들어주기 \n",
    "# 1. 먼저 (5000, 10) shape의 0으로 이루어진 matrix를 만들어준다. np.zeros 사용\n",
    "y_onehot = \n",
    "# 2. y의 값이 해당하는 index 위치만 1로 변경해준다. np.arange, reshape 사용. 위에 첨부한 웹페이지 참조하자. \n",
    "y_onehot[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***cost function을 2가지 형태로 구현 할 예정이다. ***\n",
    "1. 완전 low level로, theta는 냅두고 matrix 각 항 별로 모든 값을 구해서 다 더하는 방식을 취한다.\n",
    "2. vectorize 방식을 취한다.\n",
    "\n",
    "--> 두 가지 방식으로 구현을 한 뒤에 속도 비교도 해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# low level (for loop)\n",
    "def costfunction_vector(thetas, X, y_onehot, lambda_param=0):   \n",
    "    \"\"\"\n",
    "    thetas: layer 별로 필요한 theta값들 (list)\n",
    "    X: input 값 (array)\n",
    "    y_onehot: label을 onehot으로 변환한 값 (array)\n",
    "    lambda_param: regularizer parameter 값\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # cost function을 수식적으로 정의했으므로 항들을 매칭시켜서 코드로 나타내주면 된다. \n",
    "    # 1. m(데이터 총 개수)를 변수로 선언하자. \n",
    "    # 2. 전체 cost_total를 cost_front(h, y에 의한 항), cost_back(regularizer에 의한 항)으로 나눌 수 있다. \n",
    "    # 3. cost_back을 계산할 때는 theta가 2개라는 것을 유념하자.\n",
    "    # 4. 나머진 계산하면 된다. \n",
    "    ##############################################################################\n",
    "    \n",
    "    ### 코드 시작 ###\n",
    "    m = \n",
    "    h_x = \n",
    "    cost_front = \n",
    "    cost_back =  \n",
    "    ### 코드 끝 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector\n",
    "def costfunction_low(thetas, X, y_onehot, lambda_param=0):   \n",
    "    \"\"\"\n",
    "    thetas: layer 별로 필요한 theta값들 (list)\n",
    "    X: input 값 (array)\n",
    "    y_onehot: label을 onehot으로 변환한 값 (array)\n",
    "    lambda_param: regularizer parameter 값\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # cost function을 수식적으로 정의했으므로 항들을 매칭시켜서 코드로 나타내주면 된다. \n",
    "    # 위와 똑같은 내용이며 low level, 즉 matrix의 행,열 별로 전부다 for문을 돌려서 구한 값이다. \n",
    "    # 이해를 돕기 위해서 아래에 나타냈으니 참고만 하자. \n",
    "    ##############################################################################\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    h_x = forwardpropagate(X, thetas)[-1][-1]\n",
    "    cost_front = 0\n",
    "    cost_back = 0\n",
    "    for i in range(n_training_samples):\n",
    "        for k in range(output_layer_size):\n",
    "            cost_ik = - (1.0/m) * (y_onehot[i, k] * np.log(h_x[i, k]) + (1 - y_onehot[i, k]) * np.log(1 - h_x[i, k])) \n",
    "            cost_front += cost_ik\n",
    "    for theta in thetas:\n",
    "        cost_theta = 0.5 * lambda_param * np.sum(np.square(theta)) / m \n",
    "        cost_back += cost_theta\n",
    "    cost_total = cost_front + cost_back\n",
    "    return cost_total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** 위에서 구현한 cost function을 각각 실행시켜서 vectorize했을 때 시간이 얼마나 단축되는지 확인하자. *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n",
      "Running time for cost with for loop : 0.19s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(costfunction_low(thetas, X, y_onehot, 0))\n",
    "end = time.time()\n",
    "print(\"Running time for cost with for loop : {:.2f}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.287629165161\n",
      "Running time for cost with vector : 0.02s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cost = costfunction_vector(thetas, X, y_onehot, 0)\n",
    "print(cost)\n",
    "end = time.time()\n",
    "print(\"Running time for cost with vector : {:.2f}s\".format(end - start))\n",
    "print(\"정답 cost와의 차이 : {}\".format(rel_error(cost, 0.287629)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Regularized cost function\n",
    "\n",
    "Regularizer에 사용되는 lambda 값을 조정해서 cost function이 어떻게 되는지 확인해보자. \n",
    "\n",
    "**Q. Cost function은 어떻게 되나? 여기서 Regularizer의 역할은 뭘까?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.384487796243\n"
     ]
    }
   ],
   "source": [
    "# lambda 값을 1로 놓고 cost function을 구해보자. \n",
    "cost_regularized = costfunction_vector(thetas, X, y_onehot, 1)\n",
    "print(cost_regularized)\n",
    "print(\"정답 cost와의 차이 : {}\".format(rel_error(cost_regularized, 0.384487)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "**목적 : chain rule을 통해서 각 layer내 unit별로 모든 theta값의 gradient를 구해야 한다. **\n",
    "\n",
    "chain rule만 잘 사용하면 된다. 각각의 unit들의 연결은 식으로 나타나지고 하나의 식은 곧 바로 gradient를 구할 수 있다.\n",
    "\n",
    "문제가 되는 부분은 여러 개의 식을 지났을 때인데, 하나의 식에 대한 gradient를 알면 chain rule을 통해서 구할 수 있다. \n",
    "\n",
    "이 부분에 대해서만 명확하게 이해하면 나머지는 별거 없는 알고리즘이다. \n",
    "\n",
    "내용에 대해서 아래 사이트를 보는 것을 추천한다. 강의 노트 내용은 notation도 너무 많고 수학적이라 이해가 잘 안된다. \n",
    "\n",
    "http://aikorea.org/cs231n/optimization-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1 Sigmoid gradient\n",
    "\n",
    "sigmoid의 경우는 exp 항이 들어가서 gradient를 구하는데 어려움이 있다. 굳이 전개 과정을 알 필요는 없고 \n",
    "***어떤 식이 sigmoid의 gradient를 나타내는지만 알면 된다.***\n",
    "\n",
    "아래 z에 대한 sigmoid 함수의 gradient 식을 참고하자. <br>\n",
    "$\\frac \\partial {\\partial z} \\sigma(z) = \\sigma(z) (1- \\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid의 gradient를 구하자. 별거 없다 수식 그대로 나타내면 된다. \n",
    "def sigmoid_gradient(z):\n",
    "    ### 코드 시작 ###\n",
    "    \n",
    "    \n",
    "    ### 코드 끝 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Random initialization\n",
    "\n",
    "theta를 기존에는 0의 값들로 이루어진 matrix를 사용했는데 neural network에서 그렇게 사용할 경우에 초기에 학습이 잘 되지 않는다. \n",
    "(***왜 그런지 생각해보자.***) <br> 그래서 0에 가까운 작은 값들로 초기화를 시켜준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_init_theta(eps=0.12):\n",
    "    theta1 = np.random.rand(hidden_layer_size, input_layer_size + 1) * 2 * eps - eps\n",
    "    theta2 = np.random.rand(output_layer_size, hidden_layer_size + 1) * 2 * eps - eps \n",
    "    random_thetas = [theta1, theta2]\n",
    "    return random_thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Backpropagation\n",
    "\n",
    "Neural Network에서 배우는 개념 중에 가장 중요하다. 명확하게 이해하도록 하자. <br>\n",
    "표기법과 설명은 아래 그림을 참고해서 하도록 하겠다. 그리고 직관적인 이해를 위해 표기들은 전부다 대문자 Matrix로 나타내겠다. \n",
    "\n",
    "![NN](../../../../pictures/neural_network.png)\n",
    "\n",
    "- 목표 : $J(\\Theta)$에 대한 $\\Theta^{(1)}, \\Theta^{(2)}$ ***각각의 gradient를 구하기.*** ($\\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(1)}}, \\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(2)}}$) <br><br>\n",
    "- Forward 식 : 식이 진행되는 순서대로 쭉 나열해보자. <br>\n",
    "    1) $ Z^2 = \\Theta^{(1)} \\cdot X$ <br>\n",
    "    2) $ A^2 = g(Z^2)$ <br>\n",
    "    3) $ Z^3 = \\Theta^{(2)} \\cdot A^2$ <br>\n",
    "    4) $ H_{\\Theta}(X) = g(Z^3)$ <br>\n",
    "    5) $ J(\\Theta) = -\\frac 1 m \\big [\\sum Y{log}H_{\\Theta}(X) + (1 - Y){log}(1-H_{\\Theta}(X)) \\big] + \\frac \\lambda {2m} \\sum \\Theta^2$ <br> <br>\n",
    "- Backward 식 : 거꾸로 바로 이전 식에 대한 항들의 gradient를 구해보자. <br>\n",
    "    1) $\\frac {\\partial Z^2} {\\partial \\Theta^{(1)}} = \\frac \\partial {\\partial \\Theta^{(1)}} (\\Theta^{(1)} \\cdot X$) <br>\n",
    "    2) $\\frac {\\partial A^2} {\\partial Z^2} = \\frac \\partial {\\partial Z^2} (g(Z^2))$ <br>\n",
    "    3) $\\frac {\\partial Z^3} {\\partial A^2} = \\frac \\partial {\\partial A^2} (\\Theta^{(2)} \\cdot A^2)$ <br>\n",
    "    4) $\\frac {\\partial Z^3} {\\partial \\Theta^{(2)}} = \\frac \\partial {\\partial \\Theta^{(2)}} (\\Theta^{(2)} \\cdot A^2)$ <br>\n",
    "    5) $\\frac {\\partial H_{\\Theta}(X)} {\\partial Z^3} = \\frac \\partial {\\partial Z^3} (g(Z^3))$ <br>\n",
    "    6) $\\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} = \\frac \\partial {\\partial H_{Theta}(X)} (J(\\Theta))$ <br>\n",
    "    \n",
    "위 식들을 이용해서 구해야 하는 목표 식 2개를 풀어서 나타내보도록 하자. <br>\n",
    "*** 아래와 같이 식을 풀어서 나타낼 수 있고 위에서 계산한 Backward 1~5를 활용하면 목표 1, 2를 구할 수 있다는 것을 알았다. \n",
    "1~5를 구하는 것에 어려움은 sigmoid에 대한 gradient 밖에 없고 이는 위(2.1)에서 해결했으므로 각각의 gradient를 구현하고 아래 식에 대입하면 되겠다. ***\n",
    "\n",
    "### 목표 1 : $\\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial A^2} \\frac {\\partial A^2} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial A^2} \\frac {\\partial A^2} {\\partial Z^2} \\frac {\\partial Z^2} {\\partial \\Theta^{(1)}}$\n",
    "\n",
    "### 목표 2 : $\\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(2)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial \\Theta^{(2)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial \\Theta^{(2)}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate(thetas, X, y_onehot, lambda_param=0.):\n",
    "   \"\"\"\n",
    "    thetas: layer 별로 필요한 theta값들 (list)\n",
    "    X: input 값 (array)\n",
    "    y_onehot: label을 onehot으로 변환한 값 (array)\n",
    "    lambda_param: regularizer parameter 값\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # chain rule을 이용해서 각 수식이 진행될 때 마다 각각의 gradient를 구한 뒤\n",
    "    # 이들을 활용해서 우리의 목적인 dJ_dtheta를 구한다. \n",
    "    ##############################################################################\n",
    "    \n",
    "    # m(데이터 개수)를 선언하자. \n",
    "    m = \n",
    "    # theta를 선언하자. \n",
    "    theta1, theta2 = \n",
    "    # forward 값에서 구한 값을 통해서 Z2, A2, Z3, H를 선언하자. \n",
    "    z_and_a_per_layer = \n",
    "    Z2 = \n",
    "    A2 = \n",
    "    Z3 = \n",
    "    H = \n",
    "\n",
    "    # Back propagation 수식 그대로 구현하기\n",
    "    dJ_dH = \n",
    "    dH_dZ3 = \n",
    "    dJ_dZ3 = \n",
    "    dA2_dZ2 = \n",
    "    dJ_dZ2 = \n",
    "    \n",
    "    # A2에 추가되는 bias의 영향력을 따져야 한다. 언제 bias가 영향을 미치는 지, 안 미치는 지 생각해보자. \n",
    "    A2 = \n",
    "    dJ_dtheta1 = \n",
    "    dJ_dtheta2 = \n",
    "    \n",
    "    # 이름을 간단하게 바꿔주자\n",
    "    dtheta1 = \n",
    "    dtheta2 = \n",
    "\n",
    "    # regularization에 의한 gradient를 구할 떄 theta의 bias에 영향을 주는 부분은 무시하자. \n",
    "    dtheta1[:, 1:] = \n",
    "    dtheta2[:, 1:] = \n",
    "    \n",
    "    dthetas = [dtheta1, dtheta2]\n",
    "    \n",
    "    return dthetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtheta1, dtheta2 = backpropagate(thetas, X, y_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Gradient checking\n",
    "\n",
    "내가 Backpropagation으로 구한 Gradient가 맞게 구한건지 확인하는 작업이다. (**analytic gradient**) <br>\n",
    "확인은 $f'(x) = \\displaystyle\\lim_{h \\to 0} \\frac {f(x+h) - f(x)} {h}$ 식을 사용한 **numerical gradient** 를 이용한다. \n",
    "\n",
    "- Analytic gradient : 수식적으로 gradient를 구하는 방법. (예시: $\\frac \\partial {\\partial x} (4x) = 4$)\n",
    "- Numerical gradient : 위 식 참조. 아주 가까운 두 점을 잡고 기울기를 구하면 gradient를 근사해서 구할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\" \n",
    "    a naive implementation of numerical gradient of f at x \n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\" \n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 위 코드는 Gradient를 확인하는 코드이다. 아래에 gradient를 확인할 때에는 5000개 중에 10개만 sampling으로 뽑아서 진행했다. <br>\n",
    "전체 5000개를 진행하면 엄청 속도가 느릴 것이다 왜 그럴까? ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1 gradient 차이 : 0.000426020638045\n",
      "theta2 gradient 차이 : 1.86969328277e-07\n"
     ]
    }
   ],
   "source": [
    "# theta1 gradient 확인\n",
    "random_indices = np.random.choice(5000, 10)\n",
    "sampled_X = X[random_indices]\n",
    "sampled_y = y_onehot[random_indices]\n",
    "\n",
    "dtheta1, dtheta2 = backpropagate(thetas, sampled_X, sampled_y)\n",
    "\n",
    "f1 = lambda th : costfunction_vector([th, theta2], sampled_X, sampled_y)\n",
    "theta1_grad_num = eval_numerical_gradient(f1, theta1, verbose=False)\n",
    "print(\"theta1 gradient 차이 : {}\".format(rel_error(dtheta1, theta1_grad_num)))\n",
    "\n",
    "# theta2 gradient 확인\n",
    "f2 = lambda th : costfunction_vector([theta1, th], sampled_X, sampled_y)\n",
    "theta2_grad_num = eval_numerical_gradient(f2, theta2, verbose=False)\n",
    "print(\"theta2 gradient 차이 : {}\".format(rel_error(dtheta2, theta2_grad_num)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.5 Learning parameters \n",
    "\n",
    "이제 위에서 구현한 코드들을 활용해서 학습을 시켜보자. 순서는 아래와 같다.\n",
    "\n",
    "1) theta 초기화 <br> \n",
    "2) iteration수 만큼 back propagation, theta update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y_onehot, num_iterations, learning_rate=0.001, lambda_param=0.):\n",
    "    \"\"\"\n",
    "    X: inputs (array)\n",
    "    y_onehot: label을 onehot으로 변환한 값 (array)\n",
    "    num_iterations: training 학습시킬 반복 횟수 \n",
    "    learning_rate: 학습 속도\n",
    "    lambda_param: regularizer parameter 값\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # 전반적인 학습 과정은 전부다 비슷비슷하다. \n",
    "    # 필요한 도구들 : 학습 데이터(X, y), hyperparameter들(learning_rate, iterations ...)\n",
    "    # 학습 방법 : cost를 정의한다 -> gradient를 구한다 -> 현재 theta에서 값을 뺀다. (반복)\n",
    "    #\n",
    "    # 1. theta를 초기화한다. \n",
    "    # 2. dtheta1, dtheta2를 구한다. \n",
    "    # 3. 현재 theta에서 gradient를 빼준다. \n",
    "    # 1~3의 과정을 iteration수 만큼 loop 돌려준다. \n",
    "    # 최종적으로 theta 값을 얻게 된다. \n",
    "    # -> 추가로, 학습이 잘 되는지 확인을 위해서 cost도 누적해서 구한 뒤 반환해주자!\n",
    "    ##############################################################################\n",
    "    \n",
    "    theta1, theta2 = \n",
    "    cost_history = []\n",
    "    ### 코드 시작 ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### 코드 끝 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1만번이나 학습을 해야한다니 시간이 1~2분 정도 걸릴 것이다.\n",
    "start = time.time()\n",
    "final_thetas, cost_history = train(X, y_onehot, 10000, 0.1)\n",
    "end = time.time()\n",
    "\n",
    "print(\"total training time : {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. 위에서 주어진 theta를 이용해서 구한 cost와 아래 우리가 구한 값이랑 비교해보자 어떤게 더 큰가? 학습으로 구한 theta 값이 더 성능이 안 좋다면 어떻게 해야 성능을 높일 수 있을까? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34433083943180959"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 cost 확인\n",
    "print(\"최종 cost : {}\".format(cost_history[-1]))\n",
    "\n",
    "# 학습 결과 (cost)\n",
    "plt.plot(cost_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이제 구한 theta와 기존 데이터를 통해서 예측해보자. \n",
    "def predict(X, thetas):\n",
    "    ##############################################################################\n",
    "    # 1) H를 구해야 하고 \n",
    "    # 2) H를 수정해서 prediction값을 구하면 된다. \n",
    "    # prediction값을 구할 때 argmax를 사용해야 하고, class에 맞게 조금 조정이 있어야 한다. \n",
    "    # shape도 손봐야 한다면 reshape!\n",
    "    ##############################################################################\n",
    "    \n",
    "    output = \n",
    "    pred = \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred, correct, accurcy를 구해보자. \n",
    "pred = \n",
    "correct = \n",
    "accuracy = \n",
    "print(\"학습한 theta로 예측했을 경우 accuracy : {}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularizer 있을 때 theta, 성능도 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_thetas_regularizer, cost_history_regularizer = train(X, y_onehot, 10000, 0.1, 10.)\n",
    "computeAccuracy(X,learned_regularized_Thetas,y)\n",
    "pred = \n",
    "correct = \n",
    "accuracy = \n",
    "print(\"lambda가 10일 때 학습한 theta로 예측했을 경우 accuracy : {}%\".format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
