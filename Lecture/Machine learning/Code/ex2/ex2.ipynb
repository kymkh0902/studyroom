{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> 찾아볼 함수 </font>\n",
    "- np.where\n",
    "- np.ndarray.astype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex2data1.txt'\n",
    "##############################################################################\n",
    "# np.loadtxt를 사용해서 datafile을 불러오기.                                 \n",
    "# 조건 :구분자는 \",\" 이고 데이터 범위는 column 기준 0, 1, 2번째, unpack : True   \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ### \n",
    "cols = \n",
    "### 코드 끝 ###\n",
    "assert cols.shape == (3, 100), \"불러온 데이터의 shape가 맞지 않음.\" \n",
    "\n",
    "##############################################################################\n",
    "# X, y를 적절한 형태로 바꾼다. m x n(m: 데이터 개수, n: feature 개수)\n",
    "# transpose 나 reshape 사용하면 됨. \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ### \n",
    "X = \n",
    "y = \n",
    "m = # 데이터 개수 m으로 저장\n",
    "\n",
    "### 코드 끝 ###\n",
    "assert X.shape == (100, 2) and y.shape == (100, 1), \"X 혹은 y의 shape가 맞지 않음.\"\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# X vector에 값이 1로 이루어진 1개 열을 추가해서 matrix로 만들어준다.              \n",
    "# 강의에서 bias(theta0) 값 X에 넣어주는 연산임.                                \n",
    "# np.insert 사용.                                                        \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ###\n",
    "\n",
    "### 코드 끝 ###\n",
    "assert X.shape == (100, 3), \"X의 shape가 맞지 않음\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# X를 positive/negative 데이터로 각각 나누어주자. \n",
    "# positive는 y==1일 때의 index에 해당하는 X의 값\n",
    "# negative는 y==0일 때의 index에 해당하는 X의 값이다.                                                            \n",
    "# np.where를 사용하면 된다. \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ###\n",
    "pos = \n",
    "neg = \n",
    "### 코드 끝 ###\n",
    "assert pos.shape == (60, 3) and neg.shape == (40, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos, neg 데이터를 scatter 그래프로 나타내보자. \n",
    "def plotData():\n",
    "    plt.plot(pos[:,1],pos[:,2],'k+',label='Admitted')\n",
    "    plt.plot(neg[:,1],neg[:,2],'yo',label='Not admitted')\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "plotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# sigmoid 함수를 만들어보자. \n",
    "# 수식을 보고 작성하면 된다. \n",
    "##############################################################################\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    \"\"\"\n",
    "    ### 코드 시작 ###    \n",
    "\n",
    "    ### 코드 끝 ###\n",
    "print(\"x=1일 때 sigmoid 계산 값 차이 : \", rel_error(sigmoid(1), 0.73105857))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid 그래프로 나타내기 \n",
    "def plotsigmoid():\n",
    "    x = np.arange(-10,10,.1)\n",
    "    plt.plot(x, sigmoid(x))\n",
    "    plt.title('Sigmoid function')\n",
    "    plt.show()\n",
    "    plt.grid()\n",
    "\n",
    "    x = np.arange(-10,10,.1)\n",
    "    plt.plot(x, sigmoid(x))\n",
    "    plt.title('Sigmoid function')\n",
    "    plt.show()\n",
    "    plt.grid()plotsigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(theta,X): \n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # logistic function에서의 h식을 만들어보자. \n",
    "    # 위에서 선언한 sigmoid 함수를 활용하자 \n",
    "    ##############################################################################\n",
    "    \n",
    "    ### 코드 시작 ###\n",
    "\n",
    "    ### 코드 끝 ###\n",
    "\n",
    "def computeCost(theta, X, y): \n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    y : y 데이터, shape=(m, 1), (array)\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # Cost 함수를 구해보자. \n",
    "    # 강의에서 배운 수식을 2개로 쪼개서 앞 쪽, 뒤 쪽을 numpy로 구현한 뒤에 합치면 된다. \n",
    "    # np.dot, np.sum, transpose, 위에서 정의한 h식을 섞어서 사용하면 된다. \n",
    "    ##############################################################################\n",
    "    \n",
    "    ### 코드 시작 ###\n",
    "    \n",
    "    ### 코드 끝 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 임의로 정한 theta 값\n",
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "# cost 계산 \n",
    "computeCost(initial_theta,X,y)\n",
    "\n",
    "print(\"Cost 값과 정답과의 차이 : \", rel_error(computeCost(initial_theta, X, y), 0.6931471))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization \n",
    "\n",
    "gradient descent가 그렇듯 아래 식을 풀어서 optimization을 할 예정이다. <br>\n",
    "$\\theta_j := \\theta_j - \\alpha {\\frac \\partial {\\partial \\theta_j}} J(\\theta_0, \\theta_1)$ 식을 똑같이 미분해서 구하면 되는데 음... 조금 어려울 수도 있다. \n",
    "\n",
    "Cost function을 보면 아래와 같고, <br>\n",
    "$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m \\left[y^i\\theta x^i-\\theta x^i-\\log(1+e^{-\\theta x^i})\\right]=-\\frac{1}{m}\\sum_{i=1}^m \\left[y^i\\theta x^i-\\log(1+e^{\\theta x^i})\\right] ~~(1)$ <br>\n",
    "\n",
    "위 식을 적당히 미분하면(*일단 패스하자*) 아래 결과를 얻을 수 있다. <br>\n",
    "$\\frac{\\partial}{\\partial \\theta_j}y^i\\theta x^i=y^ix^i_j ~~(2) $ <br>\n",
    "$\\frac{\\partial}{\\partial \\theta_j}\\log(1+e^{\\theta x^i})=\\frac{x^i_je^{\\theta x^i}}{1+e^{\\theta x^i}}=x^i_jh_\\theta(x^i)~~(3)$ <br>\n",
    "\n",
    "전개 과정은 생략하고 1번 식인 $J(\\theta)$ 내부 식을 $\\theta$에 대한 derivative를 구하면 (2), (3) 번 식을 유도할 수 있다.\n",
    "\n",
    "그럼, $\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big [ y^ix^i_j + x^i_jh_\\theta(x^i) \\big ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iteration 10만번 예정이다. \n",
    "iterations = 100000\n",
    "# learning_rate는 0.001\n",
    "alpha = 0.001\n",
    "\n",
    "def descendGradient(X, theta):\n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    \"\"\"\n",
    "    theta_history = []  # theta 변화 과정 저장 \n",
    "    cost_history = []  # cost 변화 과정 저장 \n",
    "    ##############################################################################\n",
    "    # Gradient descent를 하자. \n",
    "    # 1) 위에서 혹은 강의에서 배운 수식을 gradient로 구현한다. \n",
    "    # 2) iteration 만큼 theta를 update 시켜준다. \n",
    "    # 3) 매번 업데이트 시켜줬을 때 cost, theta 값을 위에 cost_history, theta_history에 각각 저장해준다. \n",
    "    # 4) iterations 만큼 update 시켜준 뒤에 theta, cost_history, theta_history를 return 한다. \n",
    "    # linear regression에서 구현한 식이랑 거의 비슷하다. \n",
    "    ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient descent 실행\n",
    "theta, theta_history, cost_history = descendGradient(X, initial_theta)\n",
    "true_theta = np.array([[-4.81180027],\n",
    "                       [0.04528064],\n",
    "                       [0.03819149]])\n",
    "print(\"theta 값과 정답과의 차이 : \", rel_error(theta, true_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예측 함수 구현해보자. 우리가 구한 theta를 통해서 X를 넣어줬을 때 얼마나 y가 맞게 나오는 지 확인하기 위함이다. \n",
    "def predict(theta, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    threshold : y 결정하는 기준 값. (float) \n",
    "    threshold는 1에 가까이 두면 positive에 penalty를 주는 것이고 0에 가까이두면 negative에 penalty를 주는 것이다. \n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # theta, X를 이용해서 예측을 해보고 값이 threshold 보다 큰 경우를 1 작은 경우를 0으로 return 하자. \n",
    "    # astype 이라는 함수를 새롭게 사용하게 될 것이다. \n",
    "    ##############################################################################\n",
    "    pred = h(theta, X) >= threshold\n",
    "\n",
    "    return (pred.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction 값을 구해보자. \n",
    "pred = predict(theta, X) \n",
    "correct = np.sum(pred == y) / len(pred)\n",
    "print('correct 값과 정답과의 차이 : ', rel_error(correct, 0.91))\n",
    "print('Train accuracy {}%'.format(100 * correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습에 따른 cost 감소 그래프를 보자.\n",
    "plt.plot(cost_history)\n",
    "plt.title('cost history')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 동일한 내용을 scipy로 구현한 예제이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#An alternative to OCTAVE's 'fminunc' we'll use some scipy.optimize function, \"fmin\"\n",
    "#Note \"fmin\" does not need to be told explicitly the derivative terms\n",
    "#It only needs the cost function, and it minimizes with the \"downhill simplex algorithm.\"\n",
    "#http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.optimize.fmin.html\n",
    "from scipy import optimize\n",
    "\n",
    "def optimizeTheta(mytheta,myX,myy,mylambda=0.):\n",
    "    result = optimize.fmin(computeCost, x0=mytheta, args=(myX, myy, mylambda), maxiter=400, full_output=True)\n",
    "    return result[0], result[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta, mincost = optimizeTheta(initial_theta,X,y)\n",
    "#That's pretty cool. Black boxes ftw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"Call your costFunction function using the optimal parameters of θ. \n",
    "#You should see that the cost is about 0.203.\"\n",
    "print computeCost(theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting the decision boundary: two points, draw a line between\n",
    "#Decision boundary occurs when h = 0, or when\n",
    "#theta0 + theta1*x1 + theta2*x2 = 0\n",
    "#y=mx+b is replaced by x2 = (-1/thetheta2)(theta0 + theta1*x1)\n",
    "\n",
    "boundary_xs = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
    "boundary_ys = (-1./theta[2])*(theta[0] + theta[1]*boundary_xs)\n",
    "plotData()\n",
    "plt.plot(boundary_xs,boundary_ys,'b-',label='Decision Boundary')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For a student with an Exam 1 score of 45 and an Exam 2 score of 85, \n",
    "#you should expect to see an admission probability of 0.776.\n",
    "print h(theta,np.array([1, 45.,85.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scipy에서 나오는 theta는 rank가 1이라서 2로 바꿔준다. \n",
    "theta = theta.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(theta, X) \n",
    "correct = np.sum(pred == y) / len(pred)\n",
    "print('Train accuracy {}%'.format(100 * correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일단 여기까지 수정함. \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile = 'data/ex2data2.txt'\n",
    "#!head $datafile\n",
    "cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1,2),unpack=True) #Read in comma separated data\n",
    "##Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size # number of training examples\n",
    "##Insert the usual column of 1's into the \"X\" matrix\n",
    "X = np.insert(X,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide the sample into two: ones with positive classification, one with null classification\n",
    "pos = np.array([X[i] for i in xrange(X.shape[0]) if y[i] == 1])\n",
    "neg = np.array([X[i] for i in xrange(X.shape[0]) if y[i] == 0])\n",
    "#Check to make sure I included all entries\n",
    "#print \"Included everything? \",(len(pos)+len(neg) == X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotData():\n",
    "    plt.plot(pos[:,1],pos[:,2],'k+',label='y=1')\n",
    "    plt.plot(neg[:,1],neg[:,2],'yo',label='y=0')\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "#Draw it square to emphasize circular features\n",
    "plt.figure(figsize=(6,6))\n",
    "plotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Feature mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code I took from someone else (the OCTAVE equivalent was provided in the HW)\n",
    "def mapFeature( x1col, x2col ):\n",
    "    \"\"\" \n",
    "    Function that takes in a column of n- x1's, a column of n- x2s, and builds\n",
    "    a n- x 28-dim matrix of featuers as described in the homework assignment\n",
    "    \"\"\"\n",
    "    degrees = 6\n",
    "    out = np.ones( (x1col.shape[0], 1) )\n",
    "\n",
    "    for i in range(1, degrees+1):\n",
    "        for j in range(0, i+1):\n",
    "            term1 = x1col ** (i-j)\n",
    "            term2 = x2col ** (j)\n",
    "            term  = (term1 * term2).reshape( term1.shape[0], 1 ) \n",
    "            out   = np.hstack(( out, term ))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature-mapped X matrix\n",
    "mappedX = mapFeature(X[:,1],X[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cost function is the same as the one implemented above, as I included the regularization\n",
    "#toggled off for default function call (lambda = 0)\n",
    "#I do not need separate implementation of the derivative term of the cost function\n",
    "#Because the scipy optimization function I'm using only needs the cost function itself\n",
    "#Let's check that the cost function returns a cost of 0.693 with zeros for initial theta,\n",
    "#and regularized x values\n",
    "initial_theta = np.zeros((mappedX.shape[1],1))\n",
    "computeCost(initial_theta,mappedX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Learning parameters using fminunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I noticed that fmin wasn't converging (passing max # of iterations)\n",
    "#so let's use minimize instead\n",
    "\n",
    "def optimizeRegularizedTheta(mytheta,myX,myy,mylambda=0.):\n",
    "    result = optimize.minimize(computeCost, mytheta, args=(myX, myy, mylambda),  method='BFGS', options={\"maxiter\":500, \"disp\":False} )\n",
    "    return np.array([result.x]), result.fun\n",
    "    \n",
    "theta, mincost = optimizeRegularizedTheta(initial_theta,mappedX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotBoundary(mytheta, myX, myy, mylambda=0.):\n",
    "    \"\"\"\n",
    "    Function to plot the decision boundary for arbitrary theta, X, y, lambda value\n",
    "    Inside of this function is feature mapping, and the minimization routine.\n",
    "    It works by making a grid of x1 (\"xvals\") and x2 (\"yvals\") points,\n",
    "    And for each, computing whether the hypothesis classifies that point as\n",
    "    True or False. Then, a contour is drawn with a built-in pyplot function.\n",
    "    \"\"\"\n",
    "    theta, mincost = optimizeRegularizedTheta(mytheta,myX,myy,mylambda)\n",
    "    xvals = np.linspace(-1,1.5,50)\n",
    "    yvals = np.linspace(-1,1.5,50)\n",
    "    zvals = np.zeros((len(xvals),len(yvals)))\n",
    "    for i in xrange(len(xvals)):\n",
    "        for j in xrange(len(yvals)):\n",
    "            myfeaturesij = mapFeature(np.array([xvals[i]]),np.array([yvals[j]]))\n",
    "            zvals[i][j] = np.dot(theta,myfeaturesij.T)\n",
    "    zvals = zvals.transpose()\n",
    "\n",
    "    u, v = np.meshgrid( xvals, yvals )\n",
    "    mycontour = plt.contour( xvals, yvals, zvals, [0])\n",
    "    #Kind of a hacky way to display a text on top of the decision boundary\n",
    "    myfmt = { 0:'Lambda = %d'%mylambda}\n",
    "    plt.clabel(mycontour, inline=1, fontsize=15, fmt=myfmt)\n",
    "    plt.title(\"Decision Boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a figure showing contours for various values of regularization parameter, lambda\n",
    "#It shows for lambda=0 we are overfitting, and for lambda=100 we are underfitting\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.subplot(221)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,0.)\n",
    "\n",
    "plt.subplot(222)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,1.)\n",
    "\n",
    "plt.subplot(223)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,10.)\n",
    "\n",
    "plt.subplot(224)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,100.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
