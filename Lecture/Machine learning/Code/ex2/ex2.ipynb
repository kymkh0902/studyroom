{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> 찾아볼 함수 </font>\n",
    "- np.where\n",
    "- np.ndarray.astype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Exercise 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 1,
>>>>>>> ed3c81d54f04def4f2a7a3cc4a89f309ffe1f1c8
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 34.62365962  30.28671077  35.84740877  60.18259939  79.03273605\n",
      "   45.08327748  61.10666454  75.02474557  76.0987867   84.43281996\n",
      "   95.86155507  75.01365839  82.30705337  69.36458876  39.53833914\n",
      "   53.97105215  69.07014406  67.94685548  70.66150955  76.97878373\n",
      "   67.37202755  89.67677575  50.53478829  34.21206098  77.92409145\n",
      "   62.27101367  80.19018075  93.1143888   61.83020602  38.7858038\n",
      "   61.37928945  85.40451939  52.10797973  52.04540477  40.23689374\n",
      "   54.63510555  33.91550011  64.17698887  74.78925296  34.18364003\n",
      "   83.90239366  51.54772027  94.44336777  82.36875376  51.04775177\n",
      "   62.22267576  77.19303493  97.77159928  62.0730638   91.5649745\n",
      "   79.94481794  99.27252693  90.54671411  34.52451385  50.28649612\n",
      "   49.58667722  97.64563396  32.57720017  74.24869137  71.79646206\n",
      "   75.39561147  35.28611282  56.2538175   30.05882245  44.66826172\n",
      "   66.56089447  40.45755098  49.07256322  80.27957401  66.74671857\n",
      "   32.72283304  64.03932042  72.34649423  60.45788574  58.84095622\n",
      "   99.8278578   47.26426911  50.4581598   60.45555629  82.22666158\n",
      "   88.91389642  94.83450672  67.31925747  57.23870632  80.366756\n",
      "   68.46852179  42.07545454  75.47770201  78.63542435  52.34800399\n",
      "   94.09433113  90.44855097  55.48216114  74.49269242  89.84580671\n",
      "   83.48916274  42.26170081  99.31500881  55.34001756  74.775893  ]\n",
      " [ 78.02469282  43.89499752  72.90219803  86.3085521   75.34437644\n",
      "   56.31637178  96.51142588  46.55401354  87.42056972  43.53339331\n",
      "   38.22527806  30.60326323  76.4819633   97.71869196  76.03681085\n",
      "   89.20735014  52.74046973  46.67857411  92.92713789  47.57596365\n",
      "   42.83843832  65.79936593  48.85581153  44.2095286   68.97235999\n",
      "   69.95445795  44.82162893  38.80067034  50.25610789  64.99568096\n",
      "   72.80788731  57.05198398  63.12762377  69.43286012  71.16774802\n",
      "   52.21388588  98.86943574  80.90806059  41.57341523  75.23772034\n",
      "   56.30804622  46.85629026  65.56892161  40.61825516  45.82270146\n",
      "   52.06099195  70.4582      86.72782233  96.76882412  88.69629255\n",
      "   74.16311935  60.999031    43.39060181  60.39634246  49.80453881\n",
      "   59.80895099  68.86157272  95.59854761  69.82457123  78.45356225\n",
      "   85.75993667  47.02051395  39.26147251  49.59297387  66.45008615\n",
      "   41.09209808  97.53518549  51.88321182  92.11606081  60.99139403\n",
      "   43.30717306  78.03168802  96.22759297  73.0949981   75.85844831\n",
      "   72.36925193  88.475865    75.80985953  42.50840944  42.71987854\n",
      "   69.8037889   45.6943068   66.58935318  59.51428198  90.9601479\n",
      "   85.5943071   78.844786    90.424539    96.64742717  60.76950526\n",
      "   77.15910509  87.50879176  35.57070347  84.84513685  45.35828361\n",
      "   48.3802858   87.10385094  68.77540947  64.93193801  89.5298129 ]\n",
      " [  0.           0.           0.           1.           1.           0.\n",
      "    1.           1.           1.           1.           0.           0.\n",
      "    1.           1.           0.           1.           1.           0.\n",
      "    1.           1.           0.           1.           0.           0.\n",
      "    1.           1.           1.           0.           0.           0.\n",
      "    1.           1.           0.           1.           0.           0.\n",
      "    0.           1.           0.           0.           1.           0.\n",
      "    1.           0.           0.           0.           1.           1.\n",
      "    1.           1.           1.           1.           1.           0.\n",
      "    0.           0.           1.           0.           1.           1.\n",
      "    1.           0.           0.           0.           0.           0.\n",
      "    1.           0.           1.           1.           0.           1.\n",
      "    1.           1.           1.           1.           1.           1.\n",
      "    0.           0.           1.           1.           1.           1.\n",
      "    1.           1.           0.           1.           1.           0.\n",
      "    1.           1.           0.           1.           1.           1.\n",
      "    1.           1.           1.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cols = np.loadtxt('./data/ex2data1.txt', delimiter=',', unpack=True)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6ec47ace8de6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m### 코드 시작 ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;31m# 데이터 개수 m으로 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "datafile = 'data/ex2data1.txt'\n",
    "##############################################################################\n",
    "# np.loadtxt를 사용해서 datafile을 불러오기.                                 \n",
    "# 조건 :구분자는 \",\" 이고 데이터 범위는 column 기준 0, 1, 2번째, unpack : True   \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ### \n",
    "cols = np.loadtxt('./data/ex2data1.txt', delimiter=',', usecols = (0,1,2), unpack=True)\n",
    "### 코드 끝 ###\n",
    "assert cols.shape == (3, 100), \"불러온 데이터의 shape가 맞지 않음.\" \n",
    "\n",
    "##############################################################################\n",
    "# X, y를 적절한 형태로 바꾼다. m x n(m: 데이터 개수, n: feature 개수)\n",
    "# transpose 나 reshape 사용하면 됨. \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ### \n",
    "X = X.T\n",
    "y = np.reshape(cols, (100,1))\n",
    "m = 100 # 데이터 개수 m으로 저장\n",
    "\n",
    "### 코드 끝 ###\n",
    "assert X.shape == (100, 2) and y.shape == (100, 1), \"X 혹은 y의 shape가 맞지 않음.\"\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# X vector에 값이 1로 이루어진 1개 열을 추가해서 matrix로 만들어준다.              \n",
    "# 강의에서 bias(theta0) 값 X에 넣어주는 연산임.                                \n",
    "# np.insert 사용.                                                        \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ###\n",
    "\n",
    "### 코드 끝 ###\n",
    "assert X.shape == (100, 3), \"X의 shape가 맞지 않음\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# X를 positive/negative 데이터로 각각 나누어주자. \n",
    "# positive는 y==1일 때의 index에 해당하는 X의 값\n",
    "# negative는 y==0일 때의 index에 해당하는 X의 값이다.                                                            \n",
    "# np.where를 사용하면 된다. \n",
    "##############################################################################\n",
    "\n",
    "### 코드 시작 ###\n",
    "pos = \n",
    "neg = \n",
    "### 코드 끝 ###\n",
    "assert pos.shape == (60, 3) and neg.shape == (40, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos, neg 데이터를 scatter 그래프로 나타내보자. \n",
    "def plotData():\n",
    "    plt.plot(pos[:,1],pos[:,2],'k+',label='Admitted')\n",
    "    plt.plot(neg[:,1],neg[:,2],'yo',label='Not admitted')\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "plotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# sigmoid 함수를 만들어보자. \n",
    "# 수식을 보고 작성하면 된다. \n",
    "##############################################################################\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    \"\"\"\n",
    "    ### 코드 시작 ###    \n",
    "\n",
    "    ### 코드 끝 ###\n",
    "print(\"x=1일 때 sigmoid 계산 값 차이 : \", rel_error(sigmoid(1), 0.73105857))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid 그래프로 나타내기 \n",
    "def plotsigmoid():\n",
    "    x = np.arange(-10,10,.1)\n",
    "    plt.plot(x, sigmoid(x))\n",
    "    plt.title('Sigmoid function')\n",
    "    plt.show()\n",
    "    plt.grid()\n",
    "\n",
    "    x = np.arange(-10,10,.1)\n",
    "    plt.plot(x, sigmoid(x))\n",
    "    plt.title('Sigmoid function')\n",
    "    plt.show()\n",
    "    plt.grid()plotsigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(theta,X): \n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # logistic function에서의 h식을 만들어보자. \n",
    "    # 위에서 선언한 sigmoid 함수를 활용하자 \n",
    "    ##############################################################################\n",
    "    \n",
    "    ### 코드 시작 ###\n",
    "\n",
    "    ### 코드 끝 ###\n",
    "\n",
    "def computeCost(theta, X, y): \n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    y : y 데이터, shape=(m, 1), (array)\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # Cost 함수를 구해보자. \n",
    "    # 강의에서 배운 수식을 2개로 쪼개서 앞 쪽, 뒤 쪽을 numpy로 구현한 뒤에 합치면 된다. \n",
    "    # np.dot, np.sum, transpose, 위에서 정의한 h식을 섞어서 사용하면 된다. \n",
    "    ##############################################################################\n",
    "    \n",
    "    ### 코드 시작 ###\n",
    "    \n",
    "    ### 코드 끝 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 임의로 정한 theta 값\n",
    "initial_theta = np.zeros((X.shape[1],1))\n",
    "# cost 계산 \n",
    "computeCost(initial_theta,X,y)\n",
    "\n",
    "print(\"Cost 값과 정답과의 차이 : \", rel_error(computeCost(initial_theta, X, y), 0.6931471))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization \n",
    "\n",
    "gradient descent가 그렇듯 아래 식을 풀어서 optimization을 할 예정이다. <br>\n",
    "$\\theta_j := \\theta_j - \\alpha {\\frac \\partial {\\partial \\theta_j}} J(\\theta_0, \\theta_1)$ 식을 똑같이 미분해서 구하면 되는데 음... 조금 어려울 수도 있다. \n",
    "\n",
    "Cost function을 보면 아래와 같고, <br>\n",
    "$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^m \\left[y^i\\theta x^i-\\theta x^i-\\log(1+e^{-\\theta x^i})\\right]=-\\frac{1}{m}\\sum_{i=1}^m \\left[y^i\\theta x^i-\\log(1+e^{\\theta x^i})\\right] ~~(1)$ <br>\n",
    "\n",
    "위 식을 적당히 미분하면(*일단 패스하자*) 아래 결과를 얻을 수 있다. <br>\n",
    "$\\frac{\\partial}{\\partial \\theta_j}y^i\\theta x^i=y^ix^i_j ~~(2) $ <br>\n",
    "$\\frac{\\partial}{\\partial \\theta_j}\\log(1+e^{\\theta x^i})=\\frac{x^i_je^{\\theta x^i}}{1+e^{\\theta x^i}}=x^i_jh_\\theta(x^i)~~(3)$ <br>\n",
    "\n",
    "전개 과정은 생략하고 1번 식인 $J(\\theta)$ 내부 식을 $\\theta$에 대한 derivative를 구하면 (2), (3) 번 식을 유도할 수 있다.\n",
    "\n",
    "그럼, $\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\big [ y^ix^i_j + x^i_jh_\\theta(x^i) \\big ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iteration 10만번 예정이다. \n",
    "iterations = 100000\n",
    "# learning_rate는 0.001\n",
    "alpha = 0.001\n",
    "\n",
    "def descendGradient(X, theta):\n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    \"\"\"\n",
    "    theta_history = []  # theta 변화 과정 저장 \n",
    "    cost_history = []  # cost 변화 과정 저장 \n",
    "    ##############################################################################\n",
    "    # Gradient descent를 하자. \n",
    "    # 1) 위에서 혹은 강의에서 배운 수식을 gradient로 구현한다. \n",
    "    # 2) iteration 만큼 theta를 update 시켜준다. \n",
    "    # 3) 매번 업데이트 시켜줬을 때 cost, theta 값을 위에 cost_history, theta_history에 각각 저장해준다. \n",
    "    # 4) iterations 만큼 update 시켜준 뒤에 theta, cost_history, theta_history를 return 한다. \n",
    "    # linear regression에서 구현한 식이랑 거의 비슷하다. \n",
    "    ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient descent 실행\n",
    "theta, theta_history, cost_history = descendGradient(X, initial_theta)\n",
    "true_theta = np.array([[-4.81180027],\n",
    "                       [0.04528064],\n",
    "                       [0.03819149]])\n",
    "print(\"theta 값과 정답과의 차이 : \", rel_error(theta, true_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예측 함수 구현해보자. 우리가 구한 theta를 통해서 X를 넣어줬을 때 얼마나 y가 맞게 나오는 지 확인하기 위함이다. \n",
    "def predict(theta, X, threshold=0.5):\n",
    "    \"\"\"\n",
    "    theta : theta 값, shape=(n, 1), (array)\n",
    "    X : X 데이터, shape=(m, n), (array)\n",
    "    threshold : y 결정하는 기준 값. (float) \n",
    "    threshold는 1에 가까이 두면 positive에 penalty를 주는 것이고 0에 가까이두면 negative에 penalty를 주는 것이다. \n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # theta, X를 이용해서 예측을 해보고 값이 threshold 보다 큰 경우를 1 작은 경우를 0으로 return 하자. \n",
    "    # astype 이라는 함수를 새롭게 사용하게 될 것이다. \n",
    "    ##############################################################################\n",
    "    pred = h(theta, X) >= threshold\n",
    "\n",
    "    return (pred.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction 값을 구해보자. \n",
    "pred = predict(theta, X) \n",
    "correct = np.sum(pred == y) / len(pred)\n",
    "print('correct 값과 정답과의 차이 : ', rel_error(correct, 0.91))\n",
    "print('Train accuracy {}%'.format(100 * correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 학습에 따른 cost 감소 그래프를 보자.\n",
    "plt.plot(cost_history)\n",
    "plt.title('cost history')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 구한 theta를 이용해서 잘 구분되는 지 확인해보자. \n",
    "# 일단 theta가 정해졌고, x1, x2를 이용해서 식을 구성했을 때 y값이 0.5가 되면 \n",
    "# 그 위치에서 decision boundary를 형성한다고 생각할 수 있겠다. \n",
    "# 그리고 위에 sigmoid 그래프를 잘 보면 y가 0.5일 때는 np.dot(theta, X)가 0일 때이다. \n",
    "# 그래프를 그리기 위해서 x1값을 주어지고 x2를 구한다고 생각하면 쉽다. 구하는 식은 h식을 잘 가공하면 된다. \n",
    "\n",
    "# theta값 확인\n",
    "print(theta)\n",
    "\n",
    "### 코드 시작 ###\n",
    "x1 = np.linspace(30, 100, 1000) \n",
    "theta0 = \n",
    "theta1 = \n",
    "theta2 =\n",
    "x2 = \n",
    "### 코드 끝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 그래프 그려보자. 잘 구분된 거 같은지 확인한다. \n",
    "def plotData2():\n",
    "    plt.scatter(pos[:,1],pos[:,2], c='k', marker='+', label='Admitted')\n",
    "    plt.scatter(neg[:,1],neg[:,2], c='y', marker='o',label='Not admitted')\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlabel('Exam 1 score')\n",
    "    plt.ylabel('Exam 2 score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "plotData2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 동일한 내용을 scipy로 구현한 예제이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#An alternative to OCTAVE's 'fminunc' we'll use some scipy.optimize function, \"fmin\"\n",
    "#Note \"fmin\" does not need to be told explicitly the derivative terms\n",
    "#It only needs the cost function, and it minimizes with the \"downhill simplex algorithm.\"\n",
    "#http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.optimize.fmin.html\n",
    "from scipy import optimize\n",
    "\n",
    "def optimizeTheta(mytheta,myX,myy,mylambda=0.):\n",
    "    result = optimize.fmin(computeCost, x0=mytheta, args=(myX, myy, mylambda), maxiter=400, full_output=True)\n",
    "    return result[0], result[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta, mincost = optimizeTheta(initial_theta,X,y)\n",
    "#That's pretty cool. Black boxes ftw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"Call your costFunction function using the optimal parameters of θ. \n",
    "#You should see that the cost is about 0.203.\"\n",
    "print(computeCost(theta,X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting the decision boundary: two points, draw a line between\n",
    "#Decision boundary occurs when h = 0, or when\n",
    "#theta0 + theta1*x1 + theta2*x2 = 0\n",
    "#y=mx+b is replaced by x2 = (-1/thetheta2)(theta0 + theta1*x1)\n",
    "\n",
    "boundary_xs = np.array([np.min(X[:,1]), np.max(X[:,1])])\n",
    "boundary_ys = (-1./theta[2])*(theta[0] + theta[1]*boundary_xs)\n",
    "plotData()\n",
    "plt.plot(boundary_xs,boundary_ys,'b-',label='Decision Boundary')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For a student with an Exam 1 score of 45 and an Exam 2 score of 85, \n",
    "#you should expect to see an admission probability of 0.776.\n",
    "print(h(theta,np.array([1, 45.,85.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scipy에서 나오는 theta는 rank가 1이라서 2로 바꿔준다. \n",
    "theta = theta.reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = predict(theta, X) \n",
    "correct = np.sum(pred == y) / len(pred)\n",
    "print('Train accuracy {}%'.format(100 * correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일단 여기까지 수정함. \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data/ex2data2.txt'\n",
    "#!head $datafile\n",
    "cols = np.loadtxt(datafile,delimiter=',',usecols=(0,1,2),unpack=True) #Read in comma separated data\n",
    "##Form the usual \"X\" matrix and \"y\" vector\n",
    "X = np.transpose(np.array(cols[:-1]))\n",
    "y = np.transpose(np.array(cols[-1:]))\n",
    "m = y.size # number of training examples\n",
    "##Insert the usual column of 1's into the \"X\" matrix\n",
    "X = np.insert(X,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the sample into two: ones with positive classification, one with null classification\n",
    "pos = np.array([X[i] for i in range(X.shape[0]) if y[i] == 1])\n",
    "neg = np.array([X[i] for i in range(X.shape[0]) if y[i] == 0])\n",
    "#Check to make sure I included all entries\n",
    "#print \"Included everything? \",(len(pos)+len(neg) == X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAF3CAYAAACCFb2MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+P8Be+4AGcrza3UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f096c204080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotData():\n",
    "    plt.plot(pos[:,1],pos[:,2],'k+',label='y=1')\n",
    "    plt.plot(neg[:,1],neg[:,2],'yo',label='y=0')\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "#Draw it square to emphasize circular features\n",
    "plt.figure(figsize=(6,6))\n",
    "plotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Feature mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This code I took from someone else (the OCTAVE equivalent was provided in the HW)\n",
    "def mapFeature( x1col, x2col ):\n",
    "    \"\"\" \n",
    "    Function that takes in a column of n- x1's, a column of n- x2s, and builds\n",
    "    a n- x 28-dim matrix of featuers as described in the homework assignment\n",
    "    \"\"\"\n",
    "    degrees = 6\n",
    "    out = np.ones( (x1col.shape[0], 1) )\n",
    "\n",
    "    for i in range(1, degrees+1):\n",
    "        for j in range(0, i+1):\n",
    "            term1 = x1col ** (i-j)\n",
    "            term2 = x2col ** (j)\n",
    "            term  = (term1 * term2).reshape( term1.shape[0], 1 ) \n",
    "            out   = np.hstack(( out, term ))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature-mapped X matrix\n",
    "mappedX = mapFeature(X[:,1],X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappedX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Cost function and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cost function is the same as the one implemented above, as I included the regularization\n",
    "#toggled off for default function call (lambda = 0)\n",
    "#I do not need separate implementation of the derivative term of the cost function\n",
    "#Because the scipy optimization function I'm using only needs the cost function itself\n",
    "#Let's check that the cost function returns a cost of 0.693 with zeros for initial theta,\n",
    "#and regularized x values\n",
    "initial_theta = np.zeros((mappedX.shape[1],1))\n",
    "computeCost(initial_theta,mappedX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Learning parameters using fminunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I noticed that fmin wasn't converging (passing max # of iterations)\n",
    "#so let's use minimize instead\n",
    "\n",
    "def optimizeRegularizedTheta(mytheta,myX,myy,mylambda=0.):\n",
    "    result = optimize.minimize(computeCost, mytheta, args=(myX, myy, mylambda),  method='BFGS', options={\"maxiter\":500, \"disp\":False} )\n",
    "    return np.array([result.x]), result.fun\n",
    "    \n",
    "theta, mincost = optimizeRegularizedTheta(initial_theta,mappedX,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Plotting the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotBoundary(mytheta, myX, myy, mylambda=0.):\n",
    "    \"\"\"\n",
    "    Function to plot the decision boundary for arbitrary theta, X, y, lambda value\n",
    "    Inside of this function is feature mapping, and the minimization routine.\n",
    "    It works by making a grid of x1 (\"xvals\") and x2 (\"yvals\") points,\n",
    "    And for each, computing whether the hypothesis classifies that point as\n",
    "    True or False. Then, a contour is drawn with a built-in pyplot function.\n",
    "    \"\"\"\n",
    "    theta, mincost = optimizeRegularizedTheta(mytheta,myX,myy,mylambda)\n",
    "    xvals = np.linspace(-1,1.5,50)\n",
    "    yvals = np.linspace(-1,1.5,50)\n",
    "    zvals = np.zeros((len(xvals),len(yvals)))\n",
    "    for i in xrange(len(xvals)):\n",
    "        for j in xrange(len(yvals)):\n",
    "            myfeaturesij = mapFeature(np.array([xvals[i]]),np.array([yvals[j]]))\n",
    "            zvals[i][j] = np.dot(theta,myfeaturesij.T)\n",
    "    zvals = zvals.transpose()\n",
    "\n",
    "    u, v = np.meshgrid( xvals, yvals )\n",
    "    mycontour = plt.contour( xvals, yvals, zvals, [0])\n",
    "    #Kind of a hacky way to display a text on top of the decision boundary\n",
    "    myfmt = { 0:'Lambda = %d'%mylambda}\n",
    "    plt.clabel(mycontour, inline=1, fontsize=15, fmt=myfmt)\n",
    "    plt.title(\"Decision Boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a figure showing contours for various values of regularization parameter, lambda\n",
    "#It shows for lambda=0 we are overfitting, and for lambda=100 we are underfitting\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.subplot(221)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,0.)\n",
    "\n",
    "plt.subplot(222)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,1.)\n",
    "\n",
    "plt.subplot(223)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,10.)\n",
    "\n",
    "plt.subplot(224)\n",
    "plotData()\n",
    "plotBoundary(theta,mappedX,y,100.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
