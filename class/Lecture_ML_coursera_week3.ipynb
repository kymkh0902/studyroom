{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning week #3\n",
    "\n",
    "#### 학습 목표 : 변수가 2개 이상인 데이터를 Logistic regression으로 분류하기.\n",
    "\n",
    "#### Classification(분류)\n",
    "\n",
    "지금까지 특정한 ***feature***들이 주어졌을 때 어떤 ***값***을 예측하는 문제를 다뤘다. 예를 들면 방 개수, 평수에 따른 집 값의 예측이 있겠다. <br>\n",
    "이제 조금은 다른 문제인 **분류** 에 대해 살펴보도록 하겠다. \n",
    "\n",
    "분류는 공통점을 가진 집단으로 묶어서 나눈다는 개념으로 이해하면 되겠다. 예를 들면 개나 고양이를 나눈다던지, 스팸/비스팸 메일을 나눈다던지 하는 예시들이 있다. \n",
    "\n",
    "앞서 마찬가지로 데이터는 여러 개의 의미있는 feature들로 구성이 될 것이고 우리가 얻을 결과 값은 우리가 정해놓은 **범주(category)** 들 안에서 \n",
    "선택이 될 것이다. 아직 배우는 학습 과정도 supervised learning이기 때문에 많은 데이터가 각각의 범주에 따라서 다른 분포를 나타낼 것이고 \n",
    "그 중에서 ***어디에 가깝냐*** 를 맞추는 과정이라고 생각하면 쉽다. \n",
    "\n",
    "#### Binary classification (이진 분류) - logistic regression\n",
    "\n",
    "Yes or No, 0 or 1로 분류하는 가장 간단하면서 처음에 살펴볼 분류 방법이다. <br>\n",
    "가장 간단하게 생각해보자. <br> \n",
    "0, 1로 분류하기 위해서는 X 데이터를 계산해서 구한 값을 0~1 사이의 값으로 나타내주고 그 중에서 0에 가까운 값은 0으로, 1에 가까운 값은 1로 하면 되지 않을까? <br>\n",
    "\n",
    "그게 바로 이진 분류의 방법으로 사용되는 ***logistic regression***의 내용이다. <br>logistic regression은 **logistic function(sigmoid function)** 이 사용되어 붙여진 이름으로 ***sigmoid function***이 매우 중요한 역할을 하게 된다. \n",
    "\n",
    "***sigmoid function***은 위에서 얘기한 **0~1 사이의 값으로 나타내주기 위해서 사용하는 함수이다.** <br>\n",
    "수식으로 나타내면 $\\sigma(x) = \\frac 1 {1 + e^{-x}}$ 이고 $ x \\in [-\\infty, \\infty]$ 에 대해서 아래 그래프처럼 0~1값을 출력한다.\n",
    "\n",
    "![sigmoid](../pictures/sigmoid.png)\n",
    "\n",
    "그럼 **왜** 다른 함수들은 별로이고 sigmoid function을 사용할까? <br>\n",
    "이 [사이트](https://stats.stackexchange.com/questions/162988/why-sigmoid-function-instead-of-anything-else) 에 잘 나와있는데 별 다른 이유는 없고 binary 문제에 대한 분포(Bernoulli 분포)를 가장 나타내줄 수 있는 함수이기 때문이다. Gaussian 분포가 자연 현상에 대한 분포를 잘 나타내서 많은 문제들을 다룰 때 기본적인 가정으로 사용하는 것과 같은 맥락이다. \n",
    "\n",
    "자 이제, 분류가 무엇인지 배웠고 그 중에서 이진 분류에 해당하는 logistic regression이 무엇인지 배웠다. 그 다음으로 sigmoid function이 logistic regression에서 왜 필요한 지 왜 쓰이는 지 배웠다. \n",
    "\n",
    "그 다음은 cost function으로 나타내고 convex임을 확인한 후에 앞에서 배운 gradient descent 학습 방법을 이용해서 학습시켜주면 된다.!!\n",
    "\n",
    "#### Decision boundary  \n",
    "\n",
    "우리가 분류 문제를 풀면 경계에 해당되는 값 (*logistic에서는 0.5이다.*)이 생길 수 밖에 없다. 이 값들로 이루어진 경계를 ***decision boundary***라고 한다. <br> 데이터의 feature수가 적을 때는 직관적으로 나타내기 좋지만 feature가 3개 이상만 되어도 그래프로 나타내기는 어렵다. 단순하게 복잡한 차원에 feature들이 존재할 때 범주 별로 나눠지는 걸 상상하면 된다. \n",
    "\n",
    "\n",
    "#### Cost function,  Cross entropy \n",
    "\n",
    "집 값을 예측하는 문제에 대해서 우리는 MSE(mean squared error)를 cost function으로 사용했다. 이는 우리가 예측한 값과 정답과의 차이를 나타내주므로 이를 최소화하도록 학습을 하면 되니까 괜찮은 함수라고 볼 수 있다. (*다른 함수를 써도 되나 어떤 값 예측에 가장 많이 쓴다.*)\n",
    "\n",
    "\n",
    "그럼 분류 문제에 대해서는 어떤 함수를 cost function으로 사용해야 할까? 기존의 MSE를 사용할 경우에는 **non-convex function**이 된다.\n",
    "**convex function**을 사용할 경우에 global minimum을 구할 수 있다는 장점이 있기에 MSE보다는 다른 방법을 찾는게 좋겠다.\n",
    "\n",
    "이[사이트](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/)에서 MSE를 사용했을 때와 앞으로 사용하려고 하는 cross-entropy를 사용했을 때를 잘 비교해놓았다. 더 궁금하면 참고하도록 하자.\n",
    "\n",
    "그래서, MSE 일단 미뤄두고.. cross-entropy라는 정보 이론에서 사용하는 식을 도입하게 되는데 이게 괜찮다. 식으로 나타내면 \n",
    "아래와 같고 나타내는 의미를 그래프를 통해서 보면 ***정답에서 멀어질 수록 더 많은 penalty를 부여하므로*** 괜찮은 식이라고 볼 수 있다. \n",
    "\n",
    "  $Cost(h_\\theta(x),y) =\n",
    "    \\begin{cases}\n",
    "      -log(h_\\theta(x)) & \\text{if}\\ y = 1\\\\\n",
    "      -log(1-h_\\theta(x)) & \\text{if}\\ y = 0\\\\\n",
    "    \\end{cases}   $ <br>\n",
    "    \n",
    "![cost function](../pictures/logistic_cost_function1.png)<br>\n",
    "<center> <i> &lt;y=1일 때 $h_\\theta(x)$에 대한 cost function&gt;</i> </center> <br>\n",
    "\n",
    "![cost function](../pictures/logistic_cost_function2.png)\n",
    "<center> <i> &lt;y=0일 때 $h_\\theta(x)$에 대한 cost function&gt;</i> </center> <br>\n",
    "\n",
    "위의 식에서 y는 어차피 0,1로 밖에 이루어지지 않으므로 두 식을 합쳐버리면 <br>\n",
    "$J(\\theta) = \\frac 1 m [\\sum_{i=1}^m y^{(i)} log\\ h_{\\theta}(x^{(i)}) + (1 - y^{(i)}) log (1 - h_{\\theta}(x^{(i)})]$\n",
    "\n",
    "#### Gradient descent\n",
    "\n",
    "위에서 정의한 cost function을 앞선 방법과 같이 gradient descent를 활용해서 cost function의 각 theta에 대한 미분값을 구하고 \n",
    "한 번에 업데이트 해주면 되겠다.!!\n",
    "\n",
    "#### Multi class classification \n",
    "\n",
    "더 많은 class가 있을 경우에 접근 방법이다. One vs.-all(rest),  One-vs.-one 방법이 사용되며 강의에서는 **One vs.-all(rest)**\n",
    "에 대해서만 소개하고 있다. binary와 마찬가지로 0, 1의 문제로 여러 번 풀면 decision boundary가 여러 개 생겨서 분류가 가능하다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
