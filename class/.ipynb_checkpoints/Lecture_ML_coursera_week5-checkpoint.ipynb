{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning week #5\n",
    "\n",
    "### Neural Network Back Propagation\n",
    "\n",
    "대망의 신경망의 Gradient descent를 활용한 학습 방법인 Back Propagation을 소개하는 시간이다. <br>\n",
    "딥러닝의 대부분의 모델이 학습하는 방법이기에 지금까지 수업 중에서 가장 중요하다고 꼽을만한 내용이므로 확실하게 이해하도록 하자. \n",
    "\n",
    "여태까지 살펴본 모든 모델들의 학습 방법은 동일했다. **Gradient Descent!!** 신경망에서도 똑같이 사용할텐데 뭐가 다른걸까? <br>\n",
    "차이점은 바로 ***신경망은 여러 개의 수식이 나열된 모습의 모델을 나타낸다는 점이다.*** 수식을 통과함에 따라 각각의 theta에 대한 cost function이 매우 복잡해지므로 기존과 같이 간단하게 cost function에 대한 theta의 미분값을 한 번에 얻기가 매우 힘들다. <br>\n",
    "\n",
    "그래서 제시된 방법이 Back Propagation이고 내용 자체는 그리 복잡하지 않으나 이해하고 직접 구하는데 처음에 애를 먹는다. \n",
    "\n",
    "#### Chain Rule \n",
    "Back Propagation을 가능하게 해주는 수학적 원리이다. 신경망은 위에서 설명했듯 여러 개의 수식을 통과하는 형태이다. \n",
    "각각의 수식에 대한 개별 Gradient를 구했을 때 전체 Gradient를 구할 수 있다면? 문제가 한결 쉬워질 것이다. 이를 가능하게 해주는 것이 **Chain Rule**이다. 고등학교 때(?) 아마 배웠을 것이고 간단한 예제를 보면 떠올릴 수 있을 것이다. <br>\n",
    "\n",
    "$\\text{chain rule} : \\frac {\\partial y} {\\partial x} = \\frac {\\partial y} {\\partial z} \\frac {\\partial z} {\\partial x}$ <br>\n",
    "\n",
    "$\\begin{align} \\text{예시: }\\ y &= z^2, z = 2x\\ \\text{일 때,}\\ \\frac {\\partial y} {\\partial z} = 2z, \\frac {\\partial z} {\\partial x} = 2\\ \\text{이므로}\\ \\\\\n",
    "\\frac {\\partial y} {\\partial x} &= \\frac {\\partial y} {\\partial z} \\frac {\\partial z} {\\partial x} = 2z \\cdot 2 = 4z = 8x \\end{align}$\n",
    "\n",
    "#### 2 layer NN back propagation \n",
    "\n",
    "수업 내용으로 배운 수식을 활용해서 Chain rule이 가능하게 개별 식 단위로 쪼개서 각각의 gradient를 구해보자. <br>\n",
    "모든 표기법은 대문자, Matrix로 진행할 예정이다. \n",
    "\n",
    "![NN](../pictures/neural_network.png)\n",
    "<center> *&lt;출처: [coursera - machine learning lecture9 note](http://www.holehouse.org/mlclass/09_Neural_Networks_Learning.html) &gt;* </center>\n",
    "\n",
    "- 목표 : $J(\\Theta)$에 대한 $\\Theta^{(1)}, \\Theta^{(2)}$ ***각각의 gradient를 구하기.*** ($\\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(1)}}, \\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(2)}}$) <br><br>\n",
    "- Forward 식 : 식이 진행되는 순서대로 쭉 나열해보자. <br>\n",
    "    1) $ Z^2 = \\Theta^{(1)} \\cdot X$ <br>\n",
    "    2) $ A^2 = g(Z^2)$ <br>\n",
    "    3) $ Z^3 = \\Theta^{(2)} \\cdot A^2$ <br>\n",
    "    4) $ H_{\\Theta}(X) = g(Z^3)$ <br>\n",
    "    5) $ J(\\Theta) = -\\frac 1 m \\big [\\sum Y{log}H_{\\Theta}(X) + (1 - Y){log}(1-H_{\\Theta}(X)) \\big] + \\frac \\lambda {2m} \\sum \\Theta^2$ <br> <br>\n",
    "- Backward 식 : 거꾸로 바로 이전 식에 대한 항들의 gradient를 구해보자. <br>\n",
    "    1) $\\frac {\\partial Z^2} {\\partial \\Theta^{(1)}} = \\frac \\partial {\\partial \\Theta^{(1)}} (\\Theta^{(1)} \\cdot X$) <br>\n",
    "    2) $\\frac {\\partial A^2} {\\partial Z^2} = \\frac \\partial {\\partial Z^2} (g(Z^2))$ <br>\n",
    "    3) $\\frac {\\partial Z^3} {\\partial A^2} = \\frac \\partial {\\partial A^2} (\\Theta^{(2)} \\cdot A^2)$ <br>\n",
    "    4) $\\frac {\\partial Z^3} {\\partial \\Theta^{(2)}} = \\frac \\partial {\\partial \\Theta^{(2)}} (\\Theta^{(2)} \\cdot A^2)$ <br>\n",
    "    5) $\\frac {\\partial H_{\\Theta}(X)} {\\partial Z^3} = \\frac \\partial {\\partial Z^3} (g(Z^3))$ <br>\n",
    "    6) $\\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} = \\frac \\partial {\\partial H_{Theta}(X)} (J(\\Theta))$ <br>\n",
    "    \n",
    "위 식들을 이용해서 구해야 하는 목표 식 2개를 풀어서 나타내보도록 하자. <br>\n",
    "*** 아래와 같이 식을 풀어서 나타낼 수 있고 위에서 계산한 Backward 1~5를 활용하면 목표 1, 2를 구할 수 있다는 것을 알았다. \n",
    "1~5를 구하는 것에 어려움은 sigmoid에 대한 gradient 밖에 없고 이는 위(2.1)에서 해결했으므로 각각의 gradient를 구현하고 아래 식에 대입하면 되겠다. ***\n",
    "\n",
    "### 목표 1 : $\\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial A^2} \\frac {\\partial A^2} {\\partial \\Theta^{(1)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial A^2} \\frac {\\partial A^2} {\\partial Z^2} \\frac {\\partial Z^2} {\\partial \\Theta^{(1)}}$\n",
    "\n",
    "### 목표 2 : $\\frac {\\partial J(\\Theta)} {\\partial \\Theta^{(2)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial \\Theta^{(2)}} = \\frac {\\partial J(\\Theta)} {\\partial H_{\\Theta}(X)} \\frac {\\partial H_{\\Theta(X)}} {\\partial Z^3} \\frac {\\partial Z^3} {\\partial \\Theta^{(2)}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [참고 자료]\n",
    "\n",
    "Backpropagation을 적용할 때 layer가 많아지면 수식을 다 쓰고 하기가 너무 복잡하고 어렵다. <br>\n",
    "그래서 아래와 같은 방식으로 그래프를 그린 뒤에 각각의 적용되는 수식 별로 gradient를 구해서 뒤로 전달하는 방식을 취하면 쉽게 해결할 수 있다.!!\n",
    "![bp](../pictures/backpropagation.png)\n",
    "<center> *&lt;출처: 스탠포드 대학교 CS231n 강의 자료 &gt;* </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
