{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두를 위한 딥러닝 #1\n",
    "\n",
    "머신러닝 모델의 기본 틀은 아래와 같이 이루어진다. 우리는 계속 Supervised Learning(지도 학습)에 대해서 다룰 예정이다.\n",
    "\n",
    "코드 상으로 나타낼 때에도 틀이 정해져 있어서 처음에 한 번 작성해놓으면 틀에 맞춰서 사용하면 된다.\n",
    "1. 전처리: \n",
    "    - train/test split: 데이터를 train, test로 나눈다. 필요 시 validation set도 만들어준다.\n",
    "    - normalization: 보통 데이터를 입력하기 전에 정규화를 시켜준다.\n",
    "    - generator: 배치 단위로 쪼개서 입력해주어야 하기 때문에 batch를 생성해주는 generator를 만들어준다.\n",
    "2. 모델: \n",
    "    - input_data: 학습을 위한 데이터 입력\n",
    "        - X: 데이터 \n",
    "        - y: label\n",
    "    - model: 모델을 선정한다. 앞으로는 딥러닝 기반 모델을 주로 사용하게 될 예정이다.\n",
    "        - Logistic regression\n",
    "        - Linear regression\n",
    "        - Neural Network\n",
    "    - loss: loss를 선정한다. 무엇을 예측하느냐에 따라서 사용하는 방식이 나뉘고 주로 분류에는 Cross entropy를, 회귀에는 MSE를 사용한다.\n",
    "        - MSE\n",
    "        - Cross entropy\n",
    "    - optimizer: optimizer를 선정한다. 지금까지는 Gradient descent를 사용했지만 더 다양한 학습 방법을 소개할 예정이다. \n",
    "        - Gradient descent optimizer\n",
    "        - Adam optimzer\n",
    "        - Adagrad optimizer \n",
    "    - train_op: 주로 loss를 최소화하는 training 방법을 사용한다.\n",
    "        - minimize loss\n",
    "    - prediction: 데이터를 입력해서 예상 결과를 얻는다.\n",
    "3. hyperparameter: \n",
    "    - learning_rate: 한 번에 얼마나 학습시킬 것인지 결정한다. optimizer에 들어간다.\n",
    "    - num_epochs: 얼마나 많이 학습을 시킬 것인지 결정한다. 모델의 iteration 수를 결정한다. \n",
    "    - batch_size: batch 크기를 얼마로 할 것인지 결정한다. 한 번에 얼만큼의 데이터 양으로 학습시킬 지 결정한다.\n",
    "    - n_layers: layer를 얼마나 중첩되서 쌓을 것인지 결정한다. \n",
    "    - num_units: layer의 hidden unit의 수를 몇으로 할 것인지 결정한다.\n",
    "4. 평가\n",
    "    - metrics: prediction을 통해서 얻은 결과를 바탕으로 모델의 성능을 평가한다.\n",
    "        - precision: 정답(참)/찍은 수(참)\n",
    "        - recall: 정답(참)/정답 수(참) \n",
    "        - accuracy: 정답(참 + 거짓)/찍은 수(참 + 거짓)\n",
    "    \n",
    "빠진 내용: <br>\n",
    "1. 에러 분석: 정확도를 통해 확인했을 때 어떤 부분에서 에러가 발생하는지에 대한 내용 (*코딩이 익숙해지면 다룰 예정*)\n",
    "2. 학습 모니터링: 강의에서 `Summary` 배운 뒤 활용 예정.\n",
    "3. 학습된 모델 불러와서 재사용: 강의에서 `Saver`, `restore`를 배운 뒤 활용 예정. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 목표\n",
    "\n",
    "1. 전반적인 위 flow를 코드를 따라서 보고 이해하기\n",
    "2. 아래 코드를 객체 지향적으로 설계해보기. 위의 틀처럼 공통된 분모끼리 묶고 묶었을 때 모델의 종류가 바뀌더라도 재 사용이 가능하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 불러온다. 우리가 사용할 데이터는 예전에 보던 데이터와 거의 유사한데, 흑백 0~9까지의 숫자 데이터다.\n",
    "mnist = input_data.read_data_sets('./', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터를 나누자.\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X 데이터에 대해서 정규화를 시키자.\n",
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generator를 생성할 함수를 만들자.\n",
    "def generator(data, labels, batch_size):\n",
    "    size = data.shape[0]\n",
    "    shuffled_indices = np.random.choice(size, size, replace=False)\n",
    "    data = data[shuffled_indices]\n",
    "    labels = labels[shuffled_indices]\n",
    "    for i in range(size // batch_size):\n",
    "        yield data[i*batch_size: (i+1)*batch_size], labels[i*batch_size: (i+1)*batch_size]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyperparameter를 지정해준다.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "num_units = 500\n",
    "num_classes = 10\n",
    "num_train = len(X_train)\n",
    "num_iterations = num_train // batch_size\n",
    "\n",
    "# 입력 데이터를 지정해준다. \n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 모델을 설계한다.\n",
    "fc1 = tf.layers.dense(X, units=num_units, activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(fc1, units=num_classes)\n",
    "\n",
    "# loss와 optimizer를 지정한다.\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# pred 값을 예측해보자.\n",
    "pred = logits\n",
    "correction = tf.equal(tf.argmax(y, axis=1), tf.argmax(pred, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_accuracy: 0.750, test_accuracy: 0.817\n",
      "Epoch: 1, train_accuracy: 0.870, test_accuracy: 0.865\n",
      "Epoch: 2, train_accuracy: 0.890, test_accuracy: 0.883\n",
      "Epoch: 3, train_accuracy: 0.860, test_accuracy: 0.893\n",
      "Epoch: 4, train_accuracy: 0.930, test_accuracy: 0.899\n",
      "Epoch: 5, train_accuracy: 0.890, test_accuracy: 0.905\n",
      "Epoch: 6, train_accuracy: 0.910, test_accuracy: 0.909\n",
      "Epoch: 7, train_accuracy: 0.920, test_accuracy: 0.913\n",
      "Epoch: 8, train_accuracy: 0.930, test_accuracy: 0.916\n",
      "Epoch: 9, train_accuracy: 0.920, test_accuracy: 0.919\n",
      "Epoch: 10, train_accuracy: 0.890, test_accuracy: 0.921\n",
      "Epoch: 11, train_accuracy: 0.930, test_accuracy: 0.922\n",
      "Epoch: 12, train_accuracy: 0.880, test_accuracy: 0.923\n",
      "Epoch: 13, train_accuracy: 0.920, test_accuracy: 0.925\n",
      "Epoch: 14, train_accuracy: 0.940, test_accuracy: 0.926\n",
      "Epoch: 15, train_accuracy: 0.940, test_accuracy: 0.927\n",
      "Epoch: 16, train_accuracy: 0.960, test_accuracy: 0.928\n",
      "Epoch: 17, train_accuracy: 0.950, test_accuracy: 0.930\n",
      "Epoch: 18, train_accuracy: 0.950, test_accuracy: 0.931\n",
      "Epoch: 19, train_accuracy: 0.880, test_accuracy: 0.932\n"
     ]
    }
   ],
   "source": [
    "# session을 실행시킨다. \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch수 만큼 iteration을 돌리자.\n",
    "for epoch in range(num_epochs):\n",
    "    # batch generator를 생성하자. \n",
    "    batch_generator = generator(X_train, y_train, batch_size)     \n",
    "    for i, batch in enumerate(batch_generator):\n",
    "        batch_X, batch_y = batch\n",
    "        _, train_loss, train_accuracy = sess.run([train_op, loss, accuracy], \n",
    "                                                 feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "    test_loss, test_accuracy = sess.run([loss, accuracy],\n",
    "                                        feed_dict={X: X_test, y: y_test})\n",
    "    print('Epoch: {0}, train_accuracy: {1:.3f}, test_accuracy: {2:.3f}'\\\n",
    "          .format(epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 객체 지향적 설계\n",
    "\n",
    "객체 지향적으로 설계하는 내용에 대해서는 2달 전인가 대략적으로 살펴보았다. (Class5_coding.ipynb, Class_Python_class.ipynb를 참고하자.)\n",
    "\n",
    "위 코드를 가지고 어떻게 설계를 해야 중복되는 내용을 없애고 다른 모델이 추가되었을 때 잘 쓸 수 있을지 생각해보자. <br>\n",
    "1. 우리가 위에서 나눠놓은 부분대로 설계를 해야 각 부분 별로 변경이 되어도 쉽게 반영할 수 있다. \n",
    "2. 너무 다른 부분은(전처리와 모델) 아예 다른 script에 짜서 넣고 불러와서 사용하는 것이 다루기 편리하다. \n",
    "3. 최종적으로 학습을 시킬 때에는 각각 내용이 나뉘어져있는 script로부터 불러와서 학습시키면 된다. \n",
    "4. 3번 학습 때 조건을 바꿔줘야 하는 부분 (hyperparameter)은 언제든지 변경되어야 하므로 3번에 지정해주는 것이 편리하다.\n",
    "\n",
    "예시를 들어보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드는 위에 있는 코드 중 모델에 관한 부분에서 공통적인 특징을 가진 부분들만 모아서 함수화를 한 내용이다. 아래에는 문제가 몇 가지 있다.\n",
    "1. 중복되게 입력되는 인수들이 너무 많다. `model`, `loss`, `train_op`... 앞서 반환되는 값들을 계속 입력해줘야 한다. 그리고 대부분이 입력되는 값들도 변하지 않는데 인수를 지정해주어야 하는 경우위다. 불필요하고 낭비다.\n",
    "2. 우리는 지금 함수로 만들었다. 그럼 이 각각의 함수가 다른데서 쓰일 수 있어야 하는데 쓰일 수 있나? 없다. 왜냐면 모델을 만드는데 필요한 하나의 부품일 뿐이기 때문이다. 제각각 다른 곳에 사용할 수 없다면 하나로 뭉쳐주는게 낫지 않나? \n",
    "\n",
    "-> ***위의 문제들을 해결하기 위해서 class로 묶어준다!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyperparameter를 지정해준다.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "num_units = 500\n",
    "num_classes = 10\n",
    "num_train = len(X_train)\n",
    "num_iterations = num_train // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 입력 데이터를 지정해준다. \n",
    "def add_placeholders():\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    y = tf.placeholder(tf.float32, [None, 10])\n",
    "    return X, y \n",
    "\n",
    "# 모델을 설계해서 logits를 구하자.\n",
    "def model(X, num_units, num_classes):\n",
    "    fc1 = tf.layers.dense(X, units=num_units, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(fc1, units=num_classes)\n",
    "    return logits\n",
    "\n",
    "# loss를 구하자.\n",
    "def loss(logits, labels):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    return loss\n",
    "\n",
    "# train_op를 만들자.\n",
    "def train_op(loss, learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    return train_op\n",
    "\n",
    "# prediction 값을 구하자. \n",
    "def accuracy(pred, labels):\n",
    "    correction = tf.equal(tf.argmax(y, axis=1), tf.argmax(pred, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correction, tf.float32))다\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_accuracy: 0.940, test_accuracy: 0.965\n",
      "Epoch: 1, train_accuracy: 0.980, test_accuracy: 0.971\n",
      "Epoch: 2, train_accuracy: 0.990, test_accuracy: 0.976\n",
      "Epoch: 3, train_accuracy: 0.990, test_accuracy: 0.974\n",
      "Epoch: 4, train_accuracy: 1.000, test_accuracy: 0.979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d6702b12ba08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         _, train_loss, train_accuracy = sess.run([train_op, loss, accuracy], \n\u001b[0;32m---> 19\u001b[0;31m                                                  feed_dict={X: batch_X, y: batch_y})\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     test_loss, test_accuracy = sess.run([loss, accuracy],\n",
      "\u001b[0;32m/home/whikwon/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/whikwon/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/whikwon/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/whikwon/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/whikwon/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 위에서 정의한 함수 합치기\n",
    "X, y = add_placeholders()\n",
    "logits = model(X, num_units, num_classes)\n",
    "loss = loss(logits, y)\n",
    "train_op = train_op(loss)\n",
    "accuracy = accuracy(logits, y)\n",
    "\n",
    "# session을 실행시킨다. \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch수 만큼 iteration을 돌리자.\n",
    "for epoch in range(num_epochs):\n",
    "    # batch generator를 생성하자. \n",
    "    batch_generator = generator(X_train, y_train, batch_size)     \n",
    "    for i, batch in enumerate(batch_generator):\n",
    "        batch_X, batch_y = batch\n",
    "        _, train_loss, train_accuracy = sess.run([train_op, loss, accuracy], \n",
    "                                                 feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "    test_loss, test_accuracy = sess.run([loss, accuracy],\n",
    "                                        feed_dict={X: X_test, y: y_test})\n",
    "    print('Epoch: {0}, train_accuracy: {1:.3f}, test_accuracy: {2:.3f}'\\\n",
    "          .format(epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 내용을 class로 묶어서 다시 표현해보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# hyperparameter를 지정해준다.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "num_units = 500\n",
    "num_classes = 10\n",
    "num_train = len(X_train)\n",
    "num_iterations = num_train // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hello():\n",
    "    def __init__(self, a, b, c):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    def add_placeholder(self):\n",
    "        self.x = self.a + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = hello(1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi.add_placeholder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hello(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, num_units, num_classes, \n",
    "                 learning_rate, num_epochs, batch_size):   \n",
    "        self.num_units = num_units\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.build()\n",
    "        \n",
    "    # 입력 데이터를 지정해준다. \n",
    "    def add_placeholders(self):\n",
    "        self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    # 모델을 설계해서 logits를 구하자.\n",
    "    def build_neural_net(self):\n",
    "        fc1 = tf.layers.dense(self.X, units=self.num_units, activation=tf.nn.relu)\n",
    "        self.logits = tf.layers.dense(fc1, units=self.num_classes)\n",
    "        self.pred = tf.identity(self.logits, 'prediction')\n",
    "            \n",
    "    # loss를 구하자.\n",
    "    def build_loss(self):\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits))\n",
    "\n",
    "    # train_op를 만들자.\n",
    "    def build_train_op(self):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "    # prediction 값을 구하자. \n",
    "    def build_accuracy(self):\n",
    "        correction = tf.equal(tf.argmax(self.y, axis=1), tf.argmax(self.pred, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correction, tf.float32))\n",
    "    \n",
    "    # 모델을 build 한다.\n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.build_neural_net()\n",
    "        self.build_loss()\n",
    "        self.build_train_op()\n",
    "        self.build_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델을 불러오자. 훨씬 간단해졌다. 보기에도, 쓰기에도\n",
    "neuralnet = Model(\n",
    "    num_units=num_units,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_accuracy: 0.820, test_accuracy: 0.804\n",
      "Epoch: 1, train_accuracy: 0.820, test_accuracy: 0.862\n",
      "Epoch: 2, train_accuracy: 0.860, test_accuracy: 0.881\n",
      "Epoch: 3, train_accuracy: 0.900, test_accuracy: 0.890\n",
      "Epoch: 4, train_accuracy: 0.900, test_accuracy: 0.898\n",
      "Epoch: 5, train_accuracy: 0.860, test_accuracy: 0.903\n",
      "Epoch: 6, train_accuracy: 0.910, test_accuracy: 0.907\n",
      "Epoch: 7, train_accuracy: 0.930, test_accuracy: 0.911\n",
      "Epoch: 8, train_accuracy: 0.890, test_accuracy: 0.912\n",
      "Epoch: 9, train_accuracy: 0.900, test_accuracy: 0.915\n",
      "Epoch: 10, train_accuracy: 0.890, test_accuracy: 0.917\n",
      "Epoch: 11, train_accuracy: 0.930, test_accuracy: 0.919\n",
      "Epoch: 12, train_accuracy: 0.930, test_accuracy: 0.921\n",
      "Epoch: 13, train_accuracy: 0.910, test_accuracy: 0.923\n",
      "Epoch: 14, train_accuracy: 0.920, test_accuracy: 0.925\n",
      "Epoch: 15, train_accuracy: 0.930, test_accuracy: 0.926\n",
      "Epoch: 16, train_accuracy: 0.910, test_accuracy: 0.928\n",
      "Epoch: 17, train_accuracy: 0.920, test_accuracy: 0.928\n",
      "Epoch: 18, train_accuracy: 0.930, test_accuracy: 0.930\n",
      "Epoch: 19, train_accuracy: 0.960, test_accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "# 학습에 필요한 부분을 지정해주자\n",
    "train_op = neuralnet.train_op\n",
    "loss = neuralnet.loss\n",
    "accuracy = neuralnet.accuracy\n",
    "X = neuralnet.X\n",
    "y = neuralnet.y\n",
    "\n",
    "# session을 실행시킨다. \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch수 만큼 iteration을 돌리자.\n",
    "for epoch in range(num_epochs):\n",
    "    # batch generator를 생성하자. \n",
    "    batch_generator = generator(X_train, y_train, batch_size)     \n",
    "    for i, batch in enumerate(batch_generator):\n",
    "        batch_X, batch_y = batch\n",
    "        _, train_loss, train_accuracy = sess.run([train_op, loss, accuracy], \n",
    "                                                 feed_dict={X: batch_X, y: batch_y})\n",
    "        \n",
    "    test_loss, test_accuracy = sess.run([loss, accuracy],\n",
    "                                        feed_dict={X: X_test, y: y_test})\n",
    "    print('Epoch: {0}, train_accuracy: {1:.3f}, test_accuracy: {2:.3f}'\\\n",
    "          .format(epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 외에도 더 개선할 부분이 많다. \n",
    "1. parameter들도 한 데 모아서 정리해줄 필요가 있다.\n",
    "2. class 내부적으로 밖에서 꺼내서 사용하지 않을 것 같은 매서드들은 합치고 사용할 것 같은 매서드들은 인자를 받아서 우리가 새로운 데이터를 입력해서 ***해당 부분만 실행되게*** 만들어주자. (예를 들면 검증을 위해 현재 class에서 predict라는 매서드를 만들어서 새로운 데이터를 받아서 예측 값을 얻을 필요가 있다.)\n",
    "3. train, feed data에 관한 부분도 정리할 수 있다.\n",
    "4. 1~3이 대충 완성되었다고 하면 pycharm으로 script 별로 나눠서 정리할 수 있다.!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
