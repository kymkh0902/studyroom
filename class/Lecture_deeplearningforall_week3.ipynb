{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두를 위한 딥러닝 #3\n",
    "\n",
    "오늘 소개할 내용은 Convolution Neural Network(CNN)으로 기존에 배웠던 Fully Connected Neural Network와 다른 새로운 모델이다.\n",
    "주로 이미지를 처리하는데 가장 많이 쓰이고 그 외에도 자연어 처리나 음성 인식 등에 사용되고 있다.\n",
    "\n",
    "### 학습 순서\n",
    "- 간단 역사 \n",
    "- CNN이란?\n",
    "- 구성 요소 소개 \n",
    "    - Convolutional layer\n",
    "    - Activation layer\n",
    "    - Pooling layer\n",
    "    - Fully connected layer\n",
    "    - 기타\n",
    "- 주요 모델 소개 \n",
    "    - AlexNet\n",
    "    - GoogleNet\n",
    "    - VGGNet\n",
    "    - ResNet\n",
    "\n",
    "### 1) 간단한 역사\n",
    "Hubel이라는 사람이 고양이의 뇌를 통해서 실험을 한다. 선 모양의 빛을 고양이에게 보여줬을 때 뇌의 어느 부분이 반응하는지 확인하는 실험이었다. 알아낸 사실은 **1) 뉴런이 망막에 비칠 때만 활동하고 2) 선의 방향에 따라 활동하는 뉴런이 바뀌며 3) 특정 방향으로 움직일 때만 활동하는 뉴런이 있다는 점이다.** 우리의 시신경이 어떻게 작동하는 지 알아냈다는 점에서 되게 중요한 발견인데 이와 같은 관찰로 방향성에 대해서 담당하는 부분으로 나뉘어진다는 점을 추측할 수 있다. \n",
    "\n",
    "뇌의 일부분이 특정 정보를 담당해서 처리한다는 점이 중요한데 복잡하게 생긴 물체를 볼 경우에 각기 영역들의 정보를 계층적으로 합칠 것이라고 생각할 수 있다. Neural network를 구성할 때에도 이미지를 다룰 때 이런 개념을 가져와서 특정 영역을 살피게 하고 이를 계층적으로 구성할 수도 있지 않을까? *약간 끼워 맞추기*이긴 한데 Convolutional Layer가 이런 방식으로 동작한다.\n",
    "\n",
    "CNN은 이미 1990년대 말에 나온 모델이다. (*사실 현재 사용하는 딥러닝 모델들은 거의다 20년 정도 전에 나왔다.*) 이래저래 우편번호 읽는 정도(*MNIST*)로 사용하다가 2012년에 CNN기반의 AlexNet이라는 모델이 나오면서 엄청난 파장을 일으켰고 엄청난 딥러닝의 관심을 불러왔다. 이미지, 영상 쪽은 딥러닝 분야에서 가장 관심을 많이 받는 분야이며 너무나도 데이터가 무겁기 때문에 다른 머신러닝 기법들을 적용하기 어렵고 딥러닝이 가장 잘하는 분야이다.\n",
    "![history](../pictures/hubel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CNN이란?\n",
    "\n",
    "CNN은 총 4개의 layer로 구성된다. ***(Convolutional layer, Activation layer, Pooling layer, Fully connected layer)*** 사람들이 많이 사용했을 때 어느 정도 성능적, 형식적으로 괜찮다고 해서 가장 많이 사용하는 구성이다. **이미지**를 잘 다룬다고 했는데 그 이유는 layer들을 살펴보면서 어떤 특징에서 온 것인지 살펴보도록 하자. \n",
    "\n",
    "### 3) 구성 요소\n",
    "\n",
    "#### 1.Convolutional Layer\n",
    "기본적인 Convolutional Layer(Conv layer)의 구조이다. 아래 그림을 보면 **빨간색**이 input 이미지를, **파란색**이 Conv layer를 통과했을 때 모습이라고 생각해보자. 그림이 나타내는 대로 layer를 통과했을 때 특정 영역에 대한 부분이 하나의 scalar값으로 나오게 되는데 여기에서! 자세히 보면 값이 1개가 아니라 여러 개이다. (depth가 깊다.) 여기까지 이해하면 특정 영역에 대한 정보를 하나의 값으로 나타내는데 조금씩 다르게 나타낼 수 있다. (여러 개) 이제, 개념은 대충 알았으니 Conv layer에 있는 개념들을 살펴보자.\n",
    "![cnn layer](http://cs231n.github.io/assets/cnn/depthcol.jpeg)\n",
    "- **Receptive field(filter)**: 특정 영역만 받아서 연산을 수행하는 부분이다. 영어로 표현하면 receptive field가 되겠다. 위 그림에서 보면 빨간색 이미지 내에 **표시된 크기의 사각형**이라고 볼 수 있다. 하는 역할은 이미지 내의 일부분만 보고 해당 정보를 축소해서 scalar 값으로 나타낸다. 이 때 사용하는 연산 방법이 **convolution**이다. <br> <br>여기서 잠깐, color 이미지를 생각하면 channel이 3갠데.. 이거 연산하려면 filter도 channel이 3개이어야 하지 않을까? 맞다. 그래야 연산이 가능하다. <br> 그리고, 그냥 scalar 값으로 달랑 표현해버리면 정보가 다 사라지지 않을까? deep한 network는 개뿔 금방 모든 정보가 사라질 것 같다. 그래서, 1개의 값만 만들어주는게 아니라 여러 개의 값을 만들어준다. 그럼, 1번째 layer에 적용된다고 생각했을 때 filter의 channel은 3이었고 n개 만큼 값을 만들어주려면 n만큼 filter가 존재해야 한다. 즉 \n",
    "`filter.shape = [field size, field size, 3, n]` <br><br> 즉, 우리는 원하는 크기의 filter를 만들어서 이미지에 대한 정보를 쪼개고 추상화해서 전달할 수 있고 그 개수도 우리가 맘대로 정할 수 있다.(hyperparameter) 그럼, filter를 구성하는 parameter는 어떻게 구성해야 될까?\n",
    "- **parameter sharing**: filter의 parameter 수는 filter 크기의 제곱에 depth를 곱한 값에 비례할 것이다. filter는 이미지를 위에서 아래로 순차적으로 훑는다고 생각할 수 있다. 근데 여기서 매 번 훑을 때마다 parameter가 달라진다고 생각하면.. 오예 엄청난 수의 paramter가 생길 것이다. 물론 이렇게 해도 작은 layer는 돌아가긴 하겠지만 deep하게 만드는 건 포기해야겠다. <br><br>그럼 filter의 parameter를 전부다 같다고 놓으면 어떨까? 직관적으로 생각해보면 이미지 내 각 영역 별로 다른 형태이긴 하지만 이를 구성하는 직선, 곡선, 색깔은 공통적이라고 볼 수 있다. 그럼 우리의 parameter들이 나타내는 값이 형태, 색깔이런 것들이라고 생각하면 전부다 같게 놓고 훑어도 되지 않겠나? (*물론 훑었을 때 각 영역이 다 다른 값을 가지니까 정보가 축소된 값들 scalar는 다른 값을 갖게 된다.*) 이게 parameter sharing이다. 이렇게 했을 때, 실제로 잘 작동하며 아래 그림처럼 직관적으로 생각한대로 filter가 특정 방향의 선이나 색깔을 인식할 수 있게 된다. (*위에서 간단하게 본 역사와 닮아있지 않나? 맞다. 완전히 같은 건 아니지만 비슷한 내용으로 볼 수도 있다.*) \n",
    "![parameters](http://cs231n.github.io/assets/cnn/weights.jpeg)  <br><br>\n",
    "    \n",
    "- **stride**: filter로 이미지를 훑을 때, 한 칸씩 전체를 훑는게 좋을까 아니면 띄엄 띄엄 훑는게 좋을까? 명확한 정답은 없지만 한 칸씩 훑는 경우에는 좀 더 조밀하게 모든 정보를 취합하는 것이라고 생각할 수 있고 조금 띄엄 띄엄 보면 각 영역에 대한 정보를 조금은 더 추상화해서 다룬다고 생각할 수 있다. \n",
    "- **padding**: \n",
    "Padding은 layer는 아닌데 중요한 개념이라 소개하고 넘어간다. 우리가 filter로 이미지를 읽게 되면 이미지 크기가 줄어들게 되어 있다. \n",
    "음.. 우리는 deep한 모델을 만들고 싶은데 이미지가 작은 경우에는 끝부분이 계속 닳아 없어져서 그럴 수가 없게 되어 버린다. 그래서 나타난 개념이 padding이다. 이미지를 원래 상태로 만들어주기 위해서 테두리를 별 의미가 없는 값(*주로 0*)으로 둘러싼다. 그럼 이미지가 layer들을 지나더라도 크기가 줄지 않아서 deep한 모델을 만들 수 있다! 아래 그림을 예로 들면 0을 추가함으로써 기존 크기를 유지할 수 있게 되었다.\n",
    "![padding](http://cs231n.github.io/assets/cnn/stride.jpeg)\n",
    "\n",
    "#### 2. Pooling Layer\n",
    "위 Conv layer를 통해서 우리는 지역적인 정보를 축소하고 변형해서 나타낼 수 있다는 것을 배웠다. 이번에는 pooling layer로 위 내용과는 약간 다르게 정보를 축소하는 법을 살펴보자. 직관적인 내용으로는 그냥 엄청나게 큰 이미지를 작게 만들어도 우리가 구별할 수 있다는게 끝이다. *downsampling*이라고 하는데 방법에는 max-pooling, average-pooling 등이 있다. 단순한 직관으로는 이미지를 더 추상화해서 나타냄으로써 일반적인 분류를 잘하게 만들어준다고 생각할 수 있겠다. \n",
    "![pooling example](http://cs231n.github.io/assets/cnn/pool.jpeg)\n",
    "![pooling example](http://cs231n.github.io/assets/cnn/maxpool.jpeg)\n",
    "\n",
    "#### 3. Fully Connected Layer\n",
    "이전 시간까지 다뤘던 내용으로 `tf.matmul(W, X) + b`에 해당되는 내용이다. 완전히 node간에 연결되어 있다고 해서 붙여진 이름이다.\n",
    "여기에서 필요한 이유는 우리가 결국 하려고 하는게 **분류(classification)**이기 때문이다. 그러려면 최종적으로 마지막 layer의 정보를 전부다 \n",
    "종합해서 class의 개수만큼 만들어줘야 하는데 지금까지 배우기를 FC layer가 그 작업을 잘한다고 배웠다. 아래 그림에서 보면 \n",
    "마지막 FC layer를 지나기 전에 flatten을 해주는 것을 볼 수 있는데 단순히 전체를 펴서 `tf.matmul(W, X) + b` 연산이 가능하게 만들어주는 것이다. \n",
    "![fc](https://navoshta.com/images/posts/traffic-signs-classification/traffic-signs-architecture.png)\n",
    "\n",
    "위 내용을 종합해서 각 layer를 지나갈 때 이미지가 어떤 식으로 변형되는지 보면 아래 그림처럼 나타난다.\n",
    "![overview](http://cs231n.github.io/assets/cnn/convnet.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "### 주요 모델\n",
    "1. AlexNet\n",
    "2. GoogleNet\n",
    "3. VGGNet\n",
    "4. ResNet\n",
    "\n",
    "출처: <br>\n",
    "http://cs231n.github.io/convolutional-networks/ <br>\n",
    "https://navoshta.com/images/posts/traffic-signs-classification/traffic-signs-architecture.png <br>\n",
    "http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN의 numpy, tensorflow 구현 \n",
    "\n",
    "아래 코드에 있는 데이터와 filter를 다루면서 CNN이 실제 어떤 식으로 작동하는지 살펴보자. forward만 다루며 backward는 코드로 짜지는 않겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. x값 지정 \n",
    "# x를 image라고 생각하면 7x7 크기의 3 channel(RGB)로 보면 된다. \n",
    "\n",
    "x = np.zeros((7, 7, 3))\n",
    "x[:, :, 0] = np.array([[[0, 0, 0, 0, 0, 0, 0], \n",
    "                        [0, 0, 1, 0, 1, 0, 0], \n",
    "                        [0, 2, 1, 0, 1, 2, 0], \n",
    "                        [0, 0, 2, 0, 0, 1, 0],\n",
    "                        [0, 2, 0, 1, 0, 0, 0], \n",
    "                        [0, 0, 0, 1, 2, 2, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0]]])\n",
    "x[:, :, 1] = np.array([[[0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 1, 0, 0, 1, 1, 0], \n",
    "                        [0, 0, 2, 2, 1, 1, 0],\n",
    "                        [0, 2, 1, 2, 1, 0, 0],\n",
    "                        [0, 2, 1, 1, 2, 2, 0],\n",
    "                        [0, 1, 2, 0, 2, 2, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0]]])\n",
    "x[:, :, 2] = np.array([[[0, 0, 0, 0, 0, 0, 0],\n",
    "                        [0, 2, 1, 1, 1, 1, 0],\n",
    "                        [0, 2, 2, 1, 2, 1, 0],\n",
    "                        [0, 1, 1, 0, 2, 2, 0],\n",
    "                        [0, 2, 1, 2, 2, 0, 0],\n",
    "                        [0, 1, 2, 2, 0, 0, 0],\n",
    "                        [0, 0, 0, 0, 0, 0, 0]]])\n",
    "\n",
    "# shape를 바꿔서 (N, H, W, C)을 나타내도록 한다. \n",
    "# N: 데이터 개수, H: 높이, W: 너비, C: 채널 수\n",
    "x = np.reshape(x, (1, 7, 7, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. w값 지정\n",
    "# w는 image를 잘라서 볼 receptive field이다. 3x3 크기로 보고 3 channel을 받아서 2 channel로 내보낸다. \n",
    "# shape는 순서대로 3x3 receptive field size, 3 channel_in, 2 filter_size로 구성된다. \n",
    "\n",
    "w = np.zeros((3, 3, 3, 2))\n",
    "w[:, :, 0, 0] = np.array([[0, 0, 1],\n",
    "                          [-1, 1, 1],\n",
    "                          [0, 1, 0]])\n",
    "w[:, :, 1, 0] = np.array([[1, 1, 1],\n",
    "                          [0, 1, 1],\n",
    "                          [0, 1, 0]])\n",
    "w[:, :, 2, 0] = np.array([[-1, 0, 0],\n",
    "                          [-1, 1, 1],\n",
    "                          [0, -1, 0]])\n",
    "\n",
    "# w1 = np.zeros((3,3,3))\n",
    "w[:, :, 0, 1] = np.array([[0, 0, 0],\n",
    "                          [1, 1, -1],\n",
    "                          [-1, 1, 1]])\n",
    "w[:, :, 1, 1] = np.array([[0, 1, -1],\n",
    "                          [1, 1, -1],\n",
    "                          [-1, 1, -1]])\n",
    "w[:, :, 2, 1] = np.array([[1, 1, 0],\n",
    "                          [-1, -1, 0],\n",
    "                          [0, -1, 1]])\n",
    "\n",
    "# stride: 얼마나 건너 뛰면서 볼 것인지 결정한다. \n",
    "stride = 2\n",
    "scope = \"conv_in_numpy\"\n",
    "activation = tf.nn.relu  # activation\n",
    "pad = 'VALID'  # padding\n",
    "num_filters = 2  # number of filters\n",
    "filter_size = 3  # filter size\n",
    "b = [1, 0]  # bias\n",
    "\n",
    "s = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  2.,  1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x의 좌측 구석 영역 1번 channel (3x3)\n",
    "x[0, :3, :3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  2.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x의 좌측 구석 영역 2번 channel (3x3)\n",
    "x[0, :3, :3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  2.,  1.],\n",
       "       [ 0.,  2.,  2.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x의 좌측 구석 영역 3번 channel (3x3)\n",
    "x[0, :3, :3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1., -1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  1.,  0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w의 1번 channel을 받는 receptive field (3x3)\n",
    "w[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0., -1.],\n",
       "       [ 1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w의 2번 channel을 받는 receptive field (3x3)\n",
    "w[1, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 1.,  1., -1.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w의 3번 channel을 받는 receptive field (3x3)\n",
    "w[2, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x를 w에 통과시켰을 때의 1번 filter 값 \n",
    "np.sum(x[0, :3, :3, 0] * w[:, :, 0, 0] + x[0, :3, :3, 1] * w[:, :, 1, 0] + x[0, :3, :3, 2] * w[:, :, 2, 0]) + b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x를 w에 통과시켰을 때의 2번 filter 값 \n",
    "np.sum(x[0, :3, :3, 0] * w[:, :, 0, 1] + x[0, :3, :3, 1] * w[:, :, 1, 1] + x[0, :3, :3, 2] * w[:, :, 2, 1]) + b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Convolution in Tensorflow ---\n",
      "tf_output0:\n",
      " [[  6.   4.   3.]\n",
      " [ 13.   7.   4.]\n",
      " [ 10.   9.   5.]]\n",
      "tf_output1:\n",
      " [[ -1.  -3.   1.]\n",
      " [  0.   6.   2.]\n",
      " [  1.  -3.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# 2. CNN in Tensorflow\n",
    "\n",
    "print(\"--- Convolution in Tensorflow ---\")\n",
    "tf.reset_default_graph()\n",
    "\n",
    "tf_x = tf.constant(x, dtype=tf.float32)\n",
    "tf_w = tf.constant(w, dtype=tf.float32)\n",
    "tf_b = tf.constant(b, dtype=tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope(scope):\n",
    "        # 입력 channel을 구한다. : 3\n",
    "        channel_in = tf_x.get_shape()[3].value\n",
    "        # w를 지정해준다. (위에서 만든 w를 넣어준다.)\n",
    "#         tf_w = tf.get_variable(\"w\", [filter_size, filter_size, channel_in, num_filters], \n",
    "#                                initializer=tf.constant_initializer(w))\n",
    "#         # b를 지정해준다. (위에서 만든 b를 넣어준다.)\n",
    "#         tf_b = tf.get_variable(\n",
    "#             \"b\", [num_filters],\n",
    "#             initializer=tf.constant_initializer(b, dtype=tf.float32))\n",
    "        tf_z = tf.nn.conv2d(\n",
    "            tf_x, tf_w, strides=[1, stride, stride, 1], padding=pad) + tf_b\n",
    "        tf_h = activation(tf_z)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf_output = sess.run(tf_z)\n",
    "        print(\"tf_output0:\\n\", tf_output[0, :, :, 0])\n",
    "        print(\"tf_output1:\\n\", tf_output[0, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Convolution in numpy ---\n",
      "z: 0\n",
      "z: 1\n",
      "np_output0:\n",
      " [[  6.   4.   3.]\n",
      " [ 13.   7.   4.]\n",
      " [ 10.   9.   5.]]\n",
      "np_output1:\n",
      " [[ -1.  -3.   1.]\n",
      " [  0.   6.   2.]\n",
      " [  1.  -3.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# 3. CNN in numpy\n",
    "\n",
    "print(\"--- Convolution in numpy ---\")\n",
    "\n",
    "np_output = np.zeros((1, 3, 3, 2))\n",
    "s = stride\n",
    "rf = filter_size\n",
    "nf = num_filters\n",
    "\n",
    "for z in range(nf):\n",
    "    print(\"z:\", z)\n",
    "    h_range = int((x.shape[2] - rf) / s) + 1  # (W - F + 2P) / S\n",
    "    for _h in range(h_range):\n",
    "        w_range = int((x.shape[1] - rf) / s) + 1  # (W - F + 2P) / S\n",
    "        for _w in range(w_range):\n",
    "            np_output[0, _h, _w, z] = np.sum(\n",
    "                x[0, _h * s:_h * s + rf, _w * s:_w * s + rf, :] *\n",
    "                w[:, :, :, z]) + b[z]\n",
    "\n",
    "print(\"np_output0:\\n\", np_output[0, :, :, 0])\n",
    "print(\"np_output1:\\n\", np_output[0, :, :, 1])\n",
    "\n",
    "\n",
    "np.testing.assert_almost_equal(tf_output, np_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
