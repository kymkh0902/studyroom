{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning week #4\n",
    "\n",
    "#### 지난 수업에서 빠진 내용 : Regularization 에 대한 이해 \n",
    "\n",
    "먼저, 이번 강의에 앞서서 Regularzation에 대한 설명을 조금 더 하도록 하겠다. <br>\n",
    "\n",
    "모델을 학습시킬 때 발생하는 문제(*학습이 원하는 대로 잘 되지 않는 문제*)가 2가지가 있다. **Overfitting과 Underfitting**이다. \n",
    "\n",
    "- Overfitting : low bias, high variance \n",
    "- underfitting : high bias, low variance \n",
    "\n",
    "의문이 들텐데 bias와 variance는 도대체 무엇인가? 아래 그래프에서 잘 설명하고 있다. <br>\n",
    "Murphy의 머신러닝 책에 자세히 개념이 나와있지만 일단 아래 그래프를 통해 직관적으로 설명하고 넘어가도록 하겠다. \n",
    "\n",
    "먼저 아래 그래프에 대해 소개한다. 파란 선이 Ground truth(정답)을 나타내고 학습시켰을 때 위 그래프가 underfitting 아래 그래프가 overfitting이 일어난 경우이다. 왼쪽 그래프는 데이터의 분포를 나타내므로 여러개의 선으로 나타나지고, 오른쪽 그래프는 분포를 평균 낸 하나의 곡선이다. <br>\n",
    "\n",
    "underfitting의 경우 먼저 보자. Variance가 낮으므로 데이터 분포는 고르다. \n",
    "내가 어떤 것을 택해도 ***특정한 범위를 크게 벗어날 일***(예상하지 못한 튀는 사건)이 많지 않다. 아직까진 좋다. 데이터를 예측했을 때 고르게 예측이 된다는 거니까. <br>\n",
    "다음은 bias이다. bias가 높다. 이 말은 평균적으로 편향된 경향을 보여서 정답으로부터 멀리 떨어져있다. 음..좋지 않다. 위 두 내용을 합치면 정답으로부터 고른 분포를 가지고 떨어져있다. <br> ***즉, underfitting이 일어나면 고르게는 예측하나 평균적으로 정답으로부터 떨어진 값을 예측한다. *** \n",
    "\n",
    "다음은 overfitting이다. Variance가 높으므로 데이터 분포가 고르지 않다. 그렇다는 의미는 내가 데이터로 예측을 했을 때 예측값이 계속해서 변동이 발생한다. 대신 bias를 보면 낮다. 이 의미는 평균적으로 봤을 때는 정답의 값과 매우 비슷한 형태를 띈다고 생각할 수 있다. <br>\n",
    "***즉, overfitting이 일어나면 평균적으로는 정답과 유사한 값을 예측하나 데이터의 분포가 너무 커서 일정한 예측이 어려운 모델이다.***\n",
    "\n",
    "![image](../pictures/bias_and_vars.svg)\n",
    "\n",
    "아래 표는 overfitting, underfitting 각각에 대한 간단한 예시이다. <br>\n",
    "위 내용을 참고해서 보면 어떤 점이 문제인지 쉽게 알 수 있다. 그리고 어떤게 더 나쁜지 좋은지 판단하기도 어렵다는 것을 알 수 있다. \n",
    "\n",
    "|구분|X|Y|정답| \n",
    "|---|---|---|---|\n",
    "|overfitting|10|35|25|   \n",
    "||10|10|25|   \n",
    "||10|30|25|   \n",
    "|underfitting|10|18|25| \n",
    "||10|16|25|   \n",
    "||10|17|25|   \n",
    "\n",
    "**해결 방법?** <br>\n",
    "- Overfitting : 주로 모델을 복잡도를 줄여주거나 (*데이터의 feature 수를 줄인다, regularizer을 추가한다.*), 모델의 복잡도를 만족하도록 만들어준다. (*데이터를 늘려준다.*) 현재 상태에서 둘 중에 무엇이 부족한지 무엇을 할 수 있는지 판단하면 된다. \n",
    "- Underfitting : 주로 모델의 복잡도를 늘려준다. (*데이터의 feature를 추가하거나 더 높은 차수의 수식을 적용하거나 한다.*)\n",
    "\n",
    "**Regularization** <br>\n",
    "강의 자료에 보면 ***Cost 식의 뒷부분에 $\\lambda \\sum_{i=1}^n \\theta_j^2$ 항이 더해진 것을 볼 수 있다.*** (Cost 함수에 직접적으로 영향을 준다 모델의 예측 함수에 영향은 없다.!!) <br>\n",
    "overfitting을 막기 위해서 넣어주는 항으로 regularizer로써의 역할을 한다. <br>\n",
    "$J(\\theta) = \\frac 1 {2m} \\big [ \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{i=1}^n \\theta_j^2 \\big ]$ <br>\n",
    "regularizer의 역할에 대해서 직관적으로 생각해보자. overfitting을 막기 위해서 regularizer를 넣어주므로 $lambda = 0$일 때 overfitting이 발생한다고 하자. 그럼, $lambda$가 엄청나게 커지면 어떻게 될까?(100, 1000, 10000...) 그럼, Cost식이 \n",
    "$\\lambda \\sum_{i=1}^n \\theta_j^2$에 의해 좌지우지 될 것이다. 이 상태에서 Cost는 계속 최소가 되도록 $\\theta$를 학습시켜야 하므로\n",
    "모든 $\\theta$는 0으로 이동하게 되어있다. (***모든 $\\theta$가 0이라는 의미는 하나의 직선, variance가 매우 작지만 bias가 매우 큰, 즉, underfitting이 일어난다.***)\n",
    "\n",
    "이 정도 내용이랑 regularizer의 종류에는 ***L1 regularizer, L2 regularizer***(각각 절대값, 제곱)가 있다는 정도만 알고 넘어가자. \n",
    "\n",
    "***\n",
    "#### 학습 목표 : Neural Network에 대한 이해 (Deep Learning의 기초)\n",
    "\n",
    "\n",
    "**서론**<br>\n",
    "\n",
    "Neural Network(신경망)에 대해서 배워보자. 요새 가장 유망한 분야인 딥러닝에 근간이 되는 모델이다. <br>\n",
    "딥러닝은 별거 없다. 신경망을 여러 겹겹이 쌓아서 만든 모델(**Deep**)에 대한 분야라고 생각하면 되겠다. \n",
    "\n",
    "딥러닝에서 활발하게 연구가 이루어지는 분야는 자연어 처리(Natural Language Processing), 컴퓨터 비전(Computer Vision), 음성 인식(Speech Recognition) 등이 있다. 이 분야의 공통점을 생각해보면 ***왜 딥러닝이 각광을 받는지, 다른 머신 러닝 분야가 무엇을 힘들어하는지 알 수 있다.*** <br> \n",
    "\n",
    "위에서 처리하는 영역들을 살펴보자. 먼저, 컴퓨터 비전의 경우 이미지를 다뤄야한다. pixel 별로 나타내는 것들이 다 다르므로 feature로 놓아야 할 듯하다. 근데 256*256 pixel만 되어도 feature가 6만5천개이므로 이 엄청난 복잡성을 어떻게 다뤄야 할 지 모르겠다. <br> \n",
    "다음은, 언어 관련 데이터이다. 글이나 말을 보면 시간에 따라서 정보가 전달되는데 과거 정보와 현재, 미래 정보가 서로 영향을 끼친다. 그럼 feature를 설정할 때 과거/현재/미래의 단어들 전부로 해야할 듯한데 사전만해도 단어의 개수가 50만개가 넘는다. 엄청나게 복잡한 모델이겠다. <br>\n",
    "\n",
    "이런 문제들을 기존 모델로 다루려고 하면 연산량 문제로 인해서 학습시키는게 불가능했다. ***그래서 약 4~5년 전 딥러닝이 각광을 받기 전만 하더라도  이미지 인식, 기계 번역, 음성 인식에 대한 우리가 누릴 수 있는 서비스는 많지 않았다.*** <br>\n",
    "\n",
    "하지만 최근 몇 년간 연구로 신경망으로 이런 복잡한 정보들을 효과적으로 학습할 수 있다는게 밝혀져서 엄청나게 뜨거운 분야로 올라왔다. <br>\n",
    "(*GPU의 연산 능력의 향상, 데이터 양의 증가, 기존 신경망 학습에서의 문제 극복*)  \n",
    "\n",
    "**Neural Network의 구조** <br>\n",
    "뇌의 신경망(neuron)을 닮은 구조라고 해서 붙여진 이름이 Neural Network(NN)이다. 생물학적인 내용은 넘어가도록 하자. <br>\n",
    "NN에서 하나의 neuron은 각기 제 역할을 하는 logistic unit으로 볼 수 있다. 정보가 넘어왔을 때 넘길 건지 말건지에 대한 결정을 한다고 \n",
    "보면 된다. 그래서 학습을 하다 보면 각기 ***unit들이 모델의 목적에 맞게 필요한 정보들만 모아서 원하는 예측 값을 구하는 방식***이다. <br>\n",
    "(*예시로는 이미지를 구별하는 것이 있는데 우리는 픽셀 단위로 보면서 이미지를 구별하지 않는다. 멀리서 봤을 떄 전체적인 형상을 보고 구별하는데 여기엔 어느 정도 추상적으로 개별 픽셀들이 합쳐진 형태로 받아들여지는 과정이 진행된다. 이런 작업을 NN이 한다고 보면 된다. 엄청나게 복잡한 데이터를 추상화해서 예측할 수 있도록.*)\n",
    "\n",
    "### Neural Network Forward Propagation <br>\n",
    "NN은 Forward/Back propagation 방법을 통해서 학습을 하는데 이번 주에는 일단 Forward만 살펴본다.(*쉬운 부분*)\n",
    "기본적으로 NN은 쉽게 생각하면 logistic regression이 반복적으로 진행된다고 생각하면 된다. \n",
    "\n",
    "처음에 notation이 정말 헷갈리므로 아래 강의 자료에 나온 내용을 보면서 어떤 식으로 나타내는지 보도록 하자. <br>\n",
    "\n",
    "**1) 각 항에 대한 설명** <br>\n",
    "$a, x, \\theta$ 3개의 항에 대한 수식 전개가 진행된다. 개별적으로 보자. <br>\n",
    "- $a_i^{(j)}$ : $j$번째 layer의 $i$번째 unit의 activation 값 <br>\n",
    "- $x_i$ : $i$번째 unit의 input 값 <br>\n",
    "- $\\theta_{ji}^l$ : $l$ layer에서 $j$번째 unit에서 $i$번째 unit으로 정보를 전달할 때 사용하는 $\\theta$값 <br>\n",
    "\n",
    "**2) 수식에 대한 설명** <br>\n",
    "$z$와 $g$에 대한 부연 설명을 한다. <br>\n",
    "- $z_1^2 = \\theta_{10}^1 x_0 + \\theta_{11}^1 x_1 + \\theta_{12}^1 x_2 + \\theta_{13}^1 x_3$ <br> 각 layer 별 pre-activation(*activation 하기 전*) 값을 구하기 위해 사용한다. 이전 layer의 feature 값과 theta를 곱해주면 된다. 다음 layer를 지날 땐 $x$ 대신 $a$가 쓰일 것이다.\n",
    "- $a_i^2 = g(z_1^2)$ <br> 위에서 구한 $z$를 통해서 우린 activation 값을 구해야한다. 이 때 사용하는 activation 함수를 $g$라고 해주며 강의에서는 **sigmoid** 를 사용한다. (*다른 함수들도 많다.*)\n",
    "\n",
    "![NN](../pictures/neural_network.png)\n",
    "\n",
    "**3) matrix로 수식 설명** <br>\n",
    "위에서는 전체 layer 내 unit들에 대해서 전부다 계산을 풀어서 해서 너무 복잡하다. matrix로 만들어서 조금 쉽게 만들어보자. <br>\n",
    "아래 식을 보면 크게 변한 항들은 없고 Matrix라서 대문자로 나타내주고 layer 정보는 계속 가져가면서 unit 위치 정보는 다 없애버렸다. <br>\n",
    "matrix로 만드는 이유는 연산 속도 때문임을 다시 한 번 알아두면 좋고 아래 식은 logistic regression에서 했던 식 그대로라는 점을 참고하자. <br>\n",
    "$\\begin{align} \n",
    "Z^2 &= \\Theta^{(1)} \\cdot X \\\\\n",
    "A^2 &= g(Z^2) \\\\\n",
    "Z^3 &= \\Theta^{(2)} \\cdot A^2 \\\\\n",
    "A^3 &= g(Z^3) \\\\\n",
    "H_{\\theta}(X) &= A^3 \\\\ \n",
    "\\end{align}$\n",
    "\n",
    "**4) Hyperparameter(layer, hidden_size) ** <br>\n",
    "우리는 layer를 몇 개로 해줄건지, hidden_size는 몇으로 해줄건지 정해줘야 한다. 둘 다 커지면 커질수록 모델이 복잡해지므로 \n",
    "많은 데이터를 처리할 수 있게 해준다. 대신 적은 데이터로 진행했을 땐 overfitting이 발생해서 적절한 값의 조정이 필요하다. \n",
    "\n",
    "### XOR problem \n",
    "비 선형적인 문제를 풀기 위해서 XOR 문제를 풀어야 하는데 layer가 2개일 때 가능해진다. \n",
    "\n",
    "### Multiclass classification \n",
    "\n",
    "NN을 활용해서 앞서 말했던 이미지와 같은 고차원 데이터를 분류할 수 있다. 방법은 NN에 데이터를 집어넣고, layer를 적절히 구상한 다음에 \n",
    "마지막에 output 결과를 class 수에 맞춰서 내보내면 된다. (*one-hot 벡터*) 개념에 대해서 생각한 뒤에 예제 코드를 돌려보면 쉽게 이해할 수 있을 것이다. \n",
    "\n",
    "\n",
    "\n",
    "이미지 출처 :\n",
    "1. bias and variance 설명 : http://m-clark.github.io/docs/machine_learning/02_concepts.html <br>\n",
    "2. NN 구조 설명 : http://www.holehouse.org/mlclass/08_Neural_Networks_Representation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
