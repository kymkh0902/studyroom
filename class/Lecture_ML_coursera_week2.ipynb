{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning week #2\n",
    "\n",
    "\n",
    "#### 학습 목표 : 변수가 2개 이상인 데이터를 Gradient descent로 학습시키기 \n",
    "\n",
    "다시 한번 강조하지만 이 과목에서는 주로 수식을 어떻게하면 벡터화 할 수 있을까에 대한 내용이 main이다.\n",
    "\n",
    "알고리즘이 아무리 다양해봤자 몇 줄 수식에 불과하고 학습시킬 때는 대부분이 gradient descent를 사용하므로 벡터화만 <br>\n",
    "잘 시키면 코드 상으로 바로 학습을 시킬 수 있다. 쉽고 어렵다.\n",
    "\n",
    "그리고 표현 방법에 너무 헷갈리거나 스트레스 받지 않아도 된다. $h_{\\theta}(x) = \\theta^TX$ 에서 <br> \n",
    "*transpose가 왜 나온걸까* 를 확인하기 보다는 그냥 **shape**를 확인하면 $\\theta$ 든 $\\theta^T$ 든 뭘 써도 상관없다. \n",
    "(*물론 다 이해하면 좋다.*)\n",
    "\n",
    "#### Gradient descent for multiple variables\n",
    "\n",
    "2주차 강의에서는 변수가 2개 이상인 1차 함수에 대해 다룬다. theta 수가 더 많아지고 x(feature)수도 더 많아진다.  <br>\n",
    "우리는 학습을 시킬 때 **objective function** 만 신경쓰면 되니까 \n",
    "$J(\\theta_0, \\theta_1, ... \\theta_n) = {\\frac 1 {2m}} \\Sigma_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^2$ \n",
    "만 풀어주면 된다. <br>\n",
    "\n",
    "objective function에서 뭐가 달라졌는지 보도록 하자. <br>\n",
    "#### Objective function \n",
    "\n",
    "-.기존 : $h_{\\theta}(x^{(i)}) = \\theta_0 x_0^{(i)} + \\theta_1 x_1^{(i)}$ <br>\n",
    "-.변경 : $h_{\\theta}(x^{(i)}) = \\theta_0 x_0^{(i)} + \\theta_1 x_1^{(i)} + ... + \\theta_n x_n^{(i)}$ <br>\n",
    "\n",
    "$y^{(i)}$는 어차피 주어지는 label이라서 신경 쓸 필요가 없고 $h_{\\theta}(x^{(i)})$가 변했다는 것을 알 수 있다. <br>\n",
    "\n",
    "그럼 loss는 위에 그대로 구하면 되고 gradient는 어떻게 바뀌었는지 보자.\n",
    "\n",
    "#### Gradient update \n",
    "\n",
    "-.기존 : *모든 theta에 대해서 $\\theta_j := \\theta_j - \\alpha {\\frac \\partial {\\partial \\theta_j}} J(\\theta_0, \\theta_1)$ 한번에 업데이트* <br>\n",
    "-.변경 : *모든 theta에 대해서 $\\theta_j := \\theta_j - \\alpha {\\frac \\partial {\\partial \\theta_j}} J(\\theta_0, \\theta_1, ...\\theta_n)$, 한번에 업데이트*\n",
    "\n",
    "식을 비교해봤을 때 theta 몇 개가 추가되었다. 추가된 theta들이 기존 theta를 update 하는 방식과 같은지 다른지 살펴보자. <br>\n",
    "\n",
    "대표적으로 $\\theta_n$에 대한 gradient를 구해보자 <br>\n",
    "\n",
    "\\begin{align} {\\frac \\partial {\\partial \\theta_n}}J(\\theta_0, \\theta_1, ...\\theta_n) &= {\\frac \\partial {\\partial \\theta_n}}\\big \\{ {\\frac 1 {2m}} \\Sigma_{i=1}^m(h_{\\theta}(x^{(i)}) - y^{(i)})^2\\big \\} \\\\\n",
    "&= {\\frac \\partial {\\partial \\theta_n}} \\Big \\{ {\\frac 1 {2m}} \\Sigma_{i=1}^m \\big (\\theta_0 x_0^{(i)} + \\theta_1 x_1^{(i)} + ... + \\theta_n x_n^{(i)} - y^{(i)}\\big )^2 \\Big \\} \\\\\n",
    "&= {\\frac 1 {2m}} \\Sigma_{i=1}^m \\Big \\{ {\\frac \\partial {\\partial \\theta_n}} \\big (\\theta_0 x_0^{(i)} + \\theta_1 x_1^{(i)} + ... + \\theta_n x_n^{(i)} - y^{(i)}\\big )^2 \\Big \\} \\\\\n",
    "&= {\\frac 1 {2m}} \\Sigma_{i=1}^m   (\\theta_0 x_0^{(i)} + \\theta_1 x_1^{(i)} + ... + \\theta_n x_n^{(i)} - y^{(i)}) \\cdot {2 x_n^{(i)}} \\\\\n",
    "&= {\\frac 1 {m}} \\Sigma_{i=1}^m  (h_{\\theta}(x^{(i)}) - y^{(i)}) \\cdot {x_n^{(i)}}\n",
    "\\end{align}  \n",
    "\n",
    "아래 수식처럼 전개가 가능하고 모든 $n$에 대해서 다 같은 식으로 계산이 가능해진다. 수식이 하나이므로 코드로 위 식을 짜서 모든 $\\theta$에 대해서 iteration을 돌려서 업데이트를 시켜주면 된다.\n",
    "\n",
    "#### Feature scaling\n",
    "\n",
    "데이터를 전처리하는 방법이 처음 제시되었다. feature 별로 다루는 scale의 범위가 너무 차이가 나므로 (*방 크기 vs 평수*) <br>\n",
    "일정한 범위 내에서 어떤 한 변수가 너무 힘을 받지 않도록 맞춰줘야 한다. 보통 normalization을 많이 사용한다. \n",
    "\n",
    "-.normalization : $ x_i \\leftarrow \\frac {x_i - \\mu_i} {s_i} $ (여기서, $i$는 feature 종류를 나타내고 $\\mu$ 는 평균, $s$는 표준편차나 (max-min)값을 나타낸다.) \n",
    "\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "학습 시간에 영향을 미치는 요소이다. 너무 작게 설정하면 너무 오래, 크게 설정하면 optimal에서 벗어나서 발산해버릴 수도 있다. <br>\n",
    "적절한 값을 얻는게 중요한데 어차피 경험적으로 알아야 되는 값이라서 ***너무 작지 않고, 크지 않게 진행하다가 학습 중에 줄이는 방법을 주로 쓴다.*** <br>\n",
    "(*예를 들면 10만번 학습한다고 하면 처음 값을 0.01로 진행하다가 2만번 학습할 때마다 절반으로 줄인다. *)\n",
    "\n",
    "\n",
    "#### Polynomial regression\n",
    "\n",
    "우리가 보통 예측하고자 하는 것들 중에 직선으로만 모델을 만들 수 있는 경우는 거의 없다. 강의에서 보듯 2차 함수를 사용했을 때 \n",
    "더 잘 들어맞는 것처럼 높은 차수의 함수를 사용했을 때 도움이 되는 경우도 있다.\n",
    "\n",
    "여기서 **n차 함수, m개의 변수 어떻게 설정할까?** 라는 의문이 가장 중요하다고 본다. 앞으로 살펴볼 모델도 다 마찬가지겠지만 <br>\n",
    "이에 대한 확실한 답은 없고 강의 내내 적절한 모델을 잘 찾아가는 방법을 배운다고 보면 되겠다. <br>\n",
    "우리가 단순한 데이터를 20차 함수로 모델링했을 때 최적의 모델을 그려보면 모든 데이터를 지날 것이다. 하지만 새로운 데이터가 들어왔을 때 \n",
    "너무나도 복잡한 이 그래프는 맞추지 못할 것이다. (***overfitting의 개념***)\n",
    "\n",
    "반대로, 거의 2차 함수 곡선을 이루고 있는 데이터들을 간단한 선으로 모델링한다고 하면 이 선들은 많은 데이터들을 놓치게 될 것이다. (***underfitting의 개념***)\n",
    "\n",
    "뒷 장에서 이를 해결하는 방법들에 대해서 좀 더 자세히 배울 예정이고 대부분의 머신러닝 모델의 tuning은 노가다라는 점을 다시 한 번..\n",
    "\n",
    "아, 그리고 polynomial도 위에서 linear와 마찬가지로 수식으로 나타낸 뒤에 gradient descent를 수행하면 된다. 대신에 polynomial은 \n",
    "gradient descent 식이 좀 더러울 것이다.\n",
    "\n",
    "\n",
    "#### Normal equation\n",
    "\n",
    "마지막으로 normal equation을 배우는데 이는 잘 몰라도 된다. 어차피 연산량이 커서 사용하지도 않고 수식 하나로 딸랑 구하는 개념이기 때문에 \n",
    "수식을 코드로 나타내는 정도만 알면 되겠다. normal equation으로 구한 모델이랑 gradient descent로 구한 모델 비교하는 정도만 하고 \n",
    "넘어가면 되겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
